"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[3237,1477],{74491:(e,t,n)=>{n.d(t,{Z:()=>s});var a,i=n(67294);function r(){return r=Object.assign?Object.assign.bind():function(e){for(var t=1;t<arguments.length;t++){var n=arguments[t];for(var a in n)Object.prototype.hasOwnProperty.call(n,a)&&(e[a]=n[a])}return e},r.apply(this,arguments)}const s=e=>{let{title:t,titleId:n,...s}=e;return i.createElement("svg",r({xmlns:"http://www.w3.org/2000/svg",viewBox:"-1 -1 235 174",width:235,height:174,"aria-labelledby":n},s),t?i.createElement("title",{id:n},t):null,a||(a=i.createElement("g",{fill:"none"},i.createElement("title",null,"Canvas 1"),i.createElement("title",null,"Layer 1"),i.createElement("path",{d:"M225.48 57.47a8.26 8.26 0 0 0-8.261 8.261v27.254h-13.643V43.778c0-8.757-7.105-15.862-15.852-15.862h-45.408V15.523h27.262c4.565 0 8.251-3.697 8.251-8.261 0-4.565-3.686-8.262-8.251-8.262h-71.04c-4.564 0-8.25 3.697-8.25 8.262 0 4.564 3.696 8.261 8.25 8.261h27.263v12.393H92.918s-7.022-.372-11.184.877c-4.162 1.25-8.334 6.517-8.334 6.517L53.417 55.892H42.471c-8.758 0-15.863 7.105-15.863 15.862v21.232H16.023V65.732a8.26 8.26 0 0 0-8.261-8.262C3.197 57.46-.5 61.167-.5 65.732v71.04a8.26 8.26 0 0 0 8.262 8.26 8.26 8.26 0 0 0 8.261-8.26v-27.263H26.62v21.574c0 8.757 7.105 15.852 15.862 15.852h8.014l22.09 18.454s4.491 3.78 9.985 5.608C87.558 172.659 96.594 173 96.594 173h59.328s3.14 0 5.494-1.002c2.355-.991 3.81-3.531 3.81-3.531l16.679-21.532h5.804c8.757 0 15.862-7.105 15.862-15.852v-21.584h13.631v27.262a8.261 8.261 0 0 0 8.272 8.262c4.565 0 8.252-3.697 8.252-8.262V65.733a8.244 8.244 0 0 0-8.246-8.263ZM100.56 146.686l14.592-42.588H95.416l25.704-50.664h30.115L130.684 95.65h20.922Z",fill:"#00a4f1"}))))}},96101:(e,t,n)=>{n.d(t,{Z:()=>s});var a,i=n(67294);function r(){return r=Object.assign?Object.assign.bind():function(e){for(var t=1;t<arguments.length;t++){var n=arguments[t];for(var a in n)Object.prototype.hasOwnProperty.call(n,a)&&(e[a]=n[a])}return e},r.apply(this,arguments)}const s=e=>{let{title:t,titleId:n,...s}=e;return i.createElement("svg",r({xmlns:"http://www.w3.org/2000/svg",viewBox:"-283 202 306.371 235.39",width:306.371,height:235.39,"aria-labelledby":n},s),t?i.createElement("title",{id:n},t):null,a||(a=i.createElement("g",{fill:"none"},i.createElement("title",null,"Canvas 1"),i.createElement("title",null,"Layer 1"),i.createElement("title",null,"Group"),i.createElement("path",{stroke:"#bdcfdb",strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:5,d:"m-248.423 384.184 22.591-33.037M-199.532 330.08l54.416-21.716M-207.5 352.095l35.837 66.134M3.559 287.651l-40.145 55.739M-54.579 229.96.75 270.154M-256.081 301.082l26.996 23.864M-53.219 339.195-66.548 237.07M-123.27 310.921l54.006 37.974"}),i.createElement("circle",{cx:-215.891,cy:336.609,r:17.609,fill:"#00a4f1"}),i.createElement("circle",{cx:-257.111,cy:396.889,r:15.389,fill:"#00a4f1"}),i.createElement("circle",{cx:-133.5,cy:303.728,r:12.504,fill:"#fe2600"}),i.createElement("circle",{cx:-166.805,cy:427.195,r:10.195,fill:"#00a4f1"}),i.createElement("circle",{cx:-50.203,cy:362.297,r:23.297,fill:"#00a4f1"}),i.createElement("circle",{cx:10.867,cy:277.504,r:12.504,fill:"#00a4f1"}),i.createElement("circle",{cx:-68.827,cy:219.609,r:17.609,fill:"#00a4f1"}),i.createElement("path",{d:"m-107.6 346.79-5.296-9.173c18.09-10.988 24.357-34.377 14.185-52.938l9.172-5.296a.9.9 0 0 0 .33-1.228.898.898 0 0 0-1.23-.33l-9.171 5.296c-10.989-18.09-34.377-24.356-52.939-14.185l-5.295-9.172a.898.898 0 0 0-1.229-.329.9.9 0 0 0-.329 1.229l5.296 9.172c-18.088 10.992-24.355 34.38-14.183 52.942l-9.172 5.295a.898.898 0 0 0-.33 1.229.898.898 0 0 0 1.23.33l9.171-5.296c10.993 18.087 34.381 24.354 52.943 14.182l5.295 9.172a.9.9 0 0 0 1.229.33.91.91 0 0 0 .323-1.23Zm-18.702-28.795 10.95 18.964c-17.704 9.676-39.994 3.703-50.487-13.528l18.966-10.945a.898.898 0 0 0 .33-1.229.898.898 0 0 0-1.23-.329l-18.964 10.95c-9.676-17.704-3.703-39.994 13.528-50.487l10.95 18.964a.898.898 0 0 0 1.228.33.898.898 0 0 0 .329-1.23l-10.949-18.964c17.703-9.676 39.993-3.703 50.486 13.528l-18.964 10.95a.898.898 0 0 0-.329 1.228.898.898 0 0 0 1.229.329l18.964-10.95c9.676 17.704 3.703 39.994-13.528 50.487l-10.95-18.964a.9.9 0 0 0-1.228-.33.897.897 0 0 0-.33 1.226Z",fill:"#ea400c"}),i.createElement("path",{d:"M-161.901 314.346a30.262 30.262 0 0 1 9.093-33.993.899.899 0 0 0-1.143-1.387 32.057 32.057 0 0 0-9.63 36.014.898.898 0 1 0 1.681-.633ZM-156.86 323.05a.899.899 0 0 0-1.387 1.143c8.701 10.548 23.158 14.416 35.967 9.622a.899.899 0 0 0-.63-1.683c-12.091 4.526-25.737.876-33.95-9.082ZM-110.109 284.444c.313.384.88.44 1.265.121a.897.897 0 0 0 .122-1.264 32.062 32.062 0 0 0-36.007-9.665.899.899 0 0 0 .63 1.684c12.118-4.532 25.776-.87 33.99 9.123ZM-105.087 293.159c4.518 12.096.853 25.735-9.111 33.944a.898.898 0 1 0 1.143 1.386c10.558-8.692 14.438-23.146 9.65-35.959a.899.899 0 0 0-1.683.63Z",fill:"#ea400c"}),i.createElement("path",{d:"M-151.746 319.256a.9.9 0 0 0-.057 1.273c6.49 7.086 16.609 9.794 25.772 6.892a.897.897 0 1 0-.545-1.712c-8.5 2.69-17.882.182-23.905-6.392a.89.89 0 0 0-1.265-.06ZM-155.469 310.688c-2.71-8.518-.197-17.918 6.402-23.95a.9.9 0 0 0 .059-1.27.903.903 0 0 0-1.273-.056c-7.112 6.506-9.821 16.64-6.896 25.819a.903.903 0 0 0 1.128.584.9.9 0 0 0 .58-1.127ZM-117.937 320.722a.9.9 0 0 0-.06 1.27.9.9 0 0 0 1.27.058c7.098-6.487 9.813-16.6 6.92-25.767a.895.895 0 0 0-1.124-.586.893.893 0 0 0-.586 1.125c2.682 8.503.162 17.882-6.42 23.9ZM-115.222 288.239a.899.899 0 0 0 .059-1.27c-6.493-7.124-16.627-9.842-25.815-6.933a.899.899 0 0 0 .545 1.712c8.524-2.698 17.925-.176 23.946 6.429.33.37.9.398 1.265.06Z",fill:"#ea400c"}),i.createElement("circle",{cx:-267.611,cy:290.889,r:15.389,fill:"#00a4f1"}))))}},64970:(e,t,n)=>{n.d(t,{Z:()=>s});var a,i=n(67294);function r(){return r=Object.assign?Object.assign.bind():function(e){for(var t=1;t<arguments.length;t++){var n=arguments[t];for(var a in n)Object.prototype.hasOwnProperty.call(n,a)&&(e[a]=n[a])}return e},r.apply(this,arguments)}const s=e=>{let{title:t,titleId:n,...s}=e;return i.createElement("svg",r({xmlns:"http://www.w3.org/2000/svg",viewBox:"112.5 9 210.044 150",width:210.044,height:150,"aria-labelledby":n},s),t?i.createElement("title",{id:n},t):null,a||(a=i.createElement("g",{fill:"none"},i.createElement("title",null,"Canvas 1"),i.createElement("title",null,"Layer 1"),i.createElement("title",null,"B\xe9zier"),i.createElement("path",{d:"M300.693 24.003a11.248 11.248 0 0 0 12.505 7.34 11.256 11.256 0 0 0 9.346-11.09 11.256 11.256 0 0 0-9.346-11.09 11.248 11.248 0 0 0-12.505 7.34H270.02c-.815 0-1.603.263-2.25.75l-14.999 11.25s0 .187-.262.262a56.274 56.274 0 0 0-37.62-12.158 56.29 56.29 0 0 0-36.327 15.608l-14.625-14.624a3.727 3.727 0 0 0-2.662-1.087h-23.174a11.248 11.248 0 0 0-12.505-7.34 11.256 11.256 0 0 0-9.346 11.09 11.256 11.256 0 0 0 9.346 11.09 11.248 11.248 0 0 0 12.505-7.34h21.637l13.761 13.798a55.69 55.69 0 0 0-11.699 42.45h-19.949a11.248 11.248 0 0 0-12.505-7.341A11.256 11.256 0 0 0 120 84a11.256 11.256 0 0 0 9.346 11.09 11.248 11.248 0 0 0 12.505-7.34h21.524a56.269 56.269 0 0 0 19.836 29.472l-20.061 26.773h-28.798a11.248 11.248 0 0 0-12.506-7.34 11.256 11.256 0 0 0-9.346 11.09 11.256 11.256 0 0 0 9.346 11.09 11.248 11.248 0 0 0 12.506-7.34h30.672a3.747 3.747 0 0 0 3-1.5l19.499-25.986v21.861c0 7.247 5.878 13.125 13.124 13.125 7.247 0 13.125-5.878 13.125-13.125v-16.873h3.75v13.124c.028 8.287 6.065 15.327 14.249 16.611.862.179 1.743.272 2.625.263.815.01 1.63-.066 2.437-.225 8.259-1.237 14.38-8.306 14.437-16.649v-14.4l19.686 22.5a3.756 3.756 0 0 0 2.812 1.274h19.424a11.248 11.248 0 0 0 12.506 7.34 11.256 11.256 0 0 0 9.346-11.09 11.256 11.256 0 0 0-9.346-11.09 11.248 11.248 0 0 0-12.506 7.34h-17.736l-23.511-26.885a55.91 55.91 0 0 0 19.761-29.36h21.486a11.248 11.248 0 0 0 12.506 7.34 11.256 11.256 0 0 0 9.346-11.09 11.256 11.256 0 0 0-9.346-11.09 11.248 11.248 0 0 0-12.506 7.34h-19.949c.347-2.484.525-4.986.525-7.499a55.91 55.91 0 0 0-15.674-38.885l13.162-9.862Zm10.574-7.5c1.519 0 2.887.91 3.468 2.316a3.778 3.778 0 0 1-.815 4.087 3.778 3.778 0 0 1-4.087.816 3.747 3.747 0 0 1-2.316-3.469 3.749 3.749 0 0 1 3.75-3.75Zm-183.738 7.5a3.747 3.747 0 0 1-3.469-2.315 3.778 3.778 0 0 1 .816-4.087 3.778 3.778 0 0 1 4.087-.816 3.747 3.747 0 0 1 2.316 3.469c0 .993-.394 1.95-1.097 2.653a3.753 3.753 0 0 1-2.653 1.096Zm3.75 63.745a3.747 3.747 0 0 1-3.469-2.315 3.778 3.778 0 0 1 .816-4.087 3.778 3.778 0 0 1 4.087-.816A3.747 3.747 0 0 1 135.028 84c0 .993-.393 1.95-1.096 2.653a3.753 3.753 0 0 1-2.653 1.096Zm-7.5 63.745a3.747 3.747 0 0 1-3.468-2.315 3.778 3.778 0 0 1 .815-4.087 3.778 3.778 0 0 1 4.087-.816 3.747 3.747 0 0 1 2.316 3.469c0 .993-.394 1.95-1.097 2.653a3.753 3.753 0 0 1-2.653 1.097Zm179.99-7.5c1.518 0 2.887.91 3.468 2.316a3.778 3.778 0 0 1-.815 4.088 3.778 3.778 0 0 1-4.088.815 3.747 3.747 0 0 1-2.315-3.468 3.749 3.749 0 0 1 3.75-3.75Zm0-63.744c1.518 0 2.887.91 3.468 2.315a3.778 3.778 0 0 1-.815 4.087 3.778 3.778 0 0 1-4.088.816 3.747 3.747 0 0 1-2.315-3.468 3.749 3.749 0 0 1 3.75-3.75Zm-57.859 3c-9.262 6.224-21.814 3.787-28.085-5.438a4.621 4.621 0 0 1-.713-3.412c.235-1.2.938-2.25 1.95-2.925L245.048 54a4.578 4.578 0 0 1 6.225 1.125c6.262 9.253 3.862 21.824-5.363 28.124Zm-40.31-3a14.71 14.71 0 0 1-9.712 6.15c-.853.15-1.716.224-2.587.224a14.78 14.78 0 0 1-8.587-2.625 14.976 14.976 0 0 1-6.206-9.665 14.982 14.982 0 0 1 2.456-11.22 3.746 3.746 0 0 1 5.212-.938l18.412 12.936a3.744 3.744 0 0 1 1.537 2.4 3.755 3.755 0 0 1-.525 2.738Z",fill:"#00a4f1"}),i.createElement("path",{d:"M189.025 77.851a7.23 7.23 0 0 0 5.587 1.238 8.157 8.157 0 0 0 1.875-.563l-10.65-7.5a7.489 7.489 0 0 0 3.188 6.825ZM228.774 75.939a11.248 11.248 0 0 0 12.074-.225 11.235 11.235 0 0 0 4.762-11.25Z",fill:"#ea400c"}))))}},80615:(e,t,n)=>{n.r(t),n.d(t,{default:()=>G});var a=n(67294),i=n(86010),r=n(12724),s=n(39960),o=n(52263),l=n(92949);const h="heroBanner_qdFl",d="buttons_AeoN",c="carouselCard_veYf",m="vastLogo_OteY",p="carousel_qyqS",u="container_bfhl",g="blogDesc_gbil",f="top_wZiK";var y=n(87462);const b="features_t9lD",v="featureSvg_GfXr",w=[{title:"Telemetry Data Engine",Svg:n(74491).Z,description:a.createElement(a.Fragment,null,"Your SIEM keels over and costs explode? Deploy VAST close to the data source to transform, store, anonymize, query, and age your high-volume telemetry at the edge.")},{title:"Security Content Execution",Svg:n(64970).Z,description:a.createElement(a.Fragment,null,"Easy-button detection: operationalize your security content by synchronizing it with your threat intelligence platform, unifying live and retro detection with one system.")},{title:"Threat Hunting & Data Science",Svg:n(96101).Z,description:a.createElement(a.Fragment,null,"Pivot to what matters and contextualize the gaps. Need richer analysis? Bring your own data science tooling: VAST offers high-bandwidth data access via ",a.createElement("a",{href:"https://arrow.apache.org"},"Apache Arrow")," and Parquet.")}];function T(e){let{title:t,Svg:n,description:r}=e;return a.createElement("div",{className:(0,i.Z)("col col--4")},a.createElement("div",{className:"text--center"},a.createElement(n,{className:v,role:"img"})),a.createElement("div",{className:"text--center padding-horiz--md"},a.createElement("h3",null,t),a.createElement("p",null,r)))}function k(){return a.createElement("section",{className:b},a.createElement("div",{className:"container"},a.createElement("div",{className:"row"},w.map(((e,t)=>a.createElement(T,(0,y.Z)({key:t},e)))))))}var x,_;function S(){return S=Object.assign?Object.assign.bind():function(e){for(var t=1;t<arguments.length;t++){var n=arguments[t];for(var a in n)Object.prototype.hasOwnProperty.call(n,a)&&(e[a]=n[a])}return e},S.apply(this,arguments)}const A=e=>{let{title:t,titleId:n,...i}=e;return a.createElement("svg",S({xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 260 70","aria-labelledby":n},i),t?a.createElement("title",{id:n},t):null,x||(x=a.createElement("defs",null,a.createElement("style",null,".cls-1{fill:#fff}"))),a.createElement("g",{id:"Ebene_1","data-name":"Ebene 1"},_||(_=a.createElement("path",{className:"cls-1",d:"M81.63 63.36a.56.56 0 0 1-.57-.56v-55a.57.57 0 1 1 1.13 0v55a.56.56 0 0 1-.56.56ZM177.63 54.44l-13.82-38.09a1.52 1.52 0 0 0-1.34-.94h-9.19a1.53 1.53 0 0 0-1.34.94l-13.82 38.09a.66.66 0 0 0 .66.94h9.87a1.45 1.45 0 0 0 1.3-1l1.58-5.11a1.44 1.44 0 0 1 1.29-1h10a1.45 1.45 0 0 1 1.29 1l1.58 5.11a1.44 1.44 0 0 0 1.3 1h10a.65.65 0 0 0 .64-.94Zm-17-14.85H155a.36.36 0 0 1-.37-.48l.47-1.66.28-1 2.32-9c.06-.27.16-.27.22 0l2.32 9c.08.27.2.7.27 1l.48 1.66a.36.36 0 0 1-.36.48ZM253 15.41h-32.24a1 1 0 0 0-1 1v8.29a1 1 0 0 0 1 1h9.06a1 1 0 0 1 1 1v27.68a1 1 0 0 0 1 1H242a1 1 0 0 0 1-1V26.7a1 1 0 0 1 1-1h9a1 1 0 0 0 1-1v-8.29a1 1 0 0 0-1-1ZM213.57 34.88c-2.68-2.69-6.63-3.66-9.32-4.23l-3.88-.86c-3-.63-4.81-.91-5.78-1.89a2.5 2.5 0 0 1-.8-1.94 2.89 2.89 0 0 1 1.37-2.34 7.61 7.61 0 0 1 3.43-.69c2.18 0 3.32.63 3.89 1.26a2.82 2.82 0 0 1 .54.94.75.75 0 0 0 .77.56h10.36c.42 0 .66-.15.69-.53a5.82 5.82 0 0 0-.16-1 9.42 9.42 0 0 0-.54-2 10.59 10.59 0 0 0-3.08-4.29c-3.66-3.15-9.38-3.43-12.47-3.43-3.6 0-8.46.4-12.29 3.37a11.82 11.82 0 0 0-4.69 9.66 9.39 9.39 0 0 0 3.09 7.55c1.43 1.32 4.23 3.09 9.89 4.23l4 .8a16.35 16.35 0 0 1 4.69 1.6 3.17 3.17 0 0 1 1.49 2.86 3.26 3.26 0 0 1-1.49 2.86 6 6 0 0 1-3.54.86c-1.2 0-3.43-.06-4.86-1.6a5.81 5.81 0 0 1-1.33-3.16v-.2a.68.68 0 0 0-.72-.65h-11.19c-.55 0-.79.3-.65 1l.13.7a14.78 14.78 0 0 0 3.64 7.92c3.94 4.12 10.06 4.52 14 4.52 3.66 0 10.12-.34 14.12-4.4a14.82 14.82 0 0 0 3.77-9.72 10.71 10.71 0 0 0-3.08-7.76ZM140.93 15.41h-10a1.43 1.43 0 0 0-1.29 1l-.39 1.25-6.8 23.51-.57 2.2c-.06.27-.16.27-.23 0l-.28-1.11-7-24.42-.44-1.43a1.45 1.45 0 0 0-1.3-1h-9.87a.66.66 0 0 0-.66.94l13.8 38.09a1.53 1.53 0 0 0 1.34.94h9.19a1.53 1.53 0 0 0 1.34-.94l13.82-38.09a.66.66 0 0 0-.66-.94Z"})),a.createElement("path",{d:"M59.93 10.8H28a.92.92 0 0 0-.4.08 1 1 0 0 0-.6 1.34l5.2 12.46a1 1 0 0 0 .95.63H38a1.1 1.1 0 0 1 1.1 1.48l-2.68 6.44a2.44 2.44 0 0 0 0 1.75l6.51 15.52c.21.58.56 1 1.12 1a1.1 1.1 0 0 0 1-.72L61 12.42a1.08 1.08 0 0 0-1.07-1.62Z",style:{fill:"#00ede1"}}),a.createElement("path",{d:"M41.23 58.18 23.92 16.61l-2.15-5.21a1 1 0 0 0-.91-.6H7.34a1.09 1.09 0 0 0-1.08 1.62L25 57.44l.72 1.74a1 1 0 0 0 .91.61h13.52a1.09 1.09 0 0 0 1.08-1.61Z",style:{fill:"#00a4f1"}})))};var q,P;function z(){return z=Object.assign?Object.assign.bind():function(e){for(var t=1;t<arguments.length;t++){var n=arguments[t];for(var a in n)Object.prototype.hasOwnProperty.call(n,a)&&(e[a]=n[a])}return e},z.apply(this,arguments)}const V=e=>{let{title:t,titleId:n,...i}=e;return a.createElement("svg",z({id:"Layer_4","data-name":"Layer 4",xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 260 70","aria-labelledby":n},i),t?a.createElement("title",{id:n},t):null,q||(q=a.createElement("defs",null,a.createElement("style",null,".cls-1{fill:#0f0f0f}"))),P||(P=a.createElement("path",{className:"cls-1",d:"M82 64.05a.58.58 0 0 1-.57-.57v-55a.57.57 0 1 1 1.14 0v55a.58.58 0 0 1-.57.57ZM178 55.12 164.18 17a1.55 1.55 0 0 0-1.34-.94h-9.19a1.56 1.56 0 0 0-1.35.94l-13.82 38.12a.66.66 0 0 0 .66.94H149a1.44 1.44 0 0 0 1.29-1l1.6-5.06a1.46 1.46 0 0 1 1.3-1h10a1.44 1.44 0 0 1 1.29 1l1.58 5.12a1.46 1.46 0 0 0 1.3 1h10a.66.66 0 0 0 .64-1Zm-17-14.85h-5.63a.35.35 0 0 1-.36-.48l.47-1.66c.07-.27.2-.7.27-1l2.32-9c.06-.27.17-.27.23 0l2.32 9c.07.26.19.69.27 1l.47 1.66a.36.36 0 0 1-.36.48ZM253.39 16.09h-32.26a1 1 0 0 0-1 1v8.3a1 1 0 0 0 1 1h9.06a1 1 0 0 1 1 1v27.67a1 1 0 0 0 1 1h10.14a1 1 0 0 0 1-1V27.39a1 1 0 0 1 1-1h9.06a1 1 0 0 0 1-1v-8.3a1 1 0 0 0-1-1ZM213.94 35.56c-2.69-2.68-6.63-3.66-9.32-4.23l-3.89-.86c-3-.63-4.8-.91-5.77-1.88a2.49 2.49 0 0 1-.81-1.95 2.89 2.89 0 0 1 1.38-2.34 7.61 7.61 0 0 1 3.43-.69c2.17 0 3.31.63 3.89 1.26a3 3 0 0 1 .54.94.73.73 0 0 0 .76.56h10.36c.43 0 .67-.15.69-.53a5.23 5.23 0 0 0-.15-1 10.51 10.51 0 0 0-.55-2 10.56 10.56 0 0 0-3.08-4.29c-3.66-3.14-9.37-3.43-12.46-3.43-3.6 0-8.46.4-12.29 3.37a11.82 11.82 0 0 0-4.67 9.7 9.42 9.42 0 0 0 3.08 7.55c1.43 1.31 4.24 3.08 9.9 4.23l4 .8a16 16 0 0 1 4.69 1.6 3.17 3.17 0 0 1 1.48 2.86 3.26 3.26 0 0 1-1.48 2.85 6 6 0 0 1-3.55.86c-1.2 0-3.43 0-4.86-1.6a5.73 5.73 0 0 1-1.32-3.16v-.1a.38.38 0 0 0 0-.1.67.67 0 0 0-.71-.64H182c-.56 0-.79.3-.65 1l.13.69a14.84 14.84 0 0 0 3.63 7.92c3.95 4.12 10.06 4.52 14 4.52 3.66 0 10.12-.34 14.12-4.4a14.73 14.73 0 0 0 3.77-9.73 10.75 10.75 0 0 0-3.06-7.78ZM141.3 16.09h-10a1.45 1.45 0 0 0-1.3 1l-.39 1.25-6.81 23.51-.56 2.2c-.07.27-.17.27-.23 0l-.29-1.11-7-24.42-.44-1.43a1.43 1.43 0 0 0-1.29-1h-9.89a.66.66 0 0 0-.66.94l13.82 38.09a1.53 1.53 0 0 0 1.34.94h9.19a1.54 1.54 0 0 0 1.35-.94L142 17a.66.66 0 0 0-.7-.91Z"})),a.createElement("path",{d:"M60.3 11.48h-32a1 1 0 0 0-.39.08 1 1 0 0 0-.55 1.34l5.2 12.47a1 1 0 0 0 .94.63h4.86a1.1 1.1 0 0 1 1.11 1.48l-2.69 6.44a2.5 2.5 0 0 0 0 1.75l6.46 15.52c.22.57.57 1 1.13 1a1.08 1.08 0 0 0 1-.73l16-38.34a1.09 1.09 0 0 0-1.07-1.64Z",style:{fill:"#00ede1"}}),a.createElement("path",{d:"M41.6 58.86 24.28 17.29l-2.16-5.2a1 1 0 0 0-.91-.61H7.71a1.09 1.09 0 0 0-1.09 1.62l18.76 45 .72 1.74a1 1 0 0 0 .9.61h13.52a1.09 1.09 0 0 0 1.08-1.59Z",style:{fill:"#00a4f1"}}))};var E=n(82730);const I="dots_nNBT",F="dot_KkCJ",C="active_gYpA",M=e=>{let{children:t}=e;const[n,r]=a.useState(0),[s,o]=(0,a.useState)(!1),[l,h]=(0,E.E)({initial:0,loop:!0,slideChanged(e){r(e.track.details.rel)},created(){o(!0)}},[e=>{let t,n=!1;function a(){clearTimeout(t)}function i(){clearTimeout(t),n||(t=setTimeout((()=>{e.next()}),5e3))}e.on("created",(()=>{e.container.addEventListener("mouseover",(()=>{n=!0,a()})),e.container.addEventListener("mouseout",(()=>{n=!1,i()})),i()})),e.on("dragStarted",a),e.on("animationEnded",i),e.on("updated",i)}]);return a.createElement(a.Fragment,null,a.createElement("div",{className:"navigation-wrapper"},a.createElement("div",{ref:l,className:"keen-slider"},t.map(((e,t)=>a.createElement("div",{key:t,className:"keen-slider__slide"},e))))),s&&h.current&&a.createElement("div",{className:I},[...Array(h.current.track.details.slides.length).keys()].map((e=>a.createElement("button",{key:e,onClick:()=>{var t;null==(t=h.current)||t.moveToIdx(e)},className:(0,i.Z)(F,n===e?C:"")})))))};var W,R,L,N=n(62514),D=n(22588);const O=n(30010),Z=e=>{const t=e.indexOf("\x3c!--truncate--\x3e");return-1===t?e:e.substring(0,t)},j=null==O||null==(W=O.blogPosts)?void 0:W.find((e=>{var t,n,a;return null==e||null==(t=e.metadata)||null==(n=t.tags)||null==(a=n.map((e=>e.label)))?void 0:a.includes("release")})),B=null==O||null==(R=O.blogPosts)||null==(L=R.filter((e=>{var t,n,a;return!(null!=e&&null!=(t=e.metadata)&&null!=(n=t.tags)&&null!=(a=n.map((e=>e.label)))&&a.includes("release"))})))?void 0:L.slice(0,2);function H(){var e,t,n;const{siteConfig:r}=(0,o.Z)(),{colorMode:c}=(0,l.I)();return a.createElement(a.Fragment,null,a.createElement("header",null,a.createElement("div",{className:(0,i.Z)("hero",h)},a.createElement("div",{className:"container"},"dark"===c?a.createElement(A,{className:m,title:"VAST Logo"}):a.createElement(V,{className:m,title:"VAST Logo"}),a.createElement("p",{className:"hero__subtitle"},r.tagline),a.createElement("div",{className:d},a.createElement(s.Z,{className:"button button--primary button--lg",to:"/docs/about"},"Get Started")))),a.createElement("div",{className:p},a.createElement(M,null,[a.createElement(U,{key:"newsletter-carousel",title:"Sign up for our Newsletter",link:"/newsletter",label:"Get the latest news and updates from the VAST team",buttonLabel:"Subscribe"}),(null==j||null==(e=j.metadata)?void 0:e.title)&&a.createElement(U,{key:"latest-release-carousel",title:null==j||null==(t=j.metadata)?void 0:t.title,link:null==j||null==(n=j.metadata)?void 0:n.permalink,label:"Latest Relaese",buttonLabel:"Read Announcement"})].concat(null==B?void 0:B.map(((e,t)=>{var n,i,r,s,o;return a.createElement(U,{key:"blogpost-${idx}-carousel",title:null==e||null==(n=e.metadata)?void 0:n.title,link:null==e||null==(i=e.metadata)?void 0:i.permalink,label:"Latest Blogpost",buttonLabel:"Read Post",imageLink:null==e||null==(r=e.metadata)||null==(s=r.frontMatter)?void 0:s.image,description:null!=e&&null!=(o=e.metadata)&&o.hasTruncateMarker?Z(null==e?void 0:e.content):null})})))))))}const U=e=>{let{title:t,link:n,description:r,imageLink:o,label:l,buttonLabel:h}=e;const[d,m]=(0,a.useState)("");return(0,a.useEffect)((()=>{(0,D.j)().use(N.Z).process(r).then((e=>{const t=(n=e.toString(),a=300,n.length>a?n.substring(0,a)+"...":n);var n,a;m(t)}))}),[r]),a.createElement("div",{className:c},a.createElement("div",{className:(0,i.Z)(u,"container")},a.createElement("div",{className:f},a.createElement("div",null,l,a.createElement("p",{className:"hero__subtitle"},t),r&&""!==r&&a.createElement("div",{className:g},d)),a.createElement("div",null,a.createElement(s.Z,{className:"button button--info button--md",to:n},h))),o&&o.startsWith("/")&&a.createElement("img",{src:o,alt:"Blogpost Figure",className:"svglite",height:"200px"})))};function G(){const{siteConfig:e}=(0,o.Z)();return a.createElement(r.Z,{title:"Visibility Across Space and Time",description:"The network telemetry engine for data-driven security investigations."},a.createElement(H,null),a.createElement("main",null,a.createElement(k,null)))}},30010:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/the-new-rest-api","metadata":{"permalink":"/blog/the-new-rest-api","source":"@site/blog/the-new-rest-api/index.md","title":"The New REST API","description":"As of v2.4 VAST ships with a new web plugin that","date":"2023-01-26T00:00:00.000Z","formattedDate":"January 26, 2023","tags":[{"label":"frontend","permalink":"/blog/tags/frontend"},{"label":"rest","permalink":"/blog/tags/rest"},{"label":"api","permalink":"/blog/tags/api"},{"label":"architecture","permalink":"/blog/tags/architecture"}],"readingTime":7.065,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"},{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"The New REST API","authors":["lava","mavam"],"date":"2023-01-26T00:00:00.000Z","image":"/img/rest-api-deployment-single.light.png","tags":["frontend","rest","api","architecture"]},"nextItem":{"title":"Parquet & Feather: Data Engineering Woes","permalink":"/blog/parquet-and-feather-data-engineering-woes"}},"content":"As of [v2.4](/blog/vast-v2.4) VAST ships with a new `web` [plugin][plugins] that\\nprovides a [REST API][rest-api]. The [API documentation](/api) describes the\\navailable endpoints also provides an\\n[OpenAPI](https://spec.openapis.org/oas/latest.html) spec for download. This\\nblog post shows how we built the API and what you can do with it.\\n\\n[rest-api]: /docs/use/integrate/rest-api\\n[plugins]: /docs/understand/architecture/plugins\\n[actors]: /docs/understand/architecture/actor-model\\n\\n\x3c!--truncate--\x3e\\n\\nWhy does VAST need a REST API? Two reasons:\\n\\n1. **Make it easy to integrate with VAST**. To date, the only interface to VAST\\n   is the command line. This is great for testing and ad-hoc use cases, but to\\n   make it easy for other tools to integrate with VAST, a REST API is the common\\n   expectation.\\n\\n2. **Develop our own web frontend**. We are in the middle of building a\\n   [Svelte](https://svelte.dev/) frontend that delivers a web-based experience\\n   of interacting with VAST through the browser. This frontend interacts with\\n   VAST through the REST API.\\n\\nTwo architectural features of VAST made it really smooth to design the REST API:\\n[Plugins][plugins] and [Actors][actors].\\n\\nFirst, VAST\'s plugin system offers a flexible extension mechanism to add\\nadditional functionality without bloating the core. Specifically, we chose\\n[RESTinio](https://github.com/Stiffstream/restinio) as C++ library that\\nimplements an asynchronous HTTP and WebSocket server. Along with it comes a\\ndependency on Boost ASIO. We deem it acceptable to have this dependency of the\\n`web` plugin, but would feel less comfortable with adding dependencies to the\\nVAST core, which we try to keep as lean as possible.\\n\\nSecond, the [actor model architecture][actors] of VAST makes it easy to\\nintegrate new \\"microservices\\" into the system. The `web` plugin is a *component\\nplugin* that provides a new actor with a typed messaging interface. It neatly\\nfits into the existing architecture and thereby inherits the flexible\\ndistribution and scaling properties. Concretely, there exist two ways to run the\\nREST API actor: either as a separate process or embedded inside a VAST server\\nnode:\\n\\n![REST API - Single Deployment](/img/rest-api-deployment-single.light.png#gh-light-mode-only)\\n![REST API - Single Deployment](/img/rest-api-deployment-single.dark.png#gh-dark-mode-only)\\n\\nRunning the REST API as dedicated process gives you more flexibility with\\nrespect to deployment, fault isolation, and scaling. An embedded setup offers\\nhigher throughput and lower latency between the REST API and the other VAST\\ncomponents.\\n\\nThe REST API is also a *command plugin* and exposes the\u2014you guessed it\u2014`web`\\ncommand. To run the REST API as dedicated process, spin up a VAST node as\\nfollows:\\n\\n```bash\\nvast web server --certfile=/path/to/server.certificate --keyfile=/path/to/private.key\\n```\\n\\nTo run the server within the main VAST process, use a `start` command:\\n\\n```bash\\nvast start --commands=\\"web server [...]\\"\\n```\\n\\nThe server will only accept TLS requests by default. To allow clients to connect\\nsuccessfully, you need to pass a valid certificate and corresponding private key\\nwith the `--certfile` and `--keyfile` arguments.\\n\\n## Authentication\\n\\nClients must authenticate all requests with a valid token. The token is a short\\nstring that clients put in the `X-VAST-Token` request header.\\n\\nYou can generate a valid token on the command line as follows:\\n\\n```bash\\nvast web generate-token\\n```\\n\\nFor local testing and development, generating suitable certificates and tokens\\ncan be a hassle. For this scenario, you can start the server in [developer\\nmode](#developer-mode) where it accepts plain HTTP connections and does not\\nperform token authentication.\\n\\n## TLS Modes\\n\\nThere exist four modes to start the REST API, each of which suits a slightly\\ndifferent use case.\\n\\n### Developer Mode\\n\\nThe developer mode bypasses encryption and authentication token verification.\\n\\n![REST API - Developer Mode](/img/rest-api-developer-mode.light.png#gh-light-mode-only)\\n![REST API - Developer Mode](/img/rest-api-developer-mode.dark.png#gh-dark-mode-only)\\n\\nPass `--mode=dev` to start the REST API in developer mode:\\n\\n```bash\\nvast web server --mode=dev\\n```\\n\\n### Server Mode\\n\\nThe server mode reflects the \\"traditional\\" mode of operation where VAST binds to\\na network interface. This mode only accepts HTTPS connections and requires a\\nvalid authentication token for every request. This is the default mode of\\noperation.\\n\\n![REST API - Server Mode](/img/rest-api-server-mode.light.png#gh-light-mode-only)\\n![REST API - Server Mode](/img/rest-api-server-mode.dark.png#gh-dark-mode-only)\\n\\nPass `--mode=server` to start the REST API in server mode:\\n\\n```bash\\nvast web server --mode=server\\n```\\n\\n### Upstream TLS Mode\\n\\nThe upstream TLS mode is suitable when VAST sits upstream of a separate\\nTLS terminator that is running on the same machine. This kind of setup\\nis commonly encountered when running nginx as a reverse proxy.\\n\\n![REST API - TLS Upstream Mode](/img/rest-api-tls-upstream-mode.light.png#gh-light-mode-only)\\n![REST API - TLS Upstream Mode](/img/rest-api-tls-upstream-mode.dark.png#gh-dark-mode-only)\\n\\nVAST only listens on localhost addresses, accepts plain HTTP but still\\nchecks authentication tokens.\\n\\nPass `--mode=upstream` to start the REST API in server mode:\\n\\n```bash\\nvast web server --mode=upstream\\n```\\n\\n### Mutual TLS Mode\\n\\nThe mutual TLS mode is suitable when VAST sits upstream of a separate TLS\\nterminator that may be running on a different machine. In this scenario,\\nthe connection between the terminator and VAST must again be encrypted\\nto avoid leaking the authentication token to the network.\\n\\nRegular TLS requires only the server to present a certificate to prove his\\nidentity. In mutual TLS mode, the client additionally needs to provide a\\nvalid *client certificate* to the server. This ensures that the TLS terminator\\ncannot be impersonated or bypassed.\\n\\nTypically self-signed certificates are used for that purpose, since both ends of\\nthe connection are configured together and not exposed to the public internet.\\n\\n![REST API - Mutual TLS Mode](/img/rest-api-mutual-tls-mode.light.png#gh-light-mode-only)\\n![REST API - Mutual TLS Mode](/img/rest-api-mutual-tls-mode.dark.png#gh-dark-mode-only)\\n\\nPass `--mode=mtls` to start the REST API in mutual TLS mode:\\n\\n```bash\\nvast web server --mode=mtls\\n```\\n\\n## Usage Examples\\n\\nNow that you know how we put the REST API together, let\'s look at some\\nend-to-end examples.\\n\\n### See what\'s inside VAST\\n\\nOne straightforward example is checking the number of records in VAST:\\n\\n```bash\\ncurl \\"https://vast.example.org:42001/api/v0/status?verbosity=detailed\\" \\\\\\n  | jq .index.statistics\\n```\\n\\n```json\\n{\\n  \\"events\\": {\\n    \\"total\\": 8462\\n  },\\n  \\"layouts\\": {\\n    \\"zeek.conn\\": {\\n      \\"count\\": 8462,\\n      \\"percentage\\": 100\\n    }\\n  }\\n}\\n```\\n\\n:::caution Status changes in v3.0\\nIn the upcoming v3.0 release, the statistics under the key `.index.statistics`\\nwill move to `.catalog`. This change is already merged into the master branch.\\nConsult the [status key reference](/docs/setup/monitor#reference) for details.\\n:::\\n\\n### Perform a HTTP health check\\n\\nThe `/status` endpoint can also be used as a HTTP health check in\\n`docker-compose`:\\n\\n```yaml\\nversion: \'3.4\'\\nservices:\\n  web:\\n    image: tenzir/vast\\n    environment:\\n      - \\"VAST_START__COMMANDS=web server --mode=dev\\"\\n    ports:\\n      - \\"42001:42001\\"\\n    healthcheck:\\n      test: curl --fail http://localhost:42001/status || exit 1\\n      interval: 60s\\n      retries: 5\\n      start_period: 20s\\n      timeout: 10s\\n```\\n\\n### Run a query\\n\\nThe other initial endpoints can be used to get data out of VAST. For example, to\\nget up to two `zeek.conn` events which connect to the subnet `192.168.0.0/16`, using\\nthe VAST query expression `net.src.ip in 192.168.0.0/16`:\\n\\n```bash\\ncurl \\"http://127.0.0.1:42001/api/v0/export?limit=2&expression=net.src.ip%20in%20192.168.0.0%2f16\\"\\n```\\n\\n```json\\n{\\n  \\"version\\": \\"v2.4.0-457-gb35c25d88a\\",\\n  \\"num_events\\": 2,\\n  \\"events\\": [\\n    {\\n      \\"ts\\": \\"2009-11-18T08:00:21.486539\\",\\n      \\"uid\\": \\"Pii6cUUq1v4\\",\\n      \\"id.orig_h\\": \\"192.168.1.102\\",\\n      \\"id.orig_p\\": 68,\\n      \\"id.resp_h\\": \\"192.168.1.1\\",\\n      \\"id.resp_p\\": 67,\\n      \\"proto\\": \\"udp\\",\\n      \\"service\\": null,\\n      \\"duration\\": \\"163.82ms\\",\\n      \\"orig_bytes\\": 301,\\n      \\"resp_bytes\\": 300,\\n      \\"conn_state\\": \\"SF\\",\\n      \\"local_orig\\": null,\\n      \\"missed_bytes\\": 0,\\n      \\"history\\": \\"Dd\\",\\n      \\"orig_pkts\\": 1,\\n      \\"orig_ip_bytes\\": 329,\\n      \\"resp_pkts\\": 1,\\n      \\"resp_ip_bytes\\": 328,\\n      \\"tunnel_parents\\": []\\n    },\\n    {\\n      \\"ts\\": \\"2009-11-18T08:08:00.237253\\",\\n      \\"uid\\": \\"nkCxlvNN8pi\\",\\n      \\"id.orig_h\\": \\"192.168.1.103\\",\\n      \\"id.orig_p\\": 137,\\n      \\"id.resp_h\\": \\"192.168.1.255\\",\\n      \\"id.resp_p\\": 137,\\n      \\"proto\\": \\"udp\\",\\n      \\"service\\": \\"dns\\",\\n      \\"duration\\": \\"3.78s\\",\\n      \\"orig_bytes\\": 350,\\n      \\"resp_bytes\\": 0,\\n      \\"conn_state\\": \\"S0\\",\\n      \\"local_orig\\": null,\\n      \\"missed_bytes\\": 0,\\n      \\"history\\": \\"D\\",\\n      \\"orig_pkts\\": 7,\\n      \\"orig_ip_bytes\\": 546,\\n      \\"resp_pkts\\": 0,\\n      \\"resp_ip_bytes\\": 0,\\n      \\"tunnel_parents\\": []\\n    }\\n  ]\\n}\\n```\\n\\nNote that when using `curl`, all request parameters need to be properly\\nurlencoded. This can be cumbersome for the `expression` and `pipeline`\\nparameters, so we also provide an `/export` POST endpoint that accepts\\nparameters in the JSON body. The next example shows how to use POST requests\\nfrom curl. It also uses the `/query` endpoint instead of `/export` to get\\nresults iteratively instead of a one-shot result. The cost for this is having to\\nmake two API calls instead of one:\\n\\n```bash\\ncurl -XPOST -H\\"Content-Type: application/json\\" -d\'{\\"expression\\": \\"udp\\"}\' http://127.0.0.1:42001/api/v0/query/new\\n```\\n\\n```json\\n{\\"id\\": \\"31cd0f6c-915f-448e-b64a-b5ab7aae2474\\"}\\n```\\n\\n```bash\\ncurl http://127.0.0.1:42001/api/v0/query/31cd0f6c-915f-448e-b64a-b5ab7aae2474/next?n=2 | jq\\n```\\n\\n```json\\n{\\n  \\"position\\": 0,\\n  \\"events\\": [\\n    {\\n      \\"ts\\": \\"2009-11-18T08:00:21.486539\\",\\n      \\"uid\\": \\"Pii6cUUq1v4\\",\\n      \\"id.orig_h\\": \\"192.168.1.102\\",\\n      \\"id.orig_p\\": 68,\\n      \\"id.resp_h\\": \\"192.168.1.1\\",\\n      \\"id.resp_p\\": 67,\\n      \\"proto\\": \\"udp\\",\\n      \\"service\\": null,\\n      \\"duration\\": \\"163.82ms\\",\\n      \\"orig_bytes\\": 301,\\n      \\"resp_bytes\\": 300,\\n      \\"conn_state\\": \\"SF\\",\\n      \\"local_orig\\": null,\\n      \\"missed_bytes\\": 0,\\n      \\"history\\": \\"Dd\\",\\n      \\"orig_pkts\\": 1,\\n      \\"orig_ip_bytes\\": 329,\\n      \\"resp_pkts\\": 1,\\n      \\"resp_ip_bytes\\": 328,\\n      \\"tunnel_parents\\": []\\n    },\\n    {\\n      \\"ts\\": \\"2009-11-18T08:08:00.237253\\",\\n      \\"uid\\": \\"nkCxlvNN8pi\\",\\n      \\"id.orig_h\\": \\"192.168.1.103\\",\\n      \\"id.orig_p\\": 137,\\n      \\"id.resp_h\\": \\"192.168.1.255\\",\\n      \\"id.resp_p\\": 137,\\n      \\"proto\\": \\"udp\\",\\n      \\"service\\": \\"dns\\",\\n      \\"duration\\": \\"3.78s\\",\\n      \\"orig_bytes\\": 350,\\n      \\"resp_bytes\\": 0,\\n      \\"conn_state\\": \\"S0\\",\\n      \\"local_orig\\": null,\\n      \\"missed_bytes\\": 0,\\n      \\"history\\": \\"D\\",\\n      \\"orig_pkts\\": 7,\\n      \\"orig_ip_bytes\\": 546,\\n      \\"resp_pkts\\": 0,\\n      \\"resp_ip_bytes\\": 0,\\n      \\"tunnel_parents\\": []\\n    }\\n  ]\\n}\\n```\\n\\n:::note Still Experimental\\nPlease note that we consider the API version `v0` experimental, and we make no\\nstability guarantees at the moment.\\n:::\\n\\nAs always, if you have any question on usage, swing by our [community\\nSlack](http://slack.tenzir.com). Missing routes? Let us know so that we know\\nwhat to prioritize. Now happy curling! :curling_stone:"},{"id":"/parquet-and-feather-data-engineering-woes","metadata":{"permalink":"/blog/parquet-and-feather-data-engineering-woes","source":"@site/blog/parquet-and-feather-data-engineering-woes/index.md","title":"Parquet & Feather: Data Engineering Woes","description":"Apache Arrow and [Apache","date":"2023-01-10T00:00:00.000Z","formattedDate":"January 10, 2023","tags":[{"label":"arrow","permalink":"/blog/tags/arrow"},{"label":"parquet","permalink":"/blog/tags/parquet"},{"label":"feather","permalink":"/blog/tags/feather"}],"readingTime":7.115,"hasTruncateMarker":true,"authors":[{"name":"Thomas Peiselt","title":"Data Engineer","url":"https://github.com/dispanser","email":"thomas@tenzir.com","imageURL":"https://github.com/dispanser.png","key":"dispanser"}],"frontMatter":{"title":"Parquet & Feather: Data Engineering Woes","authors":"dispanser","date":"2023-01-10T00:00:00.000Z","tags":["arrow","parquet","feather"]},"prevItem":{"title":"The New REST API","permalink":"/blog/the-new-rest-api"},"nextItem":{"title":"VAST v2.4.1","permalink":"/blog/vast-v2.4.1"}},"content":"[Apache Arrow](https://arrow.apache.org/) and [Apache\\nParquet](https://parquet.apache.org) have become the de-facto columnar formats\\nfor in-memory and on-disk representations when it comes to structured data.\\nBoth are strong together, as they provide data interoperability and foster a\\ndiverse ecosystem of data tools. But how well do they actually work together\\nfrom an engineering perspective?\\n\\n\x3c!--truncate--\x3e\\n\\nIn our previous posts, we introduced the formats and did a quantitative\\ncomparison of Parquet and Feather-on the write path. In this post, we look at\\nthe developer experience.\\n\\n:::info Parquet & Feather: 3/3\\nThis blog post is the last part of a 3-piece series on Parquet and Feather.\\n\\n1. [Enabling Open Investigations][parquet-and-feather-1]\\n2. [Writing Security Telemetry][parquet-and-feather-2]\\n3. This blog post\\n\\n[parquet-and-feather-1]: /blog/parquet-and-feather-enabling-open-investigations/\\n[parquet-and-feather-2]: /blog/parquet-and-feather-writing-security-telemetry/\\n:::\\n\\nWhile our Feather implementation proved to be straight-forward, the Parquet\\nstore implementation turned out to be more difficult. Recall that VAST has its\\nown [type system](https://vast.io/docs/understand-vast/data-model/type-system)\\nrelying on [Arrow extension\\ntypes](https://arrow.apache.org/docs/format/Columnar.html#extension-types) to\\nexpress domain-specific concepts like IP addresses, subnets, or enumerations. We\\nhit a few places where the Arrow C++ implementation does not support all VAST\\ntypes directly. It\'s trickier than we thought, as we see next.\\n\\n## Row Groups\\n\\nIn Apache Parquet, a [row group](https://parquet.apache.org/docs/concepts/) is a\\nsubset of a Parquet file that\'s itself written in a columnar fashion. Smaller\\nrow groups allow for higher granularity in reading parts of an individual file,\\nat the expense of a potentially increased file size due to less optimal\\nencoding. In VAST, we send around [batches of\\ndata](https://vast.io/docs/setup-vast/tune) that are considerably smaller than\\nwhat a recommended Parquet file size would look like. A typical Parquet file\\nsize recommendation is 1GB, which translates to 5\u201310GB of data in memory when\\nreading the entire file. To produce files sized in this order of magnitude, we\\nplanned to use individual row groups, each of which aligned with the size of our\\nArrow record batches that comprise 2<sup>16</sup> events occupying a few MBs.\\n\\nHowever, attempting to read a Parquet file that was split into multiple row\\ngroups doesn\'t work for some of our schemas, yielding:\\n\\n```\\nNotImplemented: Nested data conversions not implemented for chunked array outputs\\n```\\n\\nThis appears to be related to\\n[ARROW-5030](https://issues.apache.org/jira/browse/ARROW-5030). Our current\\nworkaround is to write a single row group, and split up the resulting Arrow\\nrecord batches into the desired size after reading. However, this increases\\nlatency to first result, an important metric for some interactive use cases we\\nenvision for VAST.\\n\\n## Arrow \u2192 Parquet \u2192 Arrow Roundtrip Schema Mismatch\\n\\nParquet is a separate project which precedes Arrow, and has its own data types,\\nwhich don\'t exactly align with what Arrow provides. While it\'s possible to\\ninstruct Arrow to also serialize [its own\\nschema](https://arrow.apache.org/docs/cpp/api/formats.html#_CPPv4N7parquet21ArrowWriterProperties7BuilderE)\\ninto the Parquet file metadata, this doesn\'t seem to play well in concert with\\nextension types. As a result, a record batch written to and then read from a\\nParquet file no longer adheres to the same schema!\\n\\nThis bit us in the following scenarios.\\n\\n### VAST Enumerations\\n\\nVAST comes with an enumeration type that represents a fixed mapping of strings\\nto numeric values, where the mapping is part of the type metadata. We represent\\nenums as extension types wrapping an Arrow dictionary of strings backed by\\nunsigned 8-bit integers. On read, Arrow turns these 8-bit index values into\\n32-bit values, which is not compatible with our extension type definition, so\\nthe extension type wrapping is lost. The diagram below illustrates this issue.\\n\\n![Arrow Schema Conversion #width500](arrow-schema-conversion.light.png#gh-light-mode-only)\\n![Arrow Schema Conversion #width500](arrow-schema-conversion.dark.png#gh-dark-mode-only)\\n\\n### Extension Types inside Maps\\n\\nBoth our address type and subnet type extensions are lost if they occur in\\nnested records. For example, a map from a VAST address to a VAST enumeration of\\nthe following Arrow type is not preserved:\\n\\n```\\nmap<extension<vast.address>, extension<vast.enumeration>>\\n```\\n\\nAfter reading it from a Parquet file, the resulting Arrow type is:\\n\\n```\\nmap<fixed_size_binary[16], string>.\\n```\\n\\nThe key, an address type, has been replaced by its physical representation,\\nwhich is 16 bytes (allowing room for an IPv6 address). Interestingly, the\\nenumeration is replaced by a string instead of a dictionary as observed in the\\nprevious paragraph. So the same type behaves differently depending on where in\\nthe schema it occurs.\\n\\nWe created an issue in the Apache JIRA to track this:\\n[ARROW-17839](https://issues.apache.org/jira/browse/ARROW-17839).\\n\\nTo fix these 3 issues, we\'re post-processing the data after reading it from\\nParquet. The workaround is a multi-step process:\\n\\n1. Side-load the Arrow schema from the Parquet metadata. This yields the actual\\n   schema, because it\'s in no way related to Parquet other than using its\\n   metadata capabilities to store it.\\n\\n1. Load the actual Arrow table. This table has its own schema, which is not the\\n   same schema as the one derived from the Parquet metadata directly.\\n\\n1. Finally, recursively walk the two schema trees with the associated data\\n   columns, and whenever there\'s a mismatch between the two, fix the data arrays\\n   by casting or transforming it, yielding a table that is aligned with the\\n   expected schema.\\n\\n   - In the first case (`dictionary` vs `vast.enumeration`) we cast the `int32`\\n     Arrow array of values into a `uint8` Arrow array, and manually create the\\n     wrapping extension type and extension array. This is relatively cheap, as\\n     casting is cheap and the wrapping is done at the array level, not the value\\n     level.\\n\\n   - In the second case (physical `binary[16]` instead of `vast.address`) we\\n     just wrap it in the appropriate extension type. Again, this is a cheap\\n     operation.\\n\\n   - The most expensive fix-up we perform is when the underlying type has been\\n     changed from an enumeration to a string: we have to create the entire array\\n     from scratch after building a lookup table that translates the string values\\n     into their corresponding numerical representation.\\n\\n## Apache Spark Support\\n\\nSo now VAST writes its data into a standardized, open format\u2014we integrate\\nseamlessly with the entire big data ecosystem, for free, right? I can read my\\nVAST database with Apache Spark and analyze security telemetry data on a\\n200-node cluster?\\n\\nNope. It\u2019s not *that* standardized. Yet. Not every tool or library supports\\nevery data type. In fact, as discussed above, writing a Parquet file and reading\\nit back *even with the same tool* doesn\'t always produce the data you started\\nwith.\\n\\nWe attempting to load a Parquet file with a single row, and a single field of\\ntype VAST\'s `count` (a 64-bit unsigned integer) into Apache Spark v3.2, we are\\ngreeted with:\\n\\n```\\norg.apache.spark.sql.AnalysisException: Illegal Parquet type: INT64 (TIMESTAMP(NANOS,false))\\n  at org.apache.spark.sql.errors.QueryCompilationErrors$.illegalParquetTypeError(QueryCompilationErrors.scala:1284)\\n```\\n\\nApache Spark v3.2 refuses to read the `import_time` field (a metadata column\\nadded by VAST itself). It turns out that Spark v3.2 has a\\n[regression](https://issues.apache.org/jira/browse/SPARK-40819). Let\'s try with\\nversion v3.1 instead, which shouldn\u2019t have this problem:\\n\\n```\\norg.apache.spark.sql.AnalysisException: Parquet type not supported: INT64 (UINT_64)\\n```\\n\\nWe got past the timestamp issue, but it still doesn\'t work: Spark only supports\\nsigned integer types, and refuses to load our Parquet file with an unsigned 64\\nbit integer value. The [related Spark JIRA\\nissue](https://issues.apache.org/jira/browse/SPARK-10113) is marked as resolved,\\nbut unfortunately the resolution is \\"a better error message.\\" However, [this\\nstack overflow post](https://stackoverflow.com/q/64383029) has the solution: if\\nwe define an explicit schema, Spark happily converts our column into a signed\\ntype.\\n\\n```scala\\nval schema = StructType(\\n  Array(\\n    StructField(\\"event\\",\\n      StructType(\\n        Array(\\n          StructField(\\"c\\", LongType))))))\\n```\\n\\nFinally, it works!\\n\\n```\\nscala> spark.read.schema(schema).parquet(<file>).show()\\n+-----+\\n|event|\\n+-----+\\n| {13}|\\n+-----+\\n```\\n\\nWe were able to read VAST data in Spark, but it\'s not an easy and out-of-the-box\\nexperience we were hoping for. It turns out that different tools don\'t always\\nsupport all the data types, and additional effort is required to integrate with\\nthe big players in the Parquet ecosystem.\\n\\n## Conclusion\\n\\nWe love Apache Arrow\u2014it\'s a cornerstone of our system, and we\'d be in much\\nworse shape without it. We use it everywhere from the storage layer (using\\nFeather and Parquet) to the data plane (where we are passing around Arrow record\\nbatches).\\n\\nHowever, as VAST uses a few less common Arrow features we sometimes stumble over\\nsome of the rougher edges. We\'re looking forward to fixing some of these things\\nupstream, but sometimes you just need a quick solution to help our users.\\n\\nThe real reason why we wrote this blog post is to show how quickly the data\\nengineering can escalate. This is the long tail that nobody wants to talk about\\nwhen telling you to build your own security data lake. And it quickly adds up!\\nIt\'s also heavy-duty data wrangling, and not ideally something you want your\\nsecurity team working on when they would be more useful hunting threats. Even\\nmore reasons to use a purpose-built security data technology like VAST."},{"id":"/vast-v2.4.1","metadata":{"permalink":"/blog/vast-v2.4.1","source":"@site/blog/vast-v2.4.1/index.md","title":"VAST v2.4.1","description":"Faster Query Taste","date":"2022-12-19T00:00:00.000Z","formattedDate":"December 19, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"feather","permalink":"/blog/tags/feather"},{"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":1.705,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.4.1","description":"Faster Query Taste","authors":"dominiklohmann","date":"2022-12-19T00:00:00.000Z","tags":["release","feather","performance"]},"prevItem":{"title":"Parquet & Feather: Data Engineering Woes","permalink":"/blog/parquet-and-feather-data-engineering-woes"},"nextItem":{"title":"VAST v2.4","permalink":"/blog/vast-v2.4"}},"content":"[VAST v2.4.1][github-vast-release] improves the performance of queries when VAST\\nis under high load, and significantly reduces the time to first result for\\nqueries with a low selectivity.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.4.1\\n\\n\x3c!--truncate--\x3e\\n\\n## Reading Feather Files Incrementally\\n\\nVAST\'s Feather store na\xefvely used the [Feather reader][feather-reader] from the\\nApache Arrow C++ library in its initial implementation. However, its API is\\nrather limited: It does not support reading record batches incrementally. We\'ve\\nswapped this out with a more efficient implementation that does.\\n\\n[feather-reader]: https://github.com/apache/arrow/blob/apache-arrow-10.0.1/cpp/src/arrow/ipc/feather.h#L57-L108\\n\\nThis is best explained visually:\\n\\n![Incremental Reads](incremental-reads-light.png#gh-light-mode-only)\\n![Incremental Reads](incremental-reads-dark.png#gh-dark-mode-only)\\n\\nWithin the scope of a single Feather store file, a single query takes the same\\namount of time overall, but there exist two distinct advantages of this\\napproach:\\n1. The first result arrives much faster at the client.\\n2. Stores do less work for cancelled queries.\\n\\nOne additional benefit that is not immediately obvious comes into play when\\nqueries arrives at multiple stores in parallel: disk reads are more evenly\\nspread out now, making them less likely to overlap between stores. For\\ndeployments with slower I/O paths this can lead to a significant query\\nperformance improvement.\\n\\nTo verify and test this, we\'ve created a VAST database with 300M Zeek events\\n(33GB on disk) from a Corelight sensor. All tests were performed on a cold start\\nof VAST, i.e., we stopped and started VAST after every repetition of each test.\\n\\nWe performed three tests:\\n1. Export a single event (20 times)\\n2. Export all events (20 times)\\n3. [Rebuild][rebuild-docs] the entire database (3 times)\\n\\n[rebuild-docs]: /docs/setup/tune#rebuild-partitions\\n\\nThe results are astonishingly good:\\n\\n|Test|Benchmark|v2.4.0|v2.4.1|Improvement|\\n|:-:|:-:|:-:|:-:|:-:|\\n|**(1)**|Avg. store load time|55.1ms|4.2ms|13.1x|\\n||Time to first result/Total time|19.8ms|14.5ms|1.4x|\\n|**(2)**|Avg. store load time|386.5ms|7.3ms|52.9x|\\n||Time to first result|69.2ms|25.4ms|2.7x|\\n||Total time|39.38s|33.30s|1.2x|\\n|**(3)**|Avg. store load time|480.3ms|9.1ms|52.7x|\\n||Total time|210.5s|198.0s|1.1x|\\n\\nIf you\'re using the Feather store backend (the default as of v2.4.0), you will\\nsee an immediate improvement with VAST v2.4.1. There are no other changes\\nbetween the two releases.\\n\\n:::info Parquet Stores\\nVAST also offers an experimental Parquet store backend, for which we plan to\\nmake a similar improvement in a coming release.\\n:::"},{"id":"/vast-v2.4","metadata":{"permalink":"/blog/vast-v2.4","source":"@site/blog/vast-v2.4/index.md","title":"VAST v2.4","description":"Open Storage","date":"2022-12-09T00:00:00.000Z","formattedDate":"December 9, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"frontend","permalink":"/blog/tags/frontend"},{"label":"feather","permalink":"/blog/tags/feather"},{"label":"parquet","permalink":"/blog/tags/parquet"},{"label":"docker","permalink":"/blog/tags/docker"},{"label":"python","permalink":"/blog/tags/python"},{"label":"arrow","permalink":"/blog/tags/arrow"}],"readingTime":4.215,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.4","description":"Open Storage","authors":"dominiklohmann","date":"2022-12-09T00:00:00.000Z","last_updated":"2023-01-10T00:00:00.000Z","tags":["release","frontend","feather","parquet","docker","python","arrow"]},"prevItem":{"title":"VAST v2.4.1","permalink":"/blog/vast-v2.4.1"},"nextItem":{"title":"Parquet & Feather: Writing Security Telemetry","permalink":"/blog/parquet-and-feather-writing-security-telemetry"}},"content":"[VAST v2.4][github-vast-release] completes the switch to open storage formats,\\nand includes an early peek at three upcoming features for VAST: A web plugin\\nwith a REST API and an integrated frontend user interface, Docker Compose\\nconfiguration files for getting started with VAST faster and showing how to\\nintegrate VAST into your SOC, and new Python bindings that will make writing\\nintegrations easier and allow for using VAST with your data science libraries,\\nlike Pandas.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.4.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Preventing Vendor Lock-in with Open Storage\\n\\nVAST\'s Apache Feather (V2) and Apache Parquet storage backends are now\\nconsidered stable, and the default storage format is now Feather. This marks the\\nbeginning of a new era for VAST for all users: There is no more vendor lock-in\\nof your data!\\n\\nBoth as engineers and users of software we disdain vendor lock-in. Your data is\\nyours and no tool should hold it hostage. We want you to choose VAST because\\nit\'s the best engine when building a sustainable security data architecture. In\\nother words, *VAST decouples data acquisition from downstream security\\nanalytics*. To this end, we are not only committed to open source, but also to\\nopen standards\u2014for storage and processing.\\n\\nAs of this release, VAST no longer supports *writing* to its old proprietary\\nstorage format, but will still support *reading* from it until the next major\\nrelease. In the background, VAST transparently rebuilds old partitions to take\\nadvantage of the new format without any downtime. This may cause some additional\\nload when starting VAST first up after the update, but ensures that queries run\\nas fast as possible once all old partitions have been converted.\\n\\nIf you want to know more about Feather and Parquet, check out our in-depth blog\\npost series on them:\\n\\n1. [Enabling Open Investigations][parquet-and-feather-1]\\n2. [Writing Security Telemetry][parquet-and-feather-2]\\n3. [Data Engineering Woes][parquet-and-feather-3]\\n\\n[parquet-and-feather-1]: /blog/parquet-and-feather-enabling-open-investigations/\\n[parquet-and-feather-2]: /blog/parquet-and-feather-writing-security-telemetry/\\n[parquet-and-feather-3]: /blog/parquet-and-feather-data-engineering-woes/\\n\\n## What\'s Next?\\n\\nVAST v2.4 contains a few new and experimental toys to play with. Here\'s an\\noverview of what they are, and how they all make it easier to integrate VAST\\nwith other security tools.\\n\\n### Docker Compose\\n\\nA new set of [Docker Compose files][docker-compose] makes it easier than ever to\\nget started with VAST. This is not designed for high-performance deployments of\\nVAST, but rather to make it easier to try VAST out\u2014all-batteries included,\\nbecause we want to use this to showcase and test the myriad of integrations\\nin a modern SOC.\\n\\nOur vision for this is to show how VAST as a modular platform can power modern\\nand sustainable approaches to composable security.\\n\\n[docker-compose]: /docs/setup/deploy/docker-compose\\n\\n### REST API and Frontend User Interface\\n\\nThe experimental `web` plugin adds a [REST API][rest-api] to VAST, and also a\\nfrontend user interface we [built in Svelte][frontend-code].\\n\\nBoth the API and the frontend are still considered unstable and subject to\\nchange without notice. We plan to stabilize and version the API in the future.\\nFundamentally, the API serves two purposes:\\n\\n1. Make it easier to write integrations with VAST\\n2. Serve as a backend for VAST\'s bundled frontend\\n\\nThe frontend UI currently displays a status page for the installed VAST node.\\n\\n\x3c!--- this weird markup is to render a border around the image ---\x3e\\n![UI showing a status page](vast-ui-experimental.jpg)\\n\\nWe have some exciting features planned for both of these. Stay tuned!\\n\\n[rest-api]: /docs/use/integrate/rest-api\\n[frontend-code]: https://github.com/tenzir/vast/tree/v2.4.0/plugins/web/ui\\n\\n### Python Bindings\\n\\nWe want to make it as easy as possible to integrate VAST with other tools, so\\nwe\'re working on making that as easy as possible using VAST\'s Python bindings.\\nThe new bindings support analyzing data from VAST using industry-standard Python\\nlibraries, like Pandas.\\n\\nThis is all enabled by our commitment to open standards: VAST leverages Apache\\nArrow as its in-memory data representation. The Python bindings make it easy to\\nuse VAST\'s security-specific data types. For example, when running a query, IP\\naddresses, subnets, and patterns automatically convert to the Python-native\\ntypes, as opposed to remaining binary blobs or sheer strings.\\n\\n:::note Not yet on PyPI\\nVAST\'s new Python bindings are not yet on PyPI, as they are still heavily under\\ndevelopment. If you\'re too eager and cannot wait, go [check out the source\\ncode][python-code].\\n:::\\n\\n[python-code]: https://github.com/tenzir/vast/tree/v2.4.0/python\\n\\n## Other Noteworthy Changes\\n\\nA full list of changes to VAST since the last release is available in the\\n[changelog][changelog-2.4]. Here\'s a selection of changes that are particularly\\nnoteworthy:\\n\\n- VAST now loads all plugins by default. When asking new users for pitfalls they\\n  encountered, this ranked pretty high on the list of things we needed to\\n  change. To revert to the old behavior, set `vast.plugins: []` in your\\n  configuration file, or set `VAST_PLUGINS=` in your environment.\\n- The default endpoint changed from `localhost` to `127.0.0.1` to ensure a\\n  deterministic listening address.\\n- Exporting VAST\'s performance metrics via UDS no longer deadlocks VAST\'s\\n  metrics exporter when a listener is suspended.\\n- VAST\'s build process now natively supports building Debian packages. This\\n  makes upgrades for bare-metal deployments a breeze. As of this release, our\\n  CI/CD pipeline automatically attaches a Debian package in addition to the\\n  build archive to our releases.\\n\\n[changelog-2.4]: /changelog#v240"},{"id":"/parquet-and-feather-writing-security-telemetry","metadata":{"permalink":"/blog/parquet-and-feather-writing-security-telemetry","source":"@site/blog/parquet-and-feather-writing-security-telemetry/index.md","title":"Parquet & Feather: Writing Security Telemetry","description":"How does Apache Parquet compare to Feather for storing","date":"2022-10-24T00:00:00.000Z","formattedDate":"October 24, 2022","tags":[{"label":"benchmark","permalink":"/blog/tags/benchmark"},{"label":"arrow","permalink":"/blog/tags/arrow"},{"label":"parquet","permalink":"/blog/tags/parquet"},{"label":"feather","permalink":"/blog/tags/feather"},{"label":"quarto","permalink":"/blog/tags/quarto"},{"label":"r","permalink":"/blog/tags/r"}],"readingTime":26.56,"hasTruncateMarker":true,"authors":[{"name":"Thomas Peiselt","title":"Data Engineer","url":"https://github.com/dispanser","email":"thomas@tenzir.com","imageURL":"https://github.com/dispanser.png","key":"dispanser"},{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Parquet & Feather: Writing Security Telemetry","authors":["dispanser","mavam"],"date":"2022-10-24T00:00:00.000Z","last_updated":"2023-01-10T00:00:00.000Z","image":"image.jpg","tags":["benchmark","arrow","parquet","feather","quarto","r"]},"prevItem":{"title":"VAST v2.4","permalink":"/blog/vast-v2.4"},"nextItem":{"title":"VAST v2.3.1","permalink":"/blog/vast-v2.3.1"}},"content":"How does Apache [Parquet](https://parquet.apache.org/) compare to [Feather](https://arrow.apache.org/docs/python/feather.html) for storing\\nstructured security data? In this blog post, we answer this question.\\n\\n\x3c!--truncate--\x3e\\n\\n:::info Parquet & Feather: 2/3\\nThis is blog post is part of a 3-piece series on Parquet and Feather.\\n\\n1.  [Enabling Open Investigations](/blog/parquet-and-feather-enabling-open-investigations/)\\n2.  This blog post\\n3.  [Data Engineering Woes](/blog/parquet-and-feather-data-engineering-woes/)\\n\\n:::\\n\\nIn the [previous blog](/blog/parquet-and-feather-enabling-open-investigations/), we explained why Parquet and\\nFeather are great building blocks for modern investigations. In this blog, we take\\na look at how they actually perform on the write path in two dimensions:\\n\\n- **Size**: how much space does typical security telemetry occupy?\\n- **Speed**: how fast can we write out to a store?\\n\\nParquet and Feather have different goals. While Parquet is an on-disk format\\nthat optimizes for size, Feather is a thin layer around the native Arrow\\nin-memory representation. This puts them at different points in the spectrum of\\nthroughput and latency.\\n\\nTo better understand this spectrum, we instrumented the write path of VAST,\\nwhich consists roughly of the following steps:\\n\\n1.  Parse the input\\n2.  Convert it into Arrow record batches\\n3.  Ship Arrow record batches to a VAST server\\n4.  Write Arrow record batches out into a Parquet or Feather store\\n5.  Create an index from Arrow record batches\\n\\nSince steps (1\u20133) and (5) are the same for both stores, we ignore them in the\\nfollowing analysis and solely zoom in on (4).\\n\\n## Dataset\\n\\nFor our evaluation, we use a dataset that models a \u201cnormal day in a corporate\\nnetwork\u201d fused with data from for real-world attacks. While this approach might\\nnot be ideal for detection engineering, it provides enough diversity to analyze\\nstorage and processing behavior.\\n\\nSpecifically, we rely on a 3.77 GB PCAP trace of the [M57 case study](https://www.sciencedirect.com/science/article/pii/S1742287612000370). We\\nalso injected real-world attacks from\\n[malware-traffic-analysis.net](https://www.malware-traffic-analysis.net/index.html) into the PCAP trace. To\\nmake the timestamps look somewhat realistic, we shifted the timestamps of the\\nPCAPs to pretend that the corresponding activity happens on the same day. For\\nthis we used [`editcap`](https://www.wireshark.org/docs/wsug_html_chunked/AppToolseditcap.html) and then merged the resulting PCAPs into one\\nbig file using [`mergecap`](https://www.wireshark.org/docs/wsug_html_chunked/AppToolsmergecap.html).\\n\\nWe then ran [Zeek](https://zeek.org) and [Suricata](https://suricata.io) over\\nthe trace to produce structured logs. For full reproducibility, we host this\\ncustom data set in a [Google Drive folder](https://drive.google.com/drive/folders/1mPJYVGKTk86P2JU3KD-WFz8tUkTLK095?usp=sharing).\\n\\nVAST can ingest PCAP, Zeek, and Suricata natively. All three data sources are\\nhighly valuable for detection and investigation, which is why we use them in\\nthis analysis. They represent a good mix of nested and structured data (Zeek &\\nSuricata) vs.\xa0simple-but-bulky data (PCAP). To give you a flavor, here\u2019s an\\nexample Zeek log:\\n\\n    #separator \\\\x09\\n    #set_separator  ,\\n    #empty_field    (empty)\\n    #unset_field    -\\n    #path   http\\n    #open   2022-04-20-09-56-45\\n    #fields ts  uid id.orig_h   id.orig_p   id.resp_h   id.resp_p   trans_depth method  host    uri referrer    version user_agent  origin  request_body_len    response_body_len   status_code status_msg  info_code   info_msg    tags    username    password    proxied orig_fuids  orig_filenames  orig_mime_types resp_fuids  resp_filenames  resp_mime_types\\n    #types  time    string  addr    port    addr    port    count   string  string  string  string  string  string  string  count   count   count   string  count   string  set[enum]   string  string  set[string] vector[string]  vector[string]  vector[string]  vector[string]  vector[string]  vector[string]\\n    1637155963.249475   CrkwBA3xeEV9dzj1n   128.14.134.170  57468   198.71.247.91   80  1   GET 198.71.247.91   /   -   1.1 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36     -   0   51  200 OK  -   -   (empty) -   -   -   -   -   -   FhEFqzHx1hVpkhWci   -   text/html\\n    1637157241.722674   Csf8Re1mi6gYI3JC6f  87.251.64.137   64078   198.71.247.91   80  1   -   -   -   -   1.1 -   -   0   18  400 Bad Request -   -   (empty) -   -   -   -   -   -   FpKcQG2BmJjEU9FXwh  -   text/html\\n    1637157318.182504   C1q1Lz1gxAAyf4Wrzk  139.162.242.152 57268   198.71.247.91   80  1   GET 198.71.247.91   /   -   1.1 Mozilla/5.0 (Windows NT 6.1; WOW64; rv:8.0) Gecko/20100101 Firefox/8.0  -   0   51  200 OK  -   -   (empty) -   -   -   -   -   -   FyTOLL1rVGzjXoNAb   -   text/html\\n    1637157331.507633   C9FzNf12ebDETzvDLk  172.70.135.112  37220   198.71.247.91   80  1   GET lifeisnetwork.com   /   -   1.1 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36 -   0   51  200 OK  -   -   (empty) -   -   X-FORWARDED-FOR -> 137.135.117.126  -   -   -   Fnmp6k1xVFoqqIO5Ub  -   text/html\\n    1637157331.750342   C9FzNf12ebDETzvDLk  172.70.135.112  37220   198.71.247.91   80  2   GET lifeisnetwork.com   /   -   1.1 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36 -   0   51  200 OK  -   -   (empty) -   -   X-FORWARDED-FOR -> 137.135.117.126  -   -   -   F1uLr1giTpXx81dP4   -   text/html\\n    1637157331.915255   C9FzNf12ebDETzvDLk  172.70.135.112  37220   198.71.247.91   80  3   GET lifeisnetwork.com   /wp-includes/wlwmanifest.xml    -   1.1 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36 -   0   279 404 Not Found   -   -   (empty) -   -   X-FORWARDED-FOR -> 137.135.117.126  -   -   -   F9dg5w2y748yNX9ZCc  -   text/html\\n    1637157331.987527   C9FzNf12ebDETzvDLk  172.70.135.112  37220   198.71.247.91   80  4   GET lifeisnetwork.com   /xmlrpc.php?rsd -   1.1 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36 -   0   279 404 Not Found   -   -   (empty) -   -   X-FORWARDED-FOR -> 137.135.117.126  -   -   -   FxzLxklm7xyuzTF8h   -   text/html\\n\\nHere\u2019s a snippet of a Suricata log:\\n\\n``` json\\n{\\"timestamp\\":\\"2021-11-17T14:32:43.262184+0100\\",\\"flow_id\\":1129058930499898,\\"pcap_cnt\\":7,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"128.14.134.170\\",\\"src_port\\":57468,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":0,\\"community_id\\":\\"1:YXWfTYEyYLKVv5Ge4WqijUnKTrM=\\",\\"http\\":{\\"hostname\\":\\"198.71.247.91\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51}}\\n{\\"timestamp\\":\\"2021-11-17T14:32:43.237882+0100\\",\\"flow_id\\":675134617085815,\\"event_type\\":\\"flow\\",\\"src_ip\\":\\"54.176.143.72\\",\\"dest_ip\\":\\"198.71.247.91\\",\\"proto\\":\\"ICMP\\",\\"icmp_type\\":8,\\"icmp_code\\":0,\\"response_icmp_type\\":0,\\"response_icmp_code\\":0,\\"flow\\":{\\"pkts_toserver\\":1,\\"pkts_toclient\\":1,\\"bytes_toserver\\":50,\\"bytes_toclient\\":50,\\"start\\":\\"2021-11-17T14:43:34.649079+0100\\",\\"end\\":\\"2021-11-17T14:43:34.649210+0100\\",\\"age\\":0,\\"state\\":\\"established\\",\\"reason\\":\\"timeout\\",\\"alerted\\":false},\\"community_id\\":\\"1:WHH+8OuOygRPi50vrH45p9WwgA4=\\"}\\n{\\"timestamp\\":\\"2021-11-17T14:32:48.254950+0100\\",\\"flow_id\\":1129058930499898,\\"pcap_cnt\\":10,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"128.14.134.170\\",\\"dest_port\\":57468,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"198.71.247.91\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":51,\\"tx_id\\":0}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:18.327585+0100\\",\\"flow_id\\":652708491465446,\\"pcap_cnt\\":206,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"139.162.242.152\\",\\"src_port\\":57268,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":0,\\"community_id\\":\\"1:gEyyy4v7MJSsjLvl+3D17G/rOIY=\\",\\"http\\":{\\"hostname\\":\\"198.71.247.91\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:8.0) Gecko/20100101 Firefox/8.0\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:18.329669+0100\\",\\"flow_id\\":652708491465446,\\"pcap_cnt\\":208,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"139.162.242.152\\",\\"dest_port\\":57268,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"198.71.247.91\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:8.0) Gecko/20100101 Firefox/8.0\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":51,\\"tx_id\\":0}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.569634+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":224,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"172.70.135.112\\",\\"src_port\\":37220,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":0,\\"community_id\\":\\"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.750383+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":226,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"172.70.135.112\\",\\"dest_port\\":37220,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":51,\\"tx_id\\":0}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.812254+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":228,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"172.70.135.112\\",\\"src_port\\":37220,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":1,\\"community_id\\":\\"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.915298+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":230,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"172.70.135.112\\",\\"dest_port\\":37220,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":51,\\"tx_id\\":1}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.977269+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":232,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"172.70.135.112\\",\\"src_port\\":37220,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":2,\\"community_id\\":\\"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/wp-includes/wlwmanifest.xml\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":404,\\"length\\":279}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.987556+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":234,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"172.70.135.112\\",\\"dest_port\\":37220,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/wp-includes/wlwmanifest.xml\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":404,\\"length\\":279},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/wp-includes/wlwmanifest.xml\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":279,\\"tx_id\\":2}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:32.049539+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":236,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"172.70.135.112\\",\\"src_port\\":37220,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":3,\\"community_id\\":\\"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/xmlrpc.php?rsd\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":404,\\"length\\":279}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:32.057985+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":238,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"172.70.135.112\\",\\"dest_port\\":37220,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/xmlrpc.php?rsd\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":404,\\"length\\":279},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/xmlrpc.php\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":279,\\"tx_id\\":3}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:32.119589+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":239,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"172.70.135.112\\",\\"src_port\\":37220,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":4,\\"community_id\\":\\"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:32.127935+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":241,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"172.70.135.112\\",\\"dest_port\\":37220,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":51,\\"tx_id\\":4}}\\n```\\n\\nNote that Zeek\u2019s tab-separated value (TSV) format is already a structured table,\\nwhereas Suricata data needs to be demultiplexed first through the `event_type`\\nfield.\\n\\nThe PCAP packet type is currently hard-coded in VAST\u2019s PCAP plugin and looks\\nlike this:\\n\\n``` go\\ntype pcap.packet = record {\\n  time: timestamp,\\n  src: addr,\\n  dst: addr,\\n  sport: port,\\n  dport: port,\\n  vlan: record {\\n    outer: count,\\n    inner: count,\\n  },\\n  community_id: string #index=hash,\\n  payload: string #skip,\\n}\\n```\\n\\nNow that we\u2019ve looked at the structure of the dataset, let\u2019s take a look at our\\nmeasurement methodology.\\n\\n### Measurement\\n\\nOur objective is understanding the storage and runtime characteristics of\\nParquet and Feather on the provided input data. To this end, we instrumented\\nVAST to produce us with a measurement trace file that we then analyze with R for\\ngaining insights. The [corresponding patch](feather-parquet-zstd-experiments.diff) is not meant for further\\nproduction, so we kept it separate. But we did find an opportunity to improve\\nVAST and [made the Zstd compression level configurable](https://github.com/tenzir/vast/pull/2623). Our [benchmark\\nscript](benchmark.fish) is available for full reproducibility.\\n\\nOur instrumentation produced a [CSV file](data.csv) with the following features:\\n\\n- **Store**: the type of store plugin used in the measurement, i.e., `parquet`\\n  or `feather`.\\n- **Construction time**: the time it takes to convert Arrow record batches into\\n  Parquet or Feather. We fenced the corresponding code blocks and computed the\\n  difference in nanoseconds.\\n- **Input size**: the number of bytes that the to-be-converted record batches\\n  consume.\\n- **Output size**: the number of bytes that the store file takes up.\\n- **Number of events**: the total number of events in all input record batches\\n- **Number of record batches**: the number Arrow record batches per store\\n- **Schema**: the name of the schema; there exists one store file per schema\\n- **Zstd compression level**: the applied Zstd compression level\\n\\nEvery row corresponds to a single store file where we varied some of these\\nparameters. We used [hyperfine](https://github.com/sharkdp/hyperfine) as\\nbenchmark driver tool, configured with 8 runs. Let\u2019s take a look at the data.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(dplyr)\\nlibrary(ggplot2)\\nlibrary(lubridate)\\nlibrary(scales)\\nlibrary(stringr)\\nlibrary(tidyr)\\n\\n# For faceting, to show clearer boundaries.\\ntheme_bw_trans <- function(...) {\\n  theme_bw(...) +\\n  theme(panel.background = element_rect(fill = \\"transparent\\"),\\n        plot.background = element_rect(fill = \\"transparent\\"),\\n        legend.key = element_rect(fill = \\"transparent\\"),\\n        legend.background = element_rect(fill = \\"transparent\\"))\\n}\\n\\ntheme_set(theme_minimal())\\n\\ndata <- read.csv(\\"data.csv\\") |>\\n  rename(store = store_type) |>\\n  mutate(duration = dnanoseconds(duration))\\n\\noriginal <- read.csv(\\"sizes.csv\\") |>\\n  mutate(store = \\"original\\", store_class = \\"original\\") |>\\n  select(store, store_class, schema, bytes)\\n\\n# Global view on number of events per schema.\\nschemas <- data |>\\n  # Pick one element from the run matrix.\\n  filter(store == \\"feather\\" & zstd.level == 1) |>\\n  group_by(schema) |>\\n  summarize(n = sum(num_events),\\n            bytes_memory = sum(bytes_memory))\\n\\n# Normalize store sizes by number of events/store.\\nnormalized <- data |>\\n  mutate(duration_normalized = duration / num_events,\\n         bytes_memory_normalized = bytes_memory / num_events,\\n         bytes_storage_normalized = bytes_in_storage / num_events,\\n         bytes_ratio = bytes_in_storage / bytes_memory)\\n\\n# Compute average over measurements.\\naggregated <- normalized |>\\n  group_by(store, schema, zstd.level) |>\\n  summarize(duration = mean(duration_normalized),\\n            memory = mean(bytes_memory_normalized),\\n            storage = mean(bytes_storage_normalized))\\n\\n# Treat in-memory measurements as just another storage type.\\nmemory <- aggregated |>\\n  filter(store == \\"feather\\" & zstd.level == 1) |>\\n  mutate(store = \\"memory\\", store_class = \\"memory\\") |>\\n  select(store, store_class, schema, bytes = memory)\\n\\n# Unite with rest of data.\\nunified <-\\n  aggregated |>\\n  select(-memory) |>\\n  mutate(zstd.level = factor(str_replace_na(zstd.level),\\n                             levels = c(\\"NA\\", \\"-5\\", \\"1\\", \\"9\\", \\"19\\"))) |>\\n  rename(bytes = storage, store_class = store) |>\\n  unite(\\"store\\", store_class, zstd.level, sep = \\"+\\", remove = FALSE)\\n\\nschemas_gt10k <- schemas |> filter(n > 10e3) |> pull(schema)\\nschemas_gt100k <- schemas |> filter(n > 100e3) |> pull(schema)\\n\\n# Only schemas with > 100k events.\\ncleaned <- unified |>\\n  filter(schema %in% schemas_gt100k)\\n\\n# Helper function to format numbers with SI unit suffixes.\\nfmt_si <- function(x) scales::label_number_si(accuracy = 0.1)(x)\\n```\\n\\n</details>\\n\\n### Schemas\\n\\nWe have a total of 42 unique schemas:\\n\\n     [1] \\"zeek.dce_rpc\\"       \\"zeek.dhcp\\"          \\"zeek.x509\\"         \\n     [4] \\"zeek.dpd\\"           \\"zeek.ftp\\"           \\"zeek.files\\"        \\n     [7] \\"zeek.ntlm\\"          \\"zeek.kerberos\\"      \\"zeek.ocsp\\"         \\n    [10] \\"zeek.ntp\\"           \\"zeek.dns\\"           \\"zeek.packet_filter\\"\\n    [13] \\"zeek.pe\\"            \\"zeek.radius\\"        \\"zeek.http\\"         \\n    [16] \\"zeek.reporter\\"      \\"zeek.weird\\"         \\"zeek.smb_files\\"    \\n    [19] \\"zeek.sip\\"           \\"zeek.smb_mapping\\"   \\"zeek.smtp\\"         \\n    [22] \\"zeek.conn\\"          \\"zeek.snmp\\"          \\"zeek.tunnel\\"       \\n    [25] \\"zeek.ssl\\"           \\"suricata.krb5\\"      \\"suricata.ikev2\\"    \\n    [28] \\"suricata.http\\"      \\"suricata.smb\\"       \\"suricata.ftp\\"      \\n    [31] \\"suricata.dns\\"       \\"suricata.fileinfo\\"  \\"suricata.tftp\\"     \\n    [34] \\"suricata.snmp\\"      \\"suricata.sip\\"       \\"suricata.anomaly\\"  \\n    [37] \\"suricata.smtp\\"      \\"suricata.dhcp\\"      \\"suricata.tls\\"      \\n    [40] \\"suricata.dcerpc\\"    \\"suricata.flow\\"      \\"pcap.packet\\"       \\n\\nThe schemas belong to three data *modules*: Zeek, Suricata, and PCAP. A module\\nis the prefix of a concrete type, e.g., for the schema `zeek.conn` the module is\\n`zeek` and the type is `conn`. This is only a distinction in terminology,\\ninternally VAST stores the full-qualified type as schema name.\\n\\nHow many events do we have per schema?\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nschemas <- normalized |>\\n  # Pick one element from the run matrix.\\n  filter(store == \\"feather\\" & zstd.level == 1) |>\\n  group_by(schema) |>\\n  summarize(n = sum(num_events),\\n            bytes_memory = sum(bytes_memory))\\n\\nschemas |>\\n  separate(schema, c(\\"module\\", \\"type\\"), remove = FALSE) |>\\n  ggplot(aes(x = reorder(schema, -n), y = n, fill = module)) +\\n    geom_bar(stat = \\"identity\\") +\\n    scale_y_log10(labels = scales::label_comma()) +\\n    labs(x = \\"Schema\\", y = \\"Number of Events\\", fill = \\"Module\\") +\\n    theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))\\n```\\n\\n</details>\\n\\nimport Svg1 from \'./index_files/figure-gfm/number-of-events-by-schema-1.svg\';\\n\\n<Svg1 />\\n\\nThe above plot (log-scaled y-axis) shows how many events we have per type.\\nBetween 1 and 100M events, we almost see everything.\\n\\nWhat\u2019s the typical event size?\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nschemas |>\\n  separate(schema, c(\\"module\\", \\"type\\"), remove = FALSE) |>\\n  ggplot(aes(x = reorder(schema, -n), y = bytes_memory / n, fill = module)) +\\n    geom_bar(stat = \\"identity\\") +\\n    guides(fill = \\"none\\") +\\n    scale_y_continuous(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    labs(x = \\"Schema\\", y = \\"Bytes (in-memory)\\", color = \\"Module\\") +\\n    theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))\\n```\\n\\n</details>\\n\\nimport Svg2 from \'./index_files/figure-gfm/event-size-by-schema-1.svg\';\\n\\n<Svg2 />\\n\\nThe above plot keeps the x-axis from the previous plot, but exchanges the y-axis\\nto show normalized event size, in memory after parsing. Most events\\ntake up a few 100 bytes, with packet data consuming a bit more, and one 5x\\noutlier: `suricata.ftp`.\\n\\nSuch distributions are normal, even with these outliers. Some telemetry events\\nsimply have more string data that\u2019s a function of user input. For `suricata.ftp`\\nspecifically, it can grow linearly with the data transmitted. Here\u2019s a stripped\\ndown example of an event that is greater than 5 kB in its raw JSON:\\n\\n``` json\\n{\\n  \\"timestamp\\": \\"2021-11-19T05:08:50.885981+0100\\",\\n  \\"flow_id\\": 1339403323589433,\\n  \\"pcap_cnt\\": 5428538,\\n  \\"event_type\\": \\"ftp\\",\\n  \\"src_ip\\": \\"10.5.5.101\\",\\n  \\"src_port\\": 50479,\\n  \\"dest_ip\\": \\"62.24.128.228\\",\\n  \\"dest_port\\": 110,\\n  \\"proto\\": \\"TCP\\",\\n  \\"tx_id\\": 12,\\n  \\"community_id\\": \\"1:kUFeGEpYT1JO1VCwF8wZWUWn0J0=\\",\\n  \\"ftp\\": {\\n    \\"completion_code\\": [\\n      \\"155\\",\\n      ...\\n      <stripped 330 lines>\\n      ...\\n      \\"188\\",\\n      \\"188\\",\\n      \\"188\\"\\n    ],\\n    \\"reply\\": [\\n      \\" 41609\\",\\n      ...\\n      <stripped 330 lines>\\n      ...\\n      \\" 125448\\",\\n      \\" 126158\\",\\n      \\" 29639\\"\\n    ],\\n    \\"reply_received\\": \\"yes\\"\\n  }\\n}\\n```\\n\\nThis matches our mental model. A few hundred bytes per event with some outliers.\\n\\n### Batching\\n\\nOn the inside, a store is a concatenation of homogeneous Arrow record batches,\\nall having the same schema.\\n\\nThe Feather format is essentially the IPC wire format of record batches. Schemas\\nand dictionaries are only included when they change. For our stores, this means\\njust once in the beginning. In order to access a given row in a Feather file,\\nyou need to start at the beginning, iterate batch by batch until you arrive at\\nthe desired batch, and then materialize it before you can access the desired\\nrow via random access.\\n\\nParquet has *row groups* that are much like a record batch, except that they are\\ncreated at write time, so Parquet determines their size rather than the incoming\\ndata. Parquet offers random access over both the row groups and within an\\nindividual batch that is materialized from a row group. The on-disk layout of\\nParquet is still row-group by row-group, and in that column by column, so\\nthere\u2019s no big difference between Parquet and Feather in that regard. Parquet\\nencodes columns using different encoding techniques than Arrow\u2019s IPC format.\\n\\nMost stores only consist of a few record batches. PCAP is the only difference.\\nSmall stores are suboptimal because the catalog keeps in-memory state that is a\\nlinear function of the number of stores. (We are aware of this concern and are\\nexploring improvements, but this topic is out of scope for this post.) The issue\\nhere is catalog fragmentation.\\n\\nAs of [v2.3](/blog/vast-v2.3), VAST has automatic rebuilding in place, which\\nmerges underfull partitions to reduce pressure on the catalog. This doesn\u2019t fix\\nthe problem of linear state, but gives us much sufficient reach for real-world\\ndeployments.\\n\\n## Size\\n\\nTo better understand the difference between Parquet and Feather, we now take a\\nlook at them right next to each other. In addition to Feather and Parquet, we\\nuse two other types of \u201cstores\u201d for the analysis to facilitate comparison:\\n\\n1.  **Original**: the size of the input prior it entered VAST, e.g., the raw JSON or\\n    a PCAP file.\\n\\n2.  **Memory**: the size of the data in memory, measured as the sum of Arrow\\n    buffers that make up the table slice.\\n\\nLet\u2019s kick of the analysis by getting a better understanding at the size\\ndistribution.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nunified |>\\n  bind_rows(original, memory) |>\\n  ggplot(aes(x = reorder(store, -bytes, FUN = \\"median\\"),\\n             y = bytes, color = store_class)) +\\n  geom_boxplot() +\\n  scale_y_log10(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n  labs(x = \\"Store\\", y = \\"Bytes/Event\\", color = \\"Store\\") +\\n  theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))\\n```\\n\\n</details>\\n\\nimport Svg3 from \'./index_files/figure-gfm/plot-schema-distribution-boxplot-1.svg\';\\n\\n<Svg3 />\\n\\nEvery boxplot corresponds to one store, with `original` and `memory` being also\\ntreated like stores. The suffix `-Z` indicates Zstd level `Z`, with `NA` meaning\\n\u201ccompression turned off\u201d entirely. Parquet stores on the right (in purple) have\\nthe smallest size, followed by Feather (red), and then their corresponding\\nin-memory (green) and original (turquoise) representation. The negative Zstd\\nlevel -5 makes Parquet actually worse than Feather.\\n\\n:::tip Analysis\\nWhat stands out is that disabling compression for Feather inflates the data\\nlarger than the original. This is not the case for Parquet. Why? Because Parquet\\nhas an orthogonal layer of compression using dictionaries. This absorbs\\ninefficiencies in heavy-tailed distributions, which are pretty standard in\\nmachine-generated data.\\n:::\\n\\nThe y-axis of above plot is log-scaled, which makes it hard for relative\\ncomparison. Let\u2019s focus on the medians (the bars in the box) only and bring the\\ny-axis to a linear scale:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nmedians <- unified |>\\n  bind_rows(original, memory) |>\\n  group_by(store, store_class) |>\\n  summarize(bytes = median(bytes))\\n\\nmedians |>\\n  ggplot(aes(x = reorder(store, -bytes), y = bytes, fill = store_class)) +\\n  geom_bar(stat = \\"identity\\") +\\n  scale_y_continuous(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n  labs(x = \\"Store\\", y = \\"Bytes/Event\\", fill = \\"Store\\") +\\n  theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))\\n```\\n\\n</details>\\n\\nimport Svg4 from \'./index_files/figure-gfm/plot-schema-distribution-medians-1.svg\';\\n\\n<Svg4 />\\n\\nTo better understand the compression in numbers, we\u2019ll anchor the original size\\nat 100% and now show the *relative* gains of Parquet and Feather:\\n\\n| Store      | Class   | Bytes/Event | Size (%) | Compression Ratio |\\n|:-----------|:--------|------------:|---------:|------------------:|\\n| parquet+19 | parquet |        53.5 |     22.7 |               4.4 |\\n| parquet+9  | parquet |        54.4 |     23.1 |               4.3 |\\n| parquet+1  | parquet |        55.8 |     23.7 |               4.2 |\\n| feather+19 | feather |        57.8 |     24.6 |               4.1 |\\n| feather+9  | feather |        66.9 |     28.4 |               3.5 |\\n| feather+1  | feather |        68.9 |     29.3 |               3.4 |\\n| parquet+-5 | parquet |        72.9 |     31.0 |               3.2 |\\n| parquet+NA | parquet |        90.8 |     38.6 |               2.6 |\\n| feather+-5 | feather |        95.8 |     40.7 |               2.5 |\\n| feather+NA | feather |       255.1 |    108.3 |               0.9 |\\n\\n:::tip Analysis\\nParquet dominates Feather with respect to space savings, but not by much for\\nhigh Zstd levels. Zstd levels \\\\> 1 do not provide substantial space savings on\\naverage, where observe a compression ratio of **\\\\~4x** over the base data. Parquet\\nstill provides a **2.6** compression ratio in the absence of compression because\\nit applies dictionary encoding.\\n\\nFeather offers competitive compression with **\\\\~3x** ratio for equal Zstd levels.\\nHowever, without compression Feather expands beyond the original dataset size at\\na compression ratio of **\\\\~0.9**.\\n:::\\n\\nThe above analysis covered averages across schemas. If we juxtapose Parquet and\\nFeather per schema, we see the difference between the two formats more clearly:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(ggrepel)\\n\\nparquet_vs_feather_size <- unified |>\\n  select(-store, -duration) |>\\n  pivot_wider(names_from = store_class,\\n              values_from = bytes,\\n              id_cols = c(schema, zstd.level))\\n\\nplot_parquet_vs_feather <- function(data) {\\n  data |>\\n    mutate(zstd.level = str_replace_na(zstd.level)) |>\\n    separate(schema, c(\\"module\\", \\"type\\"), remove = FALSE) |>\\n    ggplot(aes(x = parquet, y = feather,\\n               shape = zstd.level, color = zstd.level)) +\\n      geom_abline(intercept = 0, slope = 1, color = \\"grey\\") +\\n      geom_point(alpha = 0.6, size = 3) +\\n      geom_text_repel(aes(label = schema),\\n                color = \\"grey\\",\\n                size = 1, # font size\\n                box.padding = 0.2,\\n                min.segment.length = 0, # draw all line segments\\n                max.overlaps = Inf,\\n                segment.size = 0.2,\\n                segment.color = \\"grey\\",\\n                segment.alpha = 0.3) +\\n      scale_size(range = c(0, 10)) +\\n      labs(x = \\"Bytes (Parquet)\\", y = \\"Bytes (Feather)\\",\\n           shape = \\"Zstd Level\\", color = \\"Zstd Level\\")\\n}\\n\\nparquet_vs_feather_size |>\\n  filter(schema %in% schemas_gt100k) |>\\n  plot_parquet_vs_feather() +\\n    scale_x_log10(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_log10(labels = scales::label_bytes(units = \\"auto_si\\"))\\n```\\n\\n</details>\\n\\nimport Svg5 from \'./index_files/figure-gfm/plot-parquet-vs-feather-1.svg\';\\n\\n<Svg5 />\\n\\nIn the above log-log scatterplot, the straight line is the identity function.\\nEach point represents the median store size for a given schema. If a point is on\\nthe line, it means there is no difference between Feather and Parquet. We only\\nlook at schemas with more than 100k events to ensure that the constant factor\\ndoes not perturb the analysis. (Otherwise we end up with points *below* the\\nidentity line, which are completely dwarfed by the bulk in practice.) The color\\nand shape shows the different Zstd levels, with `NA` meaning no compression.\\nPoints clouds closer to the origin mean that the corresponding store class takes\\nup less space.\\n\\n:::tip Analysis\\nWe observe that **disabling compression hits Feather the hardest**.\\nUnexpectedly, a negative Zstd level of -5 does not compress well. The remaining\\nZstd levels are difficult to take apart visually, but it appears that the point\\nclouds form a parallel line, indicating stable compression gains. Notably,\\n**compressing PCAP packets is nearly identical with Feather and Parquet**,\\npresumably because of the low entropy and packet meta data where general-purpose\\ncompressors like Zstd shine.\\n:::\\n\\nZooming in to the bottom left area with average event size of less than 100B,\\nand removing the log scaling, we see the following:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nparquet_vs_feather_size |>\\n  filter(feather <= 100 & schema %in% schemas_gt100k) |>\\n  plot_parquet_vs_feather() +\\n    scale_x_continuous(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_continuous(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    coord_fixed()\\n```\\n\\n</details>\\n\\nimport Svg6 from \'./index_files/figure-gfm/plot-parquet-vs-feather-100-1.svg\';\\n\\n<Svg6 />\\n\\nThe respective point clouds form a parallel to the identity function, i.e., the\\ncompression ratio in this region pretty constant across schemas. There\u2019s also no\\nnoticeable difference between Zstd level 1, 9, and 19.\\n\\nIf we take pick a single point, e.g., `zeek.conn` with\\n4.7M events, we\\ncan confirm that the relative performance matches the results of our analysis\\nabove:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nunified |>\\n  filter(schema == \\"zeek.conn\\") |>\\n  ggplot(aes(x = reorder(store, -bytes), y = bytes, fill = store_class)) +\\n    geom_bar(stat = \\"identity\\") +\\n    guides(fill = \\"none\\") +\\n    labs(x = \\"Store\\", y = \\"Bytes/Event\\") +\\n    scale_y_continuous(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0)) +\\n    facet_wrap(~ schema, scales = \\"free\\")\\n```\\n\\n</details>\\n\\nimport Svg7 from \'./index_files/figure-gfm/plot-zeek-suricata-1.svg\';\\n\\n<Svg7 />\\n\\nFinally, we look at the fraction of space Parquet takes compared to Feather on a\\nper schema basis, restricted to schemas with more than 10k events:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(tibble)\\n\\nparquet_vs_feather_size |>\\n  filter(feather <= 100 & schema %in% schemas_gt10k) |>\\n  mutate(zstd.level = str_replace_na(zstd.level)) |>\\n  ggplot(aes(x = reorder(schema, -parquet / feather),\\n             y = parquet / feather,\\n             fill = zstd.level)) +\\n    geom_hline(yintercept = 1) +\\n    geom_bar(stat = \\"identity\\", position = \\"dodge\\") +\\n    labs(x = \\"Schema\\", y = \\"Parquet / Feather (%)\\", fill = \\"Zstd Level\\") +\\n    scale_y_continuous(breaks = 6:1 * 20 / 100, labels = scales::label_percent()) +\\n    theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))\\n```\\n\\n</details>\\n\\nimport Svg8 from \'./index_files/figure-gfm/plot-parquet-divided-by-feather-1.svg\';\\n\\n<Svg8 />\\n\\nThe horizontal line is similar to the identity line in the scatterplot,\\nindicating that Feather and Parquet compress equally well. The bars represent\\nthat ratio of Parquet divided by Feather. The shorter the bars, the smaller the\\nsize, so the higher the gain over Feather.\\n\\n:::tip Analysis\\nWe see that Zstd level 19 brings Parquet and Feather close together. Even at\\nZstd level 1, the median ratio of Parquet stores is **78%**, and the 3rd\\nquartile **82%**. This shows that **Feather is remarkably competitive for typical\\nsecurity analytics workloads**.\\n:::\\n\\n## Speed\\n\\nNow that we have looked at the spatial properties of Parquet and Feather, we\\ntake a look at the runtime. With *speed*, we mean the time it takes to transform\\nArrow Record Batches into Parquet and Feather format. This analysis only\\nconsiders only CPU time; VAST writes the respective store in memory first and\\nthen flushes it one sequential write. Our mental model is that Feather is faster\\nthan Parquet. Is that the case when enabling compression for both?\\n\\nTo avoid distortion of small events, we also restrict the analysis to schemas\\nwith more than 100k events.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nunified |>\\n  filter(schema %in% schemas_gt100k) |>\\n  ggplot(aes(x = reorder(store, -duration, FUN = \\"median\\"),\\n             y = duration, color = store_class)) +\\n  geom_boxplot() +\\n  scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n  theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0)) +\\n  labs(x = \\"Store\\", y = \\"Speed (us)\\", color = \\"Store\\")\\n```\\n\\n</details>\\n\\nimport Svg9 from \'./index_files/figure-gfm/duration-distribution-1.svg\';\\n\\n<Svg9 />\\n\\nThe above boxplots show the time it takes to write a store for a given store and\\ncompression level combination. The log-scaled y-axis shows the normalized to number\\nof microseconds per event, across the distribution of all schemas. The sort order\\nis the median processing time, similar to the size discussion above.\\n\\n:::tip Analysis\\nAs expected, we roughly observe an ordering according to Zstd level: more\\ncompression means a longer runtime.\\n\\nUnexpectedly, for the same Zstd level, **Parquet store creation was always\\nfaster**. Our unconfirmed hunch is that Feather compression operates on more and\\nsmaller column buffers, whereas Parquet compression only runs over the\\nconcatenated Arrow buffers, yielding bigger strides.\\n:::\\n\\nWe don\u2019t have an explanation for why disabling compression for Parquet is\\n*slower* compared Zstd levels -5 and 1. In theory, strictly less cycles are\\nspent by disabling the compression code path. Perhaps compression results in\\ndifferent memory layout that is more cache-efficient. Unfortunately, we did not\\nhave the time to dig deeper into the analysis to figure out why disabling\\nParquet compression is slower. Please don\u2019t hesitate to reach out, e.g., via our\\n[Community Slack](http://slack.tenzir.com).\\n\\nLet\u2019s compare Parquet and Feather by compression level, per schema:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nparquet_vs_feather_duration <- unified |>\\n  filter(schema %in% schemas_gt100k) |>\\n  select(-store, -bytes) |>\\n  pivot_wider(names_from = store_class,\\n              values_from = duration,\\n              id_cols = c(schema, zstd.level))\\n\\nparquet_vs_feather_duration |>\\n  mutate(zstd.level = str_replace_na(zstd.level)) |>\\n  separate(schema, c(\\"module\\", \\"type\\"), remove = FALSE) |>\\n  ggplot(aes(x = parquet, y = feather,\\n             shape = zstd.level, color = zstd.level)) +\\n    geom_abline(intercept = 0, slope = 1, color = \\"grey\\") +\\n    geom_point(alpha = 0.7, size = 3) +\\n    geom_text_repel(aes(label = schema),\\n              color = \\"grey\\",\\n              size = 1, # font size\\n              box.padding = 0.2,\\n              min.segment.length = 0, # draw all line segments\\n              max.overlaps = Inf,\\n              segment.size = 0.2,\\n              segment.color = \\"grey\\",\\n              segment.alpha = 0.3) +\\n    scale_size(range = c(0, 10)) +\\n    scale_x_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    labs(x = \\"Speed (Parquet)\\", y = \\"Speed (Feather)\\",\\n         shape = \\"Zstd Level\\", color = \\"Zstd Level\\")\\n```\\n\\n</details>\\n\\nimport Svg10 from \'./index_files/figure-gfm/pairwise-runtime-comparion-1.svg\';\\n\\n<Svg10 />\\n\\nThe above scatterplot has an identity line. Points on this line indicates that\\nthere is no speed difference between Parquet and Feather. Feather is faster for\\npoints below the line, and Parquet is faster for points above the line.\\n\\n:::tip Analysis\\nIn addition to the above boxplot, this scatterplot makes it clearer to see the\\nimpact of the schemas.\\n\\nInterestingly, **there is no significant difference in Zstd levels -5 and 1,\\nwhile levels 9 and 19 stand apart further**. Disabling compression for Feather\\nhas a stronger effect on speed than for Parquet.\\n\\nOverall, we were surprised that **Feather and Parquet are not far apart in terms\\nof write performance once compression is enabled**. Only when compression is\\ndisabled, Parquet is substantially slower in our measurements.\\n:::\\n\\n## Space-Time Trade-off\\n\\nFinally, we combine the size and speed analysis into a single benchmark. Our\\ngoal is to find an optimal parameterization, i.e., one that strictly dominates\\nothers. To this end, we now plot size against speed:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\ncleaned <- unified |>\\n  filter(schema %in% schemas_gt100k) |>\\n  mutate(zstd.level = factor(str_replace_na(zstd.level),\\n                             levels = c(\\"NA\\", \\"-5\\", \\"1\\", \\"9\\", \\"19\\"))) |>\\n  group_by(schema, store_class, zstd.level) |>\\n  summarize(bytes = median(bytes), duration = median(duration))\\n\\ncleaned |>\\n  ggplot(aes(x = bytes, y = duration,\\n             shape = store_class, color = zstd.level)) +\\n    geom_point(size = 3, alpha = 0.7) +\\n    geom_text_repel(aes(label = schema),\\n              color = \\"grey\\",\\n              size = 1, # font size\\n              box.padding = 0.2,\\n              min.segment.length = 0, # draw all line segments\\n              max.overlaps = Inf,\\n              segment.size = 0.2,\\n              segment.color = \\"grey\\",\\n              segment.alpha = 0.3) +\\n    scale_x_log10(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    labs(x = \\"Size\\", y = \\"Speed\\", shape = \\"Store\\", color = \\"Zstd Level\\")\\n```\\n\\n</details>\\n\\nimport Svg11 from \'./index_files/figure-gfm/points-by-level-1.svg\';\\n\\n<Svg11 />\\n\\nEvery point in the above log-log scatterplot represents a store with a fixed\\nschema. Since we have multiple stores for a given schema, we took the median\\nsize and median speed. We then varied the run matrix by Zstd level (color) and\\nstore type (triangle/point shape). Points closer to the origin are \u201cbetter\u201d in\\nboth dimensions. So we\u2019re looking for the left-most and bottom-most ones.\\nDisabling compression puts points into the bottom-right area, and maximum\\ncompression into the top-left area.\\n\\nThe point closest to the origin has the schema `zeek.dce_rpc` for Zstd level 1,\\nboth for Feather and Parquet. Is there anything special about this log file?\\nHere\u2019s a sample:\\n\\n    #separator \\\\x09\\n    #set_separator  ,\\n    #empty_field    (empty)\\n    #unset_field    -\\n    #path   dce_rpc\\n    #open   2022-04-20-09-56-46\\n    #fields ts  uid id.orig_h   id.orig_p   id.resp_h   id.resp_p   rtt named_pipe  endpoint    operation\\n    #types  time    string  addr    port    addr    port    interval    string  string  string\\n    1637222709.134638   Cypdo7cTBbiS4Asad   10.2.9.133  49768   10.2.9.9    135 0.000254    135 epmapper    ept_map\\n    1637222709.140898   CTDU3j3iAXfRITNiah  10.2.9.133  49769   10.2.9.9    49671   0.000239    49671   drsuapi DRSBind\\n    1637222709.141520   CTDU3j3iAXfRITNiah  10.2.9.133  49769   10.2.9.9    49671   0.000311    49671   drsuapi DRSCrackNames\\n    1637222709.142068   CTDU3j3iAXfRITNiah  10.2.9.133  49769   10.2.9.9    49671   0.000137    49671   drsuapi DRSUnbind\\n    1637222709.143104   Cypdo7cTBbiS4Asad   10.2.9.133  49768   10.2.9.9    135 0.000228    135 epmapper    ept_map\\n    1637222709.143642   CTDU3j3iAXfRITNiah  10.2.9.133  49769   10.2.9.9    49671   0.000147    49671   drsuapi DRSBind\\n    1637222709.144040   CTDU3j3iAXfRITNiah  10.2.9.133  49769   10.2.9.9    49671   0.000296    49671   drsuapi DRSCrackNames\\n\\nIt appears to be rather normal: 10 columns, several different data types, unique\\nIDs, and some short strings. By looking at the data alone, there is no obvious\\nhint that explains the performance.\\n\\nWith dozens to hundreds of different schemas per data source (sometimes even\\nmore), there it will be difficult to single out individual schemas. But a point\\ncloud is unwieldy for relative comparison. To better represent the variance of\\nschemas for a given configuration, we can strip the \u201cinner\u201d points and only look\\nat their convex hull:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\n# Native convex hull does the job, no need to leverage ggforce.\\nconvex_hull <- cleaned |>\\n  group_by(store_class, zstd.level) |>\\n  slice(chull(x = bytes, y = duration))\\n\\nconvex_hull |>\\n  ggplot(aes(x = bytes, y = duration,\\n             shape = store_class, color = zstd.level)) +\\n    geom_point(size = 3) +\\n    geom_polygon(aes(fill = zstd.level, color = zstd.level),\\n                 alpha = 0.1,\\n                 show.legend = FALSE) +\\n    scale_x_log10(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    labs(x = \\"Size\\", y = \\"Speed\\", shape = \\"Store\\", color = \\"Zstd Level\\")\\n```\\n\\n</details>\\n\\nimport Svg12 from \'./index_files/figure-gfm/convex-hull-1.svg\';\\n\\n<Svg12 />\\n\\nIntuitively, the area of a given polygon captures its variance. A smaller area\\nis \u201cgood\u201d in that it offers more predictable behavior. The high amount of\\noverlap makes it still difficult to perform clearer comparisons. If we facet by\\nstore type, it becomes easier to compare the areas:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\ncleaned |>\\n  group_by(store_class, zstd.level) |>\\n  # Native convex hull does the job, no need to leverage ggforce.\\n  slice(chull(x = bytes, y = duration)) |>\\n  ggplot(aes(x = bytes, y = duration,\\n             shape = store_class, color = store_class)) +\\n    geom_point(size = 3) +\\n    geom_polygon(aes(color = store_class, fill = store_class),\\n                 alpha = 0.3,\\n                 show.legend = FALSE) +\\n    scale_x_log10(n.breaks = 4, labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    labs(x = \\"Size\\", y = \\"Speed\\", shape = \\"Store\\", color = \\"Store\\") +\\n    facet_grid(cols = vars(zstd.level)) +\\n    theme_bw_trans()\\n```\\n\\n</details>\\n\\nimport Svg13 from \'./index_files/figure-gfm/convex-hull-facet-by-level-1.svg\';\\n\\n<Svg13 />\\n\\nArranging the facets above row-wise makes it easier to compare the y-axis, i.e.,\\nspeed, where lower polygons are better. Arranging them column-wise makes it easier\\nto compare the x-axis, i.e., size, where the left-most polygons are better:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\ncleaned |>\\n  group_by(store_class, zstd.level) |>\\n  slice(chull(x = bytes, y = duration)) |>\\n  ggplot(aes(x = bytes, y = duration,\\n             shape = zstd.level, color = zstd.level)) +\\n    geom_point(size = 3) +\\n    geom_polygon(aes(color = zstd.level, fill = zstd.level),\\n                 alpha = 0.3,\\n                 show.legend = FALSE) +\\n    scale_x_log10(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    labs(x = \\"Size\\", y = \\"Speed\\", shape = \\"Zstd Level\\", color = \\"Zstd Level\\") +\\n    facet_grid(rows = vars(store_class)) +\\n    theme_bw_trans()\\n```\\n\\n</details>\\n\\nimport Svg14 from \'./index_files/figure-gfm/convex-hull-facet-by-store-1.svg\';\\n\\n<Svg14 />\\n\\n:::tip Analysis\\nAcross both dimensions, **Zstd level 1 shows the best average space-time\\ntrade-off for both Parquet and Feather**. In the above plots, we also observe our\\nfindings from the speed analysis: Parquet still dominates when compression is\\nenabled.\\n:::\\n\\n## Conclusion\\n\\nIn summary, we set out to better understand how Parquet and Feather behave on\\nthe write path of VAST, when acquiring security telemetry from high-volume data\\nsources. Our findings show that columnar Zstd compression offers great space\\nsavings for both Parquet and Feather. For certain schemas, Feather and Parquet\\nexhibit only a marginal differences. To our surprise, writing Parquet files is\\nstill faster than Feather for our workloads.\\n\\nThe pressing next question is obviously: what about the read path, i.e., query\\nlatency? This is a topic for future, stay tuned."},{"id":"/vast-v2.3.1","metadata":{"permalink":"/blog/vast-v2.3.1","source":"@site/blog/vast-v2.3.1/index.md","title":"VAST v2.3.1","description":"VAST v2.3.1 is now available. This small bugfix release","date":"2022-10-17T00:00:00.000Z","formattedDate":"October 17, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"rebuild","permalink":"/blog/tags/rebuild"},{"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":0.225,"hasTruncateMarker":false,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"}],"frontMatter":{"title":"VAST v2.3.1","authors":"lava","date":"2022-10-17T00:00:00.000Z","tags":["release","rebuild","performance"]},"prevItem":{"title":"Parquet & Feather: Writing Security Telemetry","permalink":"/blog/parquet-and-feather-writing-security-telemetry"},"nextItem":{"title":"Parquet & Feather: Enabling Open Investigations","permalink":"/blog/parquet-and-feather-enabling-open-investigations"}},"content":"[VAST v2.3.1][github-vast-release] is now available. This small bugfix release\\naddresses an issue where [compaction][compaction] would hang if encountering\\ninvalid partitions that were produced by older versions of VAST when a large\\n`max-partition-size` was set in combination with badly compressible input data.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.3.1\\n[compaction]: https://vast.io/docs/use/transform#transform-old-data-when-reaching-storage-quota"},{"id":"/parquet-and-feather-enabling-open-investigations","metadata":{"permalink":"/blog/parquet-and-feather-enabling-open-investigations","source":"@site/blog/parquet-and-feather-enabling-open-investigations/index.md","title":"Parquet & Feather: Enabling Open Investigations","description":"Apache Parquet is the common denominator for structured data at rest.","date":"2022-10-07T00:00:00.000Z","formattedDate":"October 7, 2022","tags":[{"label":"arrow","permalink":"/blog/tags/arrow"},{"label":"parquet","permalink":"/blog/tags/parquet"},{"label":"feather","permalink":"/blog/tags/feather"}],"readingTime":5.195,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"},{"name":"Thomas Peiselt","title":"Data Engineer","url":"https://github.com/dispanser","email":"thomas@tenzir.com","imageURL":"https://github.com/dispanser.png","key":"dispanser"}],"frontMatter":{"title":"Parquet & Feather: Enabling Open Investigations","authors":["mavam","dispanser"],"date":"2022-10-07T00:00:00.000Z","last_updated":"2023-01-10T00:00:00.000Z","tags":["arrow","parquet","feather"]},"prevItem":{"title":"VAST v2.3.1","permalink":"/blog/vast-v2.3.1"},"nextItem":{"title":"A Git Retrospective","permalink":"/blog/a-git-retrospective"}},"content":"[Apache Parquet][parquet] is the common denominator for structured data at rest.\\nThe data science ecosystem has long appreciated this. But infosec? Why should\\nyou care about Parquet when building a threat detection and investigation\\nplatform? In this blog post series we share our opinionated view on this\\nquestion. In the next three blog posts, we\\n\\n1. describe how VAST uses Parquet and its little brother [Feather][feather]\\n2. benchmark the two formats against each other for typical workloads\\n3. share our experience with all the engineering gotchas we encountered along\\n   the way\\n\\n[parquet]: https://parquet.apache.org/\\n[feather]: https://arrow.apache.org/docs/python/feather.html\\n\\n\x3c!--truncate--\x3e\\n\\n:::info Parquet & Feather: 1/3\\nThis is blog post is part of a 3-piece series on Parquet and Feather.\\n\\n1. This blog post\\n2. [Writing Security Telemetry][parquet-and-feather-2]\\n3. [Data Engineering Woes][parquet-and-feather-3]\\n\\n[parquet-and-feather-2]: /blog/parquet-and-feather-writing-security-telemetry/\\n[parquet-and-feather-3]: /blog/parquet-and-feather-data-engineering-woes/\\n:::\\n\\n## Why Parquet and Feather?\\n\\nParquet is the de-facto standard for storing structured data in a format\\nconducive for analytics. Nearly all analytics engines support reading Parquet\\nfiles to load a dataset in memory for subsequent analysis.\\n\\nThe data science community has long built on this foundation, but the majority\\nof infosec tooling [does not build on an open\\nfoundation](/docs/about/vision#the-soc-architecture-maze). Too many\\nproducts hide their data behind silos, either wrapped behind a SaaS with a thin\\nAPI, or in a custom format that requires cumbersome ETL pipelines. Nearly all\\nadvanced use cases require full access to the data. Especially when\\nthe goal is developing realtime threat detection and response systems.\\n\\nSecurity is a data problem. But how should we represent that data? This is where\\nParquet enters the picture. As a vendor-agnostic storage format for structured\\nand nested data, it decouples storage from analytics. This is where SIEM\\nmonoliths fail: they offer a single black box that tightly couples data\\nacquisition and processing capabilities. Providing a thin \\"open\\" API is not really\\nopen, as it prevents high-bandwidth data access that is needed for advanced\\nanalytics workloads.\\n\\nOpen storage prevents vendor-lock-in. When any tool can work with the data, you\\nbuild a sustainable foundation for implementing future use cases. For example,\\nwith Parquet\'s column encryption, you can offload fine-grained compliance use\\ncases to a dedicated application. Want to try out a new analytics engine? Just\\npoint it to the Parquet files.\\n\\n## Parquet\'s Little Brother\\n\\n[Feather][feather] is Parquet\'s little brother. It emerged while building a\\nproof of concept for \\"fast, language-agnostic data frame storage for Python\\n(pandas) and R.\\" The format is a thin layer on top of [Arrow\\nIPC](https://arrow.apache.org/docs/python/ipc.html#ipc), making it conducive for\\nmemory mapping and zero-copy usage. On the spectrum of speed and\\nspace-efficiency, think of it this way:\\n\\n![Parquet vs. Feather](parquet-vs-feather.light.png#gh-light-mode-only)\\n![Parquet vs. Feather](parquet-vs-feather.dark.png#gh-dark-mode-only)\\n\\nBefore Feather existed, VAST had its own storage format that was 95% like\\nFeather, minus a thin framing. (We called it the *segment store*.)\\n\\nWait, but Feather is an in-memory format and Parquet an on-disk format. You\\ncannot compare them! Fair point, but don\'t forget transparent Zstd compression.\\nFor some schemas, we barely notice a difference (e.g., PCAP), whereas for\\nothers, Parquet stores boil down to a fraction of their Feather equivalent.\\n\\nThe [next blog post][parquet-and-feather-2] goes into these details. For now, we\\nwant to stress that Feather is in fact a reasonable format for data at rest,\\neven when looking at space utilization alone.\\n\\n## Parquet and Feather in VAST\\n\\nVAST can store event data as Parquet or Feather. The unit of storage scaling is\\na *partition*. In Arrow terms, a partition is a persisted form of an [Arrow\\nTable][arrow-table], i.e., a concatenation of [Record\\nBatches][arrow-record-batch]. A partition has thus a fixed schema. VAST\'s [store\\nplugin][store-plugin] determines how a partition writes its buffered record\\nbatches to disk. The diagram below illustrates the architecture:\\n\\n![Parquet Analytics](parquet-analytics.light.png#gh-light-mode-only)\\n![Parquet Analytics](parquet-analytics.dark.png#gh-dark-mode-only)\\n\\n[arrow-table]: https://arrow.apache.org/docs/python/data.html#tables\\n[arrow-record-batch]: https://arrow.apache.org/docs/python/data.html#record-batches\\n[store-plugin]: /docs/understand/architecture/plugins#store\\n\\nThis architecture makes it easy to point an analytics application directly to\\nthe store files, without the need for ETLing it into a dedicated warehouse, such\\nas Spark or Hadoop.\\n\\nThe event data thrown at VAST has quite some variety of schemas. During\\ningestion, VAST first demultiplexes the heterogeneous stream of events into\\nmultiple homogeneous streams, each of which has a unique schema. VAST buffers\\nevents until the partition hits a pre-configured event limit (e.g., 1M) or until\\na timeout occurs (e.g., 60m). Thereafter, VAST writes the partition in one shot\\nand persists it.\\n\\nThe buffering provides optimal freshness of the data, as it enables queries run\\non not-yet-persisted data. But it also sets an upper bound on the partition\\nsize, given that it must fit in memory in its entirety. In the future, we plan\\nto make this freshness trade-off explicit, making it possible to write out\\nlarger-than-memory stores incrementally.\\n\\n## Imbuing Domain Semantics\\n\\nIn a [past blog][blog-arrow] we described how VAST uses Arrow\'s extensible\\ntype system to add richer semantics to the data. This is how the value of VAST\\ntranscends through the analytics stack. For example, VAST has native IP address\\ntypes that you can show up in Python as [ipaddress][ipaddress] instance. This\\navoids friction in the data exchange process. Nobody wants to spend time\\nconverting bytes or strings into the semantic objects that are ultimately need\\nfor the analysis.\\n\\n[blog-arrow]: /blog/apache-arrow-as-platform-for-security-data-engineering\\n[ipaddress]: https://docs.python.org/3/library/ipaddress.html\\n\\nHere\'s how [VAST\'s type system](/docs/understand/data-model/type-system) looks\\nlike:\\n\\n![Type System](/img/type-system-vast.light.png#gh-light-mode-only)\\n![Type System](/img/type-system-vast.dark.png#gh-dark-mode-only)\\n\\nThere exist two major classes of types: *basic*, stateless types with a static\\nstructure and a-priori known representation, and *complex*, stateful types that\\ncarry additional runtime information. We map this type system without\\ninformation loss to Arrow:\\n\\n![Type System](/img/type-system-arrow.light.png#gh-light-mode-only)\\n![Type System](/img/type-system-arrow.dark.png#gh-dark-mode-only)\\n\\nVAST converts enum, address, and subnet types to\\n[extension-types][arrow-extension-types]. All types are self-describing and part\\nof the record batch meta data. Conversion is bi-directional. Both Parquet and\\nFeather support fully nested structures in this type system. In theory. Our\\nthird blog post in this series describes the hurdles we had to overcome to make\\nit work in practice.\\n\\n[arrow-extension-types]: https://arrow.apache.org/docs/format/Columnar.html#extension-types\\n\\nIn the next blog post, we perform a quantitative analysis of the two formats: how\\nwell do they compress the original data? How much space do they take up in\\nmemory? How much CPU time do I pay for how much space savings? In the meantime,\\nif you want to learn more about Parquet, take a look at the [blog post\\nseries][arrow-parquet-blog] from the Arrow team.\\n\\n[arrow-parquet-blog]: https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/"},{"id":"/a-git-retrospective","metadata":{"permalink":"/blog/a-git-retrospective","source":"@site/blog/a-git-retrospective/index.md","title":"A Git Retrospective","description":"The VAST project is roughly a decade old. But what happened over the last 10","date":"2022-09-15T00:00:00.000Z","formattedDate":"September 15, 2022","tags":[{"label":"git","permalink":"/blog/tags/git"},{"label":"r","permalink":"/blog/tags/r"},{"label":"quarto","permalink":"/blog/tags/quarto"},{"label":"notebooks","permalink":"/blog/tags/notebooks"},{"label":"engineering","permalink":"/blog/tags/engineering"},{"label":"open-source","permalink":"/blog/tags/open-source"}],"readingTime":4.54,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"A Git Retrospective","authors":"mavam","date":"2022-09-15T00:00:00.000Z","tags":["git","r","quarto","notebooks","engineering","open-source"]},"prevItem":{"title":"Parquet & Feather: Enabling Open Investigations","permalink":"/blog/parquet-and-feather-enabling-open-investigations"},"nextItem":{"title":"Public Roadmap and Open RFCs","permalink":"/blog/public-roadmap-and-open-rfcs"}},"content":"The VAST project is roughly a decade old. But what happened over the last 10\\nyears? This blog post looks back over time through the lens of the git *merge*\\ncommits.\\n\\nWhy merge commits? Because they represent a unit of completed contribution.\\nFeature work takes place in dedicated branches, with the merge to the main\\nbranch sealing the deal. Some feature branches have just one commit, whereas\\nothers dozens. The distribution is not uniform. As of `6f9c84198` on Sep 2,\\n2022, there are a total of 13,066 commits, with 2,334 being merges (17.9%).\\nWe\u2019ll take a deeper look at the merge commits.\\n\\n\x3c!--truncate--\x3e\\n\\n``` bash\\ncd /tmp\\ngit clone https://github.com/tenzir/vast.git\\ncd vast\\ngit checkout 6f9c841980b2333028b1ac19e2a21e99d96cbd36\\ngit log --merges --pretty=format:\\"%ad|%d\\" --date=iso-strict |\\n  sed -E \'s/(.+)\\\\|.*tag: ([^,)]+).*/\\\\1 \\\\2/\' |\\n  sed -E \'s/(.*)\\\\|.*/\\\\1 NA/\' \\\\\\n  > /tmp/vast-merge-commits.txt\\n```\\n\\nFor the statistics, we\u2019ll switch to R. In all subsequent figures, a single point\\ncorresponds to a merge commit. The reduced opacity alleviates the effects of\\noverplotting.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(dplyr)\\nlibrary(ggplot2)\\nlibrary(lubridate)\\nlibrary(readr)\\n\\ntheme_set(theme_minimal())\\n\\ndata <- read_table(\\"/tmp/vast-merge-commits.txt\\",\\n  col_names = c(\\"time\\", \\"tag\\"),\\n  col_types = \\"Tc\\"\\n) |>\\n  arrange(time) |>\\n  mutate(count = row_number())\\n\\nfirst_contribution <- \\\\(x) data |>\\n  filter(time >= x) |>\\n  pull(count) |>\\n  first()\\n\\nevents <- tribble(\\n  ~time, ~event,\\n  ymd(\\"2016-03-17\\"), \\"NSDI \'16\\\\npublication\\",\\n  ymd(\\"2017-08-31\\"), \\"Tenzir\\\\nincorporated\\",\\n  ymd(\\"2018-07-01\\"), \\"Tobias\\",\\n  ymd(\\"2019-09-15\\"), \\"Dominik\\",\\n  ymd(\\"2020-01-01\\"), \\"Benno\\",\\n  ymd(\\"2021-12-01\\"), \\"Thomas\\",\\n  ymd(\\"2022-07-01\\"), \\"Patryk\\",\\n) |>\\n  mutate(time = as.POSIXct(time), count = Vectorize(first_contribution)(time))\\n\\ndata |>\\n  ggplot(aes(x = time, y = count)) +\\n  geom_point(size = 1, alpha = 0.2) +\\n  geom_segment(\\n    data = events,\\n    aes(xend = time, yend = count + 200),\\n    color = \\"red\\"\\n  ) +\\n  geom_label(\\n    data = events,\\n    aes(y = count + 200, label = event),\\n    color = \\"red\\",\\n    size = 2\\n  ) +\\n  scale_x_datetime(date_breaks = \\"1 year\\", date_labels = \\"%Y\\") +\\n  labs(x = \\"Time\\", y = \\"Merge Commits\\")\\n```\\n\\n</details>\\n\\nimport Svg1 from \'./index_files/figure-gfm/full-time-spectrum-1.svg\';\\n\\n<Svg1 />\\n\\nPrior to Tenzir taking ownership of the project and developing VAST, it was a\\ndissertation project evolving along during PhD work at the University of\\nCalifornia, Berkeley. We can see that the first pre-submission crunch started a\\nfew months before the [NSDI \u201916\\npaper](https://matthias.vallentin.net/papers/nsdi16.pdf).\\n\\nTenzir was born in fall 2017. Real-world contributions arrived as of 2018 when\\nthe small team set sails. Throughput increased as core contributors joined the\\nteam. Fast-forward to 2020 when we started doing public releases. The figure\\nbelow shows how this process matured.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(ggrepel)\\n\\ndata |>\\n  ggplot(aes(x = time, y = count, label = tag)) +\\n  geom_point(size = 1, alpha = 0.1) +\\n  geom_text_repel(\\n    size = 2,\\n    min.segment.length = 0,\\n    max.overlaps = Inf,\\n    segment.color = \\"red\\",\\n    segment.alpha = 0.2,\\n    box.padding = 0.2\\n  ) +\\n  scale_x_datetime(\\n    date_breaks = \\"1 year\\",\\n    limits = c(as.POSIXct(ymd(\\"2020-01-01\\")), max(data$time)),\\n    date_labels = \\"%Y\\"\\n  ) +\\n  labs(x = \\"Time\\", y = \\"Merge Commits\\")\\n```\\n\\n</details>\\n\\nimport Svg2 from \'./index_files/figure-gfm/since-2020-1.svg\';\\n\\n<Svg2 />\\n\\nAs visible from the tag labels, we were at [CalVer](https://calver.org) for a\\nwhile, but ultimately switched to [SemVer](https://semver.org). Because we had\\nalready commercial users at the time, this helped us better communicate breaking\\nvs.\xa0non-breaking changes.\\n\\nLet\u2019s zoom in on all releases since v1.0. At this time, we had a solid\\nengineering and release process in place.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(tidyr)\\nv1_0_0_rc1_time <- data |>\\n  filter(tag == \\"v1.0.0-rc1\\") |>\\n  pull(time)\\n\\nsince_v1_0_0_rc1 <- data |> filter(time >= v1_0_0_rc1_time)\\n\\nrc <- since_v1_0_0_rc1 |>\\n  drop_na() |>\\n  filter(grepl(\\"rc\\", tag))\\n\\nnon_rc <- since_v1_0_0_rc1 |>\\n  drop_na() |>\\n  filter(!grepl(\\"rc\\", tag))\\n\\nsince_v1_0_0_rc1 |>\\n  ggplot(aes(x = time, y = count, label = tag)) +\\n  geom_point(size = 1, alpha = 0.2) +\\n  geom_segment(\\n    data = non_rc,\\n    aes(xend = time, yend = min(count)), color = \\"red\\"\\n  ) +\\n  geom_text_repel(\\n    size = 2,\\n    min.segment.length = 0,\\n    max.overlaps = Inf,\\n    segment.color = \\"grey\\",\\n    box.padding = 0.7\\n  ) +\\n  geom_point(\\n    data = rc, aes(x = time, y = count),\\n    color = \\"blue\\",\\n    size = 2\\n  ) +\\n  geom_point(\\n    data = non_rc, aes(x = time, y = count),\\n    color = \\"red\\",\\n    size = 2\\n  ) +\\n  geom_label(data = non_rc, aes(y = min(count)), size = 2, color = \\"red\\") +\\n  scale_x_datetime(date_breaks = \\"1 month\\", date_labels = \\"%b %y\\") +\\n  labs(x = \\"Time\\", y = \\"Merge Commits\\")\\n```\\n\\n</details>\\n\\nimport Svg3 from \'./index_files/figure-gfm/since-v1.0-1.svg\';\\n\\n<Svg3 />\\n\\nThe v2.0 release was a hard one for us, given the long distance to v1.1. We\\nmerged too much and testing took forever. Burnt by the time sunk in testing and\\nfixups, we decided to switch to an LPU model (\u201cleast publishable unit\u201d) to\\nreduce release cadence. We didn\u2019t manage to implement this model until after\\nv2.1 though, where the release cadence finally gets smaller. A monthly release\\nfeels about the right for our team size.\\n\\nThe key challenge is minimizing the feature freeze phase. The first release\\ncandidate (RC) kicks this phase off, and the final release lifts the\\nrestriction. In this period, features are not allowed to be merged.[^1] This is\\na delicate time window: too long and the fixups in the RC phase cause the\\npostponed pull requests to diverge, too short and we compromise on testing\\nrigor, causing a release that doesn\u2019t meet our Q&A requirements.\\n\\nThis is where we stand as of today. We\u2019re happy how far along we came, but\\nmany challenges still lay ahead of us. Increased automation and deeper testing\\nis the overarching theme, e.g., code coverage, fuzzing, GitOps. We\u2019re constantly\\nstriving to improve or processes. With a small team of passionate, senior\\nengineers, this is a lot of fun!\\n\\n[^1]: We enforced this with a `blocked` label. CI [doesn\u2019t allow\\n    merging](https://github.com/tenzir/vast/blob/6f9c841980b2333028b1ac19e2a21e99d96cbd36/.github/workflows/blocked.yaml) when this label is on a pull request."},{"id":"/public-roadmap-and-open-rfcs","metadata":{"permalink":"/blog/public-roadmap-and-open-rfcs","source":"@site/blog/public-roadmap-and-open-rfcs/index.md","title":"Public Roadmap and Open RFCs","description":"Open Source needs Open Governance","date":"2022-09-07T00:00:00.000Z","formattedDate":"September 7, 2022","tags":[{"label":"roadmap","permalink":"/blog/tags/roadmap"},{"label":"github","permalink":"/blog/tags/github"},{"label":"rfc","permalink":"/blog/tags/rfc"},{"label":"open-source","permalink":"/blog/tags/open-source"}],"readingTime":3.745,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Public Roadmap and Open RFCs","description":"Open Source needs Open Governance","authors":"mavam","date":"2022-09-07T00:00:00.000Z","tags":["roadmap","github","rfc","open-source"]},"prevItem":{"title":"A Git Retrospective","permalink":"/blog/a-git-retrospective"},"nextItem":{"title":"VAST v2.3","permalink":"/blog/vast-v2.3"}},"content":"We are happy to announce that we have published [our engineering\\nroadmap][roadmap] along with an [RFC process][rfc] to actively participate in\\nshaping upcoming topics. This blog post explains why and how we did it.\\n\\n[roadmap]: https://vast.io/roadmap\\n[rfc]: /docs/contribute/rfc\\n\\n\x3c!--truncate--\x3e\\n\\nAs a community-first open-source project, we constantly strive for increasing\\ntransparency. Our long-term goal is establishing a fully open governance model.\\nThis will allow for a clear delineation of the open-source project and unbiased\\ncommercial offerings that we, the engineering team behind VAST at\\n[Tenzir](https://tenzir.com), provide on top. Until we have bootstrapped\\nourselves and an active community, we aim for the right balance between open\\nand closed.\\n\\nOne step in the direction of open is publishing our [roadmap][roadmap] and\\nenabling the community to participate in the planning through an [Request For\\nComments (RFC)][rfc] process.\\n\\n## Public Roadmap\\n\\nIn the process of opening the roadmap, we had to answer several questions:\\n\\n1. **Audience**: should the content be for users only? What about developers?\\n   Should we only mention features or also refactorings?\\n\\n2. **Interaction**: should this just be a read-only page or something the\\n   community can directly interact with?\\n\\n3. **Tooling**: what is the right tool to encode the roadmap?\\n\\nLet\'s go through them one by one.\\n\\nRegarding audience, we want to avoid an overly narrow target group, as we are in\\nphase of growth where breadth instead of depth is more important. Moreover, we\\ngain more transparency if we can unveil all ongoing thrusts. Therefore, we want\\nto cover the full spectrum of personas, but make it possible for each individual\\ntype of persona to get a relevant view.\\n\\nRegarding interaction, we are actively looking for engagement. Throwing a\\nread-only version over the fence to the community is certainly informational,\\nbut we are looking for creating dialogue. Therefore, we want to allow everyone\\nto discuss the various roadmap items in the open.\\n\\nRegarding tooling, we are in need for something that integrates well with the\\nexisting environment. Our GitHub presence includes code, documentation, website\\ncontent, and third-party integrations. We also promote use of GitHub Discussions\\nto engage with us. This makes GitHub the focal point to engage with the content.\\nTherefore, we decided to encode the roadmap as GitHub issues; for clarity in a\\ndedicated repository at <https://github.com/tenzir/public-roadmap>.\\n\\nWe decided against dual-purposing the issue tracker of our main repository\\n<https://github.com/tenzir/vast> because it would add roadmap items as many\\nopen, long-running issues that scatter the attention and potentially confuse the\\ncommunity. That said, the primary value of the issue tracker is the layer on top\\nof issues: [GitHub Projects][github-projects], which allows for organizing\\nissues across multiple dimensions in a visually appealing way.\\n\\n[github-projects]: https://docs.github.com/en/issues/planning-and-tracking-with-projects\\n\\nThe quarterly board view make it easy to understand ongoing thrusts:\\n\\n[![Github Roadmap - Board](roadmap-board.jpg)][roadmap]\\n\\nThe milestones view provides a different perspective that focuses more on the\\nbigger-picture theme:\\n\\n[![Github Roadmap - Milestones](roadmap-milestones.jpg)][roadmap]\\n\\n## Open RFCs\\n\\nThe roadmap provides a lens into the short-term future. We don\'t want it to be\\njust read-only. Fundamentally, we want to build something that our users love.\\nWe also want to tap into the full potential of our enthusiasts by making it\\npossible to engage in technical depth with upcoming changes. Therefore, we are\\nestablishing a formal [Request for Comments (RFC) process][rfc].\\n\\nTo get an idea, how an RFC looks like, here\'s the [RFC template][rfc-template]:\\n\\n[rfc-template]: https://github.com/tenzir/vast/blob/master/rfc/000-template/README.md\\n\\nimport CodeBlock from \'@theme/CodeBlock\';\\nimport Template from \'!!raw-loader!@site/../rfc/000-template/README.md\';\\n\\n<CodeBlock language=\\"markdown\\">{Template}</CodeBlock>\\n\\n[RFC-001: Composable Pipelines](https://github.com/tenzir/vast/pull/2511) is an\\nexample instantiation of this template.\\n\\nThe RFC reviews take place 100% in the open. As of today, reviewers constitute\\nmembers from Tenzir\'s engineering team. Given our current resource constraints\\nand project state, we can only support a corporate-backed governance model. That\\nsaid, opening ourselves up is laying the foundation of trust and committment\\nthat we want to go beyond a walled garden. We understand that this is a long\\njourney and are excited about what\'s ahead of us.\\n\\nWhen an RFC gets accepted, it means that we put it on the roadmap, adjacent to\\nexisting items that compete for prioritization. In other words, even though we\\naccepted an RFC, there will be an indeterminate period of time until we can\\ndevote resources. We will always encourage community-led efforts and are\\nenthusiastic about supporting external projects that we can support within our\\ncapacities.\\n\\nThese are our \\"growing pains\\" that we can hopefully overcome together while\\nbuilding a thriving community. We still have our [community\\nSlack](http://slack.tenzir.com) where we are looking forward to interact with\\neveryone with questions or feedback. See you there!"},{"id":"/vast-v2.3","metadata":{"permalink":"/blog/vast-v2.3","source":"@site/blog/vast-v2.3/index.md","title":"VAST v2.3","description":"Automatic Rebuilds","date":"2022-09-01T00:00:00.000Z","formattedDate":"September 1, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"rebuild","permalink":"/blog/tags/rebuild"},{"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":3.905,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.3","description":"Automatic Rebuilds","authors":"dominiklohmann","date":"2022-09-01T00:00:00.000Z","tags":["release","rebuild","performance"]},"prevItem":{"title":"Public Roadmap and Open RFCs","permalink":"/blog/public-roadmap-and-open-rfcs"},"nextItem":{"title":"Richer Typing in Sigma","permalink":"/blog/richer-typing-in-sigma"}},"content":"[VAST v2.3][github-vast-release] is now available, which introduces an automatic\\ndata defragmentation capability.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.3.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Automatic Rebuilds\\n\\nVAST server processes now continuously rebuild partitions in the background. The\\nfollowing diagram visualizes what happens under the hood:\\n\\n![Rebuild](/img/rebuild-light.png#gh-light-mode-only)\\n![Rebuild](/img/rebuild-dark.png#gh-dark-mode-only)\\n\\nRebuilding kicks in when a partition has the following properties:\\n\\n1. **Outdated**: if a partitions does not have the latest partition version, it\\n   may not enjoy the latest features and optimizations. It makes it also faster\\n   to adopt VAST versions that include breaking changes in the storage layout.\\n   Therefore, VAST rebuilds outdated partitions to bring them into the most\\n   recent state.\\n\\n2. **Undersized**: numerous small partitions can cause fragmentation in the\\n   catalog, causing higher memory consumption, larger database footprint, and\\n   slower queries. Rebuilding merges undersized partitions, thereby\\n   defragmenting the system. This reduces the resource footprint and makes\\n   queries faster.\\n\\nTo enable automatic rebuilding, set the new `vast.automatic-rebuild` option.\\n\\n```yaml\\nvast:\\n  # Control automatic rebuilding of partitions in the background for\\n  # optimization purposes. The given number controls how many rebuilds to run\\n  # concurrently, and thus directly controls the performance vs. memory and CPU\\n  # usage trade-off. Set to 0 to disable. Defaults to 1.\\n  automatic-rebuild: 1\\n```\\n\\nNow that we have an LSM-style merge operation of partitions, we reduced\\nthe partition cutoff timeout to 5 minutes from 1 hour by default (controlled\\nthrough the option `vast.active-partition-timeout`). This reduces the risk of\\ndata loss in case of a crash. This comes in handy in particular for low-volume\\ndata sources that never exhaust their capacity.\\n\\n## Optional Partition Indexes\\n\\nHistorically, VAST evolved from a special-purpose bitmap indexing system into a\\ngeneral-purpose telemetry engine for security data. Today, VAST has a two-tiered\\nindexing architecture with sparse sketch structures at the top, followed by a\\nsecond layer of dense indexes. As of this release, it is possible to disable\\nthis second layer.\\n\\nThe space savings can be substantial based on the size of your index. For\\nexample, if the first layer of indexing always yields highly selective results,\\nthen it the dense indexes do not provide a lot of value. One scenario would be\\nretro-matching: if you only do IoC-style point queries, they will be most likely\\ncovered well by the sketches. If you do not have selective queries, the dense\\nindex is not helping much anyway, since you need access the base data anyway. A\\nreally good use case for the indexes when your have a scatterd data access\\npatterns, i.e., highly selective results *within* a partition, but a result that\\nspans many disparate partitions.\\n\\nIn a simplified model, VAST performs three steps when executing a query:\\n\\n1. Send the query to the catalog, which maintains VAST\'s partitions, and ask it\\n   for a list of candidate partitions. The catalog maintains the first tier of\\n   sparse indexes, currently one per partition.\\n\\n2. Send the query to all candidate partitions in parallel, each of which\\n   contains dense indexes for all fields in the partition\'s schema. The index\\n   lookup yields a set of candidate records IDs within the partition.\\n\\n3. Send the query to all candidate partition\'s stores, provided the index lookup\\n   yielded record IDs. Then evaluating the query against the candidate events\\n   and return the result.\\n\\nHere\'s how you can configure a partition index to be disabled:\\n\\n```yaml\\nvast:\\n  index:\\n    rules:\\n        # Don\'t create partition indexes the suricata.http.http.url field.\\n      - targets:\\n          - suricata.http.http.url\\n        partition-index: false\\n        # Don\'t create partition indexes for fields of type addr.\\n      - targets:\\n          - :ip\\n        partition-index: false\\n```\\n\\n## Improved Responsiveness Under High Load\\n\\nTwo small changes improve VAST\'s behavior under exceptionally high load.\\n\\nFirst, the new `vast.connection-timeout` option allows for modifying the default\\nclient-to-server connection timeout of 10 seconds. Previously, if a VAST server\\nwas too busy to respond to a new client within 10 seconds, the client simply\\nexited with an unintelligable `request_timeout` error message. Here\'s how you\\ncan set a custom timeout:\\n\\n```yaml\\nvast:\\n  # The timeout for connecting to a VAST server. Set to 0 seconds to wait\\n  # indefinitely.\\n  connection-timeout: 10s\\n```\\n\\nThe option is additionally available under the environment variable\\n`VAST_CONNECTION_TIMEOUT` and the `--connection-timeout` command-line option.\\n\\nSecond, we improved the operability of VAST servers under high load from\\nautomated low-priority queries. We noticed that when spawning thousands of\\nautomated retro-match queries that compaction would stall and make little\\nvisible progress, risking the disk running full or no longer being compliant\\nwith GDPR-related policies enforced by compaction.\\n\\nTo ensure that compaction\'s internal and regular user-issued queries work as\\nexpected even in this scenario, VAST now considers queries issued with\\n`--low-priority`, with even less priority compared to regular queries (down from\\n33.3% to 4%) and internal high-priority queries used for rebuilding and\\ncompaction (down from 12.5% to 1%)."},{"id":"/richer-typing-in-sigma","metadata":{"permalink":"/blog/richer-typing-in-sigma","source":"@site/blog/richer-typing-in-sigma/index.md","title":"Richer Typing in Sigma","description":"Towards Native Sigma Rule Execution","date":"2022-08-12T00:00:00.000Z","formattedDate":"August 12, 2022","tags":[{"label":"sigma","permalink":"/blog/tags/sigma"},{"label":"regex","permalink":"/blog/tags/regex"},{"label":"query-frontend","permalink":"/blog/tags/query-frontend"}],"readingTime":4.72,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Richer Typing in Sigma","description":"Towards Native Sigma Rule Execution","authors":"mavam","date":"2022-08-12T00:00:00.000Z","tags":["sigma","regex","query-frontend"]},"prevItem":{"title":"VAST v2.3","permalink":"/blog/vast-v2.3"},"nextItem":{"title":"VAST v2.2","permalink":"/blog/vast-v2.2"}},"content":"VAST\'s [Sigma frontend](/docs/understand/query-language/frontends/sigma)\\nnow supports more modifiers. In the Sigma language, modifiers transform\\npredicates in various ways, e.g., to apply a function over a value or to change\\nthe operator of a predicate. Modifiers are the customization point to enhance\\nexpressiveness of query operations.\\n\\nThe new [pySigma][pysigma] effort, which will eventually replace the\\nnow-considered-legacy [sigma][sigma] project, comes with new modifiers as well.\\nMost notably, `lt`, `lte`, `gt`, `gte` provide comparisons over value domains\\nwith a total ordering, e.g., numbers: `x >= 42`. In addition, the `cidr`\\nmodifier interprets a value as subnet, e.g., `10.0.0.0/8`. Richer typing!\\n\\n[sigma]: https://github.com/SigmaHQ/sigma\\n[pysigma]: https://github.com/SigmaHQ/pySigma\\n\\n\x3c!--truncate--\x3e\\n\\nHow does the frontend work? Think of it as a parser that processes the YAML and\\ntranslates it into an expression tree, where the leaves are predicates with\\ntyped operands according to VAST\'s data model. Here\'s how it works:\\n\\n![Sigma Query Frontend](/img/sigma-query-frontend-light.png#gh-light-mode-only)\\n![Sigma Query Frontend](/img/sigma-query-frontend-dark.png#gh-dark-mode-only)\\n\\nLet\'s take a closer look at some Sigma rule modifiers:\\n\\n```yaml\\nselection:\\n  x|re: \'f(o+|u)\'\\n  x|lt: 42\\n  x|cidr: 192.168.0.0/23\\n  x|base64offset|contains: \'http://\'\\n```\\n\\nThe `|` symbol applies a modifier to a field. Let\'s walk through the above\\nexample:\\n\\n1. The `re` modifier changes the predicate operand from `x == \\"f(o+|u)\\"` to\\n   `x == /f(o+|u)/`, i.e., the type of the right-hand side changes from `string`\\n   to `pattern`.\\n\\n2. The `lt` modifier changes the predicate operator from `==` to `<`, i.e.,\\n   `x == 42` becomes `x < 42`.\\n\\n3. The `cidr` modifier changes the predicate operand to type subnet. In VAST,\\n   parsing the operand type into a subnet happens automatically, so the Sigma\\n   frontend only changes the operator to `in`. That is, `x == \\"192.168.0.0/23\\"`\\n   becomes `x in 192.168.0.0/23`. Since VAST supports top-k prefix search on\\n   subnets natively, nothing else needs to be changed.\\n\\n   Other backends expand this to:\\n\\n   ```c\\n   x == \\"192.168.0.*\\" || x == \\"192.168.1.*\\"\\n   ```\\n\\n   This expansion logic on strings doesn\'t scale very well: for a `/22`, you\\n   would have to double the number of predicates, and for a `/21` quadruple\\n   them. This is where rich and deep typing in the language pays off.\\n\\n4. `x`: there are two modifiers that operate in a chained fashion,\\n   transforming the predicate in two steps:\\n\\n   1. Initial: `x == \\"http://\\"`\\n   2. `base64offset`: `x == \\"aHR0cDovL\\" || x == \\"h0dHA6Ly\\" || x == \\"odHRwOi8v\\"`\\n   3. `contains`: `x in \\"aHR0cDovL\\" || x in \\"h0dHA6Ly\\" || x in \\"odHRwOi8v\\"`\\n\\n   First, `base64offset` always expands a value into a disjunction of 3\\n   predicates, each of which performs an equality comparison to a\\n   Base64-transformed value.[^1]\\n\\n   Thereafter, the `contains` modifier translates the respective predicate\\n   operator from `==` to `in`. Other Sigma backends that don\'t support substring\\n   search natively transform the value instead by wrapping it into `*`\\n   wildcards, e.g., translate `\\"foo\\"` into `\\"*foo*\\"`.\\n\\n[^1]: What happens under the hood is a padding a string with spaces. [Anton\\nKutepov\'s article][sigma-article] illustrates how this works.\\n\\n[sigma-article]: https://tech-en.netlify.app/articles/en513032/index.html\\n\\nOur ultimate goal is to support a fully function executional platform for Sigma\\nrules. The table below shows the current implementation status of modifiers,\\nwhere \u2705 means implemented, \ud83d\udea7 not yet implemented but possible, and \u274c not yet\\nsupported by VAST\'s execution engine:\\n\\n|Modifier|Use|sigmac|VAST|\\n|--------|---|:----:|:--:|\\n|`contains`|perform a substring search with the value|\u2705|\u2705|\\n|`startswith`|match the value as a prefix|\u2705|\u2705|\\n|`endswith`|match the value as a suffix|\u2705|\u2705|\\n|`base64`|encode the value with Base64|\u2705|\u2705\\n|`base64offset`|encode value as all three possible Base64 variants|\u2705|\u2705\\n|`utf16le`/`wide`|transform the value to UTF16 little endian|\u2705|\ud83d\udea7\\n|`utf16be`|transform the value to UTF16 big endian|\u2705|\ud83d\udea7\\n|`utf16`|transform the value to UTF16|\u2705|\ud83d\udea7\\n|`re`|interpret the value as regular expression|\u2705|\ud83d\udea7\\n|`cidr`|interpret the value as a IP CIDR|\u274c|\u2705\\n|`all`|changes the expression logic from OR to AND|\u2705|\u2705\\n|`lt`|compare less than (`<`) the value|\u274c|\u2705\\n|`lte`|compare less than or equal to (`<=`) the value|\u274c|\u2705\\n|`gt`|compare greater than (`>`) the value|\u274c|\u2705\\n|`gte`|compare greater than or equal to (`>=`) the value|\u274c|\u2705\\n|`expand`|expand value to placeholder strings, e.g., `%something%`|\u274c|\u274c\\n\\nAside from completing the implementation of the missing modifiers, there are\\nthree missing pieces for Sigma rule execution to become viable in VAST:\\n\\n1. **Regular expressions**: VAST currently has no efficient mechanism to execute\\n   regular expressions. A regex lookup requires a full scan of the data.\\n   Moreover, the regular expression execution speed is abysimal. But we are\\n   aware of it and are working on this soon. The good thing is that the\\n   complexity of regular expression execution over batches of data is\\n   manageable, given that we would call into the corresponding [Arrow Compute\\n   function][arrow-containment-tests] for the heavy lifting. The number one\\n   challenge will be reduing the data to scan, because the Bloom-filter-like\\n   sketch data structures in the catalog cannot handle pattern types. If the\\n   sketches cannot identify a candidate set, all data needs to be scanned,\\n\\n   To alleviate the effects of full scans, it\'s possible to winnow down the\\n   candidate set of partitions by executing rules periodically. When making the\\n   windows asymptotically small, this yields effectively streaming execution,\\n   which VAST already supports in the form of \\"live queries\\".\\n\\n2. **Case-insensitive strings**: All strings in Sigma rules are case-insensitive\\n   by default, but VAST\'s string search is case-sensitive. As a workaround, we\\n   could translate Sigma strings into regular expressions, e.g., `\\"Foo\\"` into\\n   `/Foo/i`. Unfortunately there is a big performance gap between string\\n   equality search and regular expression search. We will need to find a better\\n   solution for production-grade rule execution.\\n\\n3. **Field mappings**: while Sigma rules execute already syntactically, VAST\\n   currently doesn\'t touch the field names in the rules and interprets them as\\n   [field extractors][field-extractors]. In other words, VAST doesn\'t support\\n   the Sigma taxonomy yet. Until we provide the mappings, you can already write\\n   generic Sigma rules using [concepts][concepts].\\n\\n[arrow-containment-tests]: https://arrow.apache.org/docs/cpp/compute.html#containment-tests\\n[field-extractors]: https://vast.io/docs/understand/query-language/expressions#field-extractor\\n[concepts]: https://vast.io/docs/understand/data-model/taxonomies#concepts\\n\\nPlease don\'t hesitate to swing by our [Community Slack](http://slack.tenzir.com)\\nand talk with us if you are passionate about Sigma and other topics around open\\ndetection and response."},{"id":"/vast-v2.2","metadata":{"permalink":"/blog/vast-v2.2","source":"@site/blog/vast-v2.2/index.md","title":"VAST v2.2","description":"Pipelines","date":"2022-08-05T00:00:00.000Z","formattedDate":"August 5, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"summarize","permalink":"/blog/tags/summarize"},{"label":"pipelines","permalink":"/blog/tags/pipelines"}],"readingTime":2.145,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"}],"frontMatter":{"title":"VAST v2.2","description":"Pipelines","authors":"lava","date":"2022-08-05T00:00:00.000Z","tags":["release","summarize","pipelines"]},"prevItem":{"title":"Richer Typing in Sigma","permalink":"/blog/richer-typing-in-sigma"},"nextItem":{"title":"VAST v2.1","permalink":"/blog/vast-v2.1"}},"content":"We released [VAST v2.2][github-vast-release] \ud83d\ude4c! Transforms now have a new name:\\n[pipelines](/blog/vast-v2.2#transforms-are-now-pipelines). The [summarize\\noperator](/blog/vast-v2.2#summarization-improvements) also underwent a facelift,\\nmaking aggregation functions pluggable and allowing for assigning names to\\noutput fields.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.2.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Transforms are now Pipelines\\n\\nAfter carefully reconsidering our naming decisions related to query execution\\nand data transformation, we came up with a naming convention that does a better\\njob in capturing the underlying concepts.\\n\\nMost notably, we renamed *transforms* to *pipelines*. A transform *step* is now a\\npipeline *operator*. This nomenclature is much more familiar to users coming\\nfrom dataflow and collection-based query engines. The implementation underneath\\nhasn\'t changed. As in the [Volcano model][volcano], data still flows through\\noperators, each of which consumes input from upstream operators and produces\\noutput for downstream operators. What we term a pipeline is the sequence of such\\nchained operators.\\n\\n[volcano]: https://paperhub.s3.amazonaws.com/dace52a42c07f7f8348b08dc2b186061.pdf\\n\\nWhile pipelines are not yet available at the query layer, they soon will be.\\nUntil then, you can deploy pipelines at load-time to [transform data in motion\\nor data at rest](/docs/use/transform).\\n\\nFrom a user perspective, the configuration keys associated with transforms have\\nchanged. Here\'s the updated example from our previous [VAST v1.0 release\\nblog](/blog/vast-v1.0).\\n\\n```yaml\\nvast:\\n  # Specify and name our pipelines, each of which are a list of configured\\n  # pipeline operators. Pipeline operators are plugins, enabling users to \\n  # write complex transformations in native code using C++ and Apache Arrow.\\n  pipelines:\\n     # Prevent events with certain strings to be exported, e.g., \\n     # \\"tenzir\\" or \\"secret-username\\".\\n     remove-events-with-secrets:\\n       - select:\\n           expression: \':string !in [\\"tenzir\\", \\"secret-username\\"]\'\\n\\n  # Specify whether to trigger each pipeline at server- or client-side, on\\n  # `import` or `export`, and restrict them to a list of event types.\\n  pipeline-triggers:\\n    export:\\n      # Apply the remove-events-with-secrets transformation server-side on\\n      # export to the suricata.dns and suricata.http event types.\\n      - pipeline: remove-events-with-secrets\\n        location: server\\n        events:\\n          - suricata.dns\\n          - suricata.http\\n```\\n\\n## Summarization Improvements\\n\\nIn line with the above nomenclature changes, we\'ve improved the behavior of the\\n[`summarize`][summarize] operator. It is now possible to specify an explicit\\nname for the output fields. This is helpful when the downstream processing needs\\na predictable schema. Previously, VAST took simply the name of the input field.\\nThe syntax was as follows:\\n\\n```yaml\\nsummarize:\\n  group-by:\\n    - ...\\n  aggregate:\\n    min:\\n      - ts # implied name for aggregate field\\n```\\n\\nWe now switched the syntax such that the new field name is at the beginning:\\n\\n```yaml\\nsummarize:\\n  group-by:\\n    - ...\\n  aggregate:\\n    ts_min: # explicit name for aggregate field\\n      min: ts\\n```\\n\\nIn SQL, this would be the `AS` token: `SELECT min(ts) AS min_ts`.\\n\\n[summarize]: /docs/understand/query-language/operators/summarize"},{"id":"/vast-v2.1","metadata":{"permalink":"/blog/vast-v2.1","source":"@site/blog/vast-v2.1/index.md","title":"VAST v2.1","description":"VAST v2.1 - Tune VAST Databases","date":"2022-07-07T00:00:00.000Z","formattedDate":"July 7, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":3.935,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.1","description":"VAST v2.1 - Tune VAST Databases","authors":"dominiklohmann","date":"2022-07-07T00:00:00.000Z","tags":["release","performance"]},"prevItem":{"title":"VAST v2.2","permalink":"/blog/vast-v2.2"},"nextItem":{"title":"Apache Arrow as Platform for Security Data Engineering","permalink":"/blog/apache-arrow-as-platform-for-security-data-engineering"}},"content":"[VAST v2.1][github-vast-release] is out! This release comes with a particular\\nfocus on performance and reducing the size of VAST databases. It brings a new\\nutility for optimizing databases in production, allowing existing deployments to\\ntake full advantage of the improvements after upgrading.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.1.0\\n\\n\x3c!--truncate--\x3e\\n\\n## New Project Site\\n\\nVAST has new project site: [vast.io](https://vast.io). We ported all\\ndocumentation from `https://docs.tenzir.com`, added a lot of new content, and\\nrestructured the reading experience along the user journey.\\n\\nYou can find the Threat Bus documentation in [Use VAST \u2192 Integrate \u2192 Threat\\nBus](/docs/use/integrate/threatbus). Threat Bus is now officially in\\nmaintainance mode: we are only supporting existing features with bugfixes. That\\nsaid, Threat Bus will resurface in a new shape with its existing functionality\\nintegrated into VAST itself. Stay tuned.\\n\\n## Performance Improvements\\n\\nVAST now compresses data with [Zstd](http://www.zstd.net). The default\\nconfiguration achieves over 2x space savings. When transferring data between\\nclient and server processes, compression reduces the amount of transferred data\\nby up to 5x.\\n\\nAdditionally, VAST now compresses on-disk indexes with Zstd, resulting in a\\n50-80% size reduction depending on the type of indexes used.\\n\\nThis allowed us to increase the default partition size from 1,048,576 to\\n4,194,304 events[^1], and the default number of events in a single batch from 1,024\\nto 65,536, resulting in a massive performance increase at the cost of a ~20%\\nlarger memory footprint at peak loads. Use the option `vast.max-partition-size`\\nto tune this space-time tradeoff.\\n\\nTo benchmark this, we used [`speeve`][speeve] to generate 20 EVE JSON files\\ncontaining 8,388,608 events each[^2]. We spawned a VAST server process and ran\\n20 VAST client processes in parallel, with one process per file.\\n\\nWe observed a reduction of **up to 73%** of disk space utilization:\\n\\n![Database Size](storage-light.png#gh-light-mode-only)\\n![Database Size](storage-dark.png#gh-dark-mode-only)\\n\\nIn addition, we were able to scale the ingest rate by almost **6x** due to the\\nhigher batch size and the reduced memory usage per batch:\\n\\n![Ingest Rate](rate-light.png#gh-light-mode-only)\\n![Ingest Rate](rate-dark.png#gh-dark-mode-only)\\n\\nThe table below summaries the benchmarks:\\n\\n||VAST v2.0|VAST v2.1|Change|\\n|-:|:-|:-|:-|\\n|Ingest Duration|1,650 s|242 s|-85.3%|\\n|Ingest Rate|101,680 events/s|693,273 events/s|+581.8%|\\n|Index Size|14,791 MiB|5,721 MiB|-61.3%|\\n|Store Size|37,656 MiB|8,491 MiB|-77.5%|\\n|Database Size|52,446 MiB|14,212 MiB|-72.9%|\\n\\n:::note Compressed Filesystems\\nThe above benchmarks ran on filesystems without compression. We expect the gain\\nfrom compression to be smaller when using compressed filesystems like\\n[`btrfs`][btrfs].\\n:::\\n\\n[speeve]: https://github.com/satta/speeve\\n[btrfs]: https://btrfs.wiki.kernel.org/index.php/Main_Page\\n\\n[^1]: VAST v2.0 failed to write its partitions to disk with the defaults for\\n  v2.1 because the on-disk size exceeded the maximum possible size of a\\n  FlatBuffers table, which VAST internally uses to have an open standard for its\\n  persistent state.\\n[^2]: This resulted in 167,772,160 events, with a total of 200\'917\'930 unique\\n  values with a schema distribution of 80.74% `suricata.flow`, 7.85%\\n  `suricata.dns`, 5.35% `suricata.http`, 4.57% `suricata.fileinfo`, 1.04%\\n  `suricata.tls`, 0.41% `suricata.ftp`, and 0.04% `suricata.smtp`.\\n\\n## Rebuild VAST Databases\\n\\nThe new changes to VAST\'s internal data format only apply to newly ingested\\ndata. To retrofit changes, we introduce a new `rebuild` command with this\\nrelease. A rebuild effectively re-ingests events from existing partitions and\\natomically replaces them with partitions of the new format.\\n\\nThis makes it possible to upgrade persistent state to a newer version, or\\nrecreate persistent state after changing configuration parameters, e.g.,\\nswitching from the Feather to the Parquet store backend (that will land in\\nv2.2). Rebuilding partitions also recreates their sparse indexes that\\naccellerate query execution. The process takes place asynchronously in the\\nbackground.\\n\\nWe recommend running `vast rebuild` to upgrade your VAST v1.x partitions to VAST\\nv2.x partitions to take advantage of the new compression and an improved\\ninternal representation.\\n\\nThis is how you run it:\\n\\n```bash\\nvast rebuild [--all] [--undersized] [--parallel=<number>] [<expression>]\\n```\\n\\nA rebuild is not only useful when upgrading outdated partitions, but also when\\nchanging parameters of up-to-date partitions. Use the `--all` flag to extend a\\nrebuild operation to _all_ partitions. (Internally, VAST versions the partition\\nstate via FlatBuffers. An outdated partition is one whose version number is not\\nthe newest.)\\n\\nThe `--undersized` flag causes VAST to only rebuild partitions that are under\\nthe configured partition size limit `vast.max-partition-size`.\\n\\nThe `--parallel` options is a performance tuning knob. The parallelism level\\ncontrols how many sets of partitions to rebuild in parallel. This value defaults\\nto 1 to limit the CPU and memory requirements of the rebuilding process, which\\ngrow linearly with the selected parallelism level.\\n\\nAn optional expression allows for restricting the set of partitions to rebuild.\\nVAST performs a catalog lookup with the expression to identify the set of\\ncandidate partitions. This process may yield false positives, as with regular\\nqueries, which may cause unaffected partitions to undergo a rebuild. For\\nexample, to rebuild outdated partitions containing `suricata.flow` events\\nolder than 2 weeks, run the following command:\\n\\n```bash\\nvast rebuild \'#type == \\"suricata.flow\\" && #import_time < 2 weeks ago\'\\n```"},{"id":"/apache-arrow-as-platform-for-security-data-engineering","metadata":{"permalink":"/blog/apache-arrow-as-platform-for-security-data-engineering","source":"@site/blog/apache-arrow-as-platform-for-security-data-engineering/index.md","title":"Apache Arrow as Platform for Security Data Engineering","description":"How VAST leverages Apache Arrow for Security Data Engineering","date":"2022-06-17T00:00:00.000Z","formattedDate":"June 17, 2022","tags":[{"label":"architecture","permalink":"/blog/tags/architecture"},{"label":"arrow","permalink":"/blog/tags/arrow"},{"label":"performance","permalink":"/blog/tags/performance"},{"label":"query","permalink":"/blog/tags/query"}],"readingTime":6.05,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"description":"How VAST leverages Apache Arrow for Security Data Engineering","authors":"mavam","date":"2022-06-17T00:00:00.000Z","tags":["architecture","arrow","performance","query"]},"prevItem":{"title":"VAST v2.1","permalink":"/blog/vast-v2.1"},"nextItem":{"title":"VAST v2.0","permalink":"/blog/vast-v2.0"}},"content":"VAST bets on [Apache Arrow][arrow] as the open interface to structured data. By\\n\\"bet,\\" we mean that VAST does not work without Arrow. And we are not alone.\\nInflux\'s [IOx][iox], DataDog\'s [Husky][husky], Anyscale\'s [Ray][ray],\\n[TensorBase][tensorbase], and [others][arrow-projects] committed themselves to\\nmaking Arrow a corner stone of their system architecture. For us, Arrow was not\\nalways a required dependency. We shifted to a tighter integration over the years\\nas the Arrow ecosystem matured. In this blog post we explain our journey of\\nbecoming an Arrow-native engine.\\n\\n[arrow]: https://arrow.apache.org\\n[iox]: https://github.com/influxdata/influxdb_iox\\n[husky]: https://www.datadoghq.com/blog/engineering/introducing-husky/\\n[ray]: https://github.com/ray-project/ray\\n[tensorbase]: https://github.com/tensorbase/tensorbase\\n[arrow-projects]: https://arrow.apache.org/powered_by/\\n\\n\x3c!--truncate--\x3e\\n\\nToday, the need to bring advanced security analytics and data engineering\\ntogether is stronger than ever, but there is a huge gap between the two fields.\\nWe see Arrow as the vehicle to close this gap, allowing us developers to\\npractice *security data engineering* to make security analytics easy for users.\\nThat is, the experience should allow experts to interact with the data in the\\nsecurity domain, end-to-end without context switching. To achieve this, we began\\nour journey with VAST by developing a data model for structured security\\ntelemetry. Having worked for a decade with the [Zeek][zeek] (fka. Bro) network\\nsecurity monitor, we understood the value of having first-class support for\\ndomain-specific entities (e.g., native representation of IPv4 and IPv6\\naddresses) and type-specific operations (e.g., the ability to perform top-k\\nprefix search to answer subnet membership queries). In addition, the ability to\\nembed domain semantics with user-defined types (e.g., IP addresses, subnets, and\\nURLs) was central to expressing complex relationships to develop effective\\nanalytical models. It was clear that we needed the domain model deep in the core\\nof the system to successfully support security analytics.\\n\\nAfter having identified the data model requirements, the question of\\nrepresentation came next. At first, we unified the internal representation with\\na row-oriented representation using [MsgPack][msgpack], which comes with a\\nmechanism for adding custom types. The assumption was that a row-based data\\nrepresentation more closely matches typical event data (e.g., JSONL) and\\ntherefore allows for much higher processing rates. Moreover, early use cases of\\nVAST were limited to interactive, multi-dimensional search to extract a subset\\nof *entire* records, spread over a longitudinal archive of data. The\\nrow-oriented encoding worked well for this.\\n\\nBut as security operations were maturing, requirements extended to analytical\\nprocessing of structured data, making a columnar format increasingly beneficial.\\nAfter having witnessed first-hand the early commitment of [Ray][ray] to Arrow,\\nwe started using Arrow as optional dependency as additional column-oriented\\nencoding. We abstracted a batch of data encoding-independent behind a \\"table\\nslice\\":\\n\\n![MsgPack & Arrow](msgpack-arrow.light.png#gh-light-mode-only)\\n![MsgPack & Arrow](msgpack-arrow.dark.png#gh-dark-mode-only)\\n\\nHiding the concrete encoding behind a cell-based access interface worked for\\nlow-volume use cases, but backfired as we scaled up and slowed us down\\nsubstantially in development. We needed to make a choice. This is where timing\\nwas right: our perception of the rapidly evolving Arrow ecosystem changed.\\nArrow-based runtimes were mushrooming all over the place. Nowadays it requires\\nonly a few lines of code to integrate Arrow data into the central logic of\\napplications. We realized that the primary value proposition of Arrow is to\\n*make data interoperability easy*.\\n\\nBut data interoperability is only a sufficient condition for enabling\\nsustainable security analytics. The differentiating value of a *security* data\\nplatform is support for the *security* domain. This is where Arrow\'s [extension\\ntypes][extension-types] come into play. They add *semantics* to otherwise\\ngeneric types, e.g., by telling the user \\"this is a transport-layer port\\" and\\nnot just a 16-bit unsigned integer, or \\"this is a connection 4-tuple to\\nrepresent a network flow\\" instead of \\"this is a record with 4 fields of type\\nstring and unsigned integer\\". Extension types are composable and allow for\\ncreating a rich typing layer with meaningful domain objects on top of a\\nstandardized data representation. Since they are embedded in the data, they do\\nnot have to be made available out-of-band when crossing the boundaries of\\ndifferent tools. Now we have self-describing security data.\\n\\nInteroperability plus support for a domain-specific data model makes Arrow a\\nsolid *data plane*. It turns out that Arrow is much more than a standardized\\ndata representation. Arrow also comes with bag of tools for working with the\\nstandardized data. In the diagram below, we show the various Arrow pieces that\\npower the architecture of VAST:\\n\\n![Arrow Data Plane](arrow-data-plane.light.png#gh-light-mode-only)\\n![Arrow Data Plane](arrow-data-plane.dark.png#gh-dark-mode-only)\\n\\nIn the center we have the Arrow data plane that powers other parts of the\\nsystem. Green elements highlight Arrow building blocks that we use today, and\\norange pieces elements we plan to use in the future. There are several aspects\\nworth pointing out:\\n\\n1. **Unified Data Plane**: When users ingest data into VAST, the\\n   parsing process converts the native data into Arrow. Similarly, a\\n   conversation boundary exists when data leaves the system, e.g., when a user\\n   wants a query result shown in JSON, CSV, or some custom format. Source and\\n   sink data formats are [exchangeable\\n   plugins](/docs/understand/architecture/plugins).\\n\\n2. **Read/Write Path Separation**: one design goal of VAST is a strict\\n   separation of read and write path, in order to scale them independently. The\\n   write path follows a horizontally scalable architecture where builders (one per\\n   schema) turn the in-memory record batches into a persistent representation.\\n   VAST currently has support for Parquet and Feather.\\n\\n3. **Pluggable Query Engine**: VAST has live/continuous queries that simply run\\n   over the stream of incoming data, and historical queries that operate on\\n   persistent data. The harboring execution engine is something we are about to\\n   make pluggable. The reason is that VAST runs in extremely different\\n   environments, from cluster to edge. Query engines are usually optimized for a\\n   specific use case, so why not use the best engine for the job at hand? Arrow\\n   makes this possible. [DuckDB][duckdb] and [DataFusion][datafusion] are great\\n   example of embeddable query engines.\\n\\n4. **Unified Control Plane**: to realize a pluggable query engine, we also need\\n   a standardized control plane. This is where [Substrait][substrait] and\\n   [Flight][flight] come into play. Flight for communication and Substrait as\\n   canonical query representation. We already experimented with Substrait,\\n   converting VAST queries into a logical query plan. In fact, VAST has a \\"query\\n   language\\" plugin to make it possible to translate security content. (For\\n   example, our [Sigma plugin][sigma-plugin] translates [Sigma rules][sigma]\\n   into VAST queries.) In short: Substrait is to the control plane what Arrow is\\n   to the data plane. Both are needed to modularize the concept of a query\\n   engine.\\n\\nMaking our own query engine more suitable for analytical workloads has\\nreceived less attention in the past, as we prioritized high-performance data\\nacquisition, low-latency search, in-stream matching using [Compute][compute],\\nand expressiveness of the underlying domain data model. We did so because VAST\\nmust run robustly in production on numerous appliances all over the world in a\\nsecurity service provider setting, with confined processing and storage where\\nefficiency is key.\\n\\nMoving forward, we are excited to bring more analytical horse power to the\\nsystem, while opening up the arena for third-party engines. With the bag of\\ntools from the Arrow ecosystem, plus all other embeddable Arrow engines that are\\nemerging, we have a modular architecture to can cover a very wide spectrum of\\nuse cases.\\n\\n[compute]: https://arrow.apache.org/docs/cpp/compute.html\\n[extension-types]: https://arrow.apache.org/docs/format/Columnar.html#extension-types\\n[flight]: https://arrow.apache.org/docs/format/Flight.html\\n[substrait]: https://substrait.io/\\n[datafusion]: https://arrow.apache.org/datafusion/\\n[datafusion-c]: https://github.com/datafusion-contrib/datafusion-c\\n[msgpack]: https://msgpack.org/index.html\\n[duckdb]: https://duckdb.org/\\n[sigma]: https://github.com/SigmaHQ/sigma\\n[sigma-plugin]: /docs/understand/query-language/frontends/sigma\\n[zeek]: https://zeek.org"},{"id":"/vast-v2.0","metadata":{"permalink":"/blog/vast-v2.0","source":"@site/blog/vast-v2.0/index.md","title":"VAST v2.0","description":"VAST v2.0 - Smarter Query Scheduling & Tunable Filters","date":"2022-05-16T00:00:00.000Z","formattedDate":"May 16, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"compaction","permalink":"/blog/tags/compaction"},{"label":"performance","permalink":"/blog/tags/performance"},{"label":"pcap","permalink":"/blog/tags/pcap"}],"readingTime":6.335,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.0","description":"VAST v2.0 - Smarter Query Scheduling & Tunable Filters","authors":"dominiklohmann","date":"2022-05-16T00:00:00.000Z","tags":["release","compaction","performance","pcap"]},"prevItem":{"title":"Apache Arrow as Platform for Security Data Engineering","permalink":"/blog/apache-arrow-as-platform-for-security-data-engineering"},"nextItem":{"title":"VAST v1.1.2","permalink":"/blog/vast-v1.1.2"}},"content":"Dear community, we are excited to announce [VAST v2.0][github-vast-release],\\nbringing faster execution of bulk-submitted queries, improved tunability of\\nindex structures, and new configurability through environment variables.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.0.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Query Scheduling\\n\\nVAST is now more intelligent in how it schedules queries.\\n\\nWhen a query arrives at the VAST server, VAST first goes to the catalog which\\nreturns a set of on-disk candidate partitions that the query may be applicable\\nto. Previous versions of VAST simply iterated through the available queries as\\nthey came in, loading partition by partition to extract events. Due to memory\\nconstraints, VAST is only able to keep some partitions in memory, which causes\\nfrequent loading and unloading of the same partitions for queries that access\\nthe same data. Now, VAST loads partitions depending on how many queries they are\\nrelevant for and evaluates all ongoing queries for one partition at a time.\\n\\nAdditionally, VAST now partitions the data for each schema separately, moving\\naway from partitions that contain events of multiple schemas. This helps with\\ncommon access patterns and speeds up queries restricted to a single schema.\\n\\nThe numbers speak for themselves:\\n\\n![Benchmarks](scheduler-light.png#gh-light-mode-only)\\n![Benchmarks](scheduler-dark.png#gh-dark-mode-only)\\n\\n## Updates to Aging, Compaction, and the Disk Monitor\\n\\nVAST v1.0 deprecated the experimental aging feature. Given popular demand we\'ve\\ndecided to un-deprecate it and to actually implement it on top of the same\\nbuilding blocks the new compaction mechanism uses, which means that it is now\\nfully working and no longer considered experimental.\\n\\nThe compaction plugin is now able to apply general time-based compactions that\\nare not restricted to a specific set of types. This makes it possible for\\noperators to implement rules like \\"delete all data after 1 week\\", without having\\nto list all possible data types that may occur.\\n\\nSome smaller interface changes improve the observability of the compactor for\\noperators: The  `vast compaction status` command prints the current compaction\\nstatus, and the `vast compaction list` command now lists all configured\\ncompaction rules of the VAST node.\\n\\nAdditionally, we\'ve improved overall stability and fault tolerance improvements\\nsurrounding the disk monitor and compaction features.\\n\\n## Fine-tuned Catalog Configuration\\n\\n:::note Advanced Users\\nThis section is for advanced users only.\\n:::\\n\\nThe catalog manages partition metadata and is responsible for deciding whether a\\npartition qualifies for a certain query. It does so by maintaining sketch data\\nstructures (e.g., Bloom filters, summary statistics) for each partition.\\nSketches are highly space-efficient at the cost of being probabilistic and\\nyielding false positives.\\n\\nDue to this characteristic, sketches can grow sublinear: doubling the number of\\nevents in a sketch does not lead to a doubling of the memory requirement.\\nBecause the catalog must be traversed in full for a given query it needs to be\\nmaintained in active memory to provide high responsiveness.\\n\\nA false positive can have substantial impact on the query latency by\\nmaterializing irrelevant partitions, which involves unnecessary I/O. Based on\\nthe cost of I/O, this penalty may be substantial. Conversely, reducing the false\\npositive rate increases the memory consumption, leading to a higher resident set\\nsize and larger RAM requirements.\\n\\nYou can control this space-time trade-off in the configuration section\\n`vast.index` by specifying index rules. Each rule corresponds to one sketch and\\nconsists of the following components:\\n\\n`targets`: a list of extractors to describe the set of fields whose values to\\nadd to the sketch. `fp-rate`: an optional value to control the false-positive\\nrate of the sketch.\\n\\nVAST does not create field-level sketches unless a dedicated rule with a\\nmatching target configuration exists. Here\'s an example:\\n\\n```yaml\\nvast:\\n  index:\\n    rules:\\n      - targets:\\n          # field synopses: need to specify fully qualified field name\\n          - suricata.http.http.url\\n        fp-rate: 0.005\\n      - targets:\\n          - :ip\\n        fp-rate: 0.1\\n```\\n\\nThis configuration includes two rules (= two sketches), where the first rule\\nincludes a field extractor and the second a type extractor. The first rule\\napplies to a single field, `suricata.http.http.url`, and has a false-positive\\nrate of 0.5%. The second rule creates one sketch for all fields of type `addr`\\nthat has a false-positive rate of 10%.\\n\\n## Configuring VAST with Environment Variables\\n\\nVAST now offers an additional configuration path besides editing YAML\\nconfiguration files and providing command line arguments: *setting environment\\nvariables*. This enables a convenient configuration experience when using\\ncontainer runtimes, such as Docker, where the other two configuration paths have\\na mediocre UX at best:\\n\\nThe container entry point is limited to adding command line arguments, where not\\nall options may be set. For Docker Compose and Kubernetes, it is often not\\ntrivially possible to even add command line arguments.\\n\\nProviding a manual configuration file is a heavy-weight action, because it\\nrequires (1) generating a potentially templated configuration file, and (2)\\nmounting that file into a location where VAST would read it.\\n\\nAn environment variable has the form `KEY=VALUE`. VAST processes only\\nenvironment variables having the form `VAST_{KEY}=VALUE`. For example,\\n`VAST_ENDPOINT=1.2.3.4` translates to the command line option\\n`--endpoint=1.2.3.4` and YAML configuration `vast.endpoint: 1.2.3.4`.\\n\\nRegarding precedence, environment variables override configuration file\\nsettings, and command line arguments override environment variables. Please\\nconsult the [documentation](/docs/setup/configure#environment-variables)\\nfor a more detailed explanation of how to specify keys and values.\\n\\n## VLAN Tag Extraction and Better Packet Decapsulation\\n\\nVAST now extracts [802.1Q VLAN tags](https://en.wikipedia.org/wiki/IEEE_802.1Q)\\nfrom packets, making it possible to filter packets based on VLAN ID. The packet\\nschema includes a new nested record `vlan` with two fields: `outer` and `inner`\\nto represent the respective VLAN ID. For example, you can generate PCAP traces\\nof packets based on VLAN IDs as follows:\\n\\n```bash\\nvast export pcap \'vlan.outer > 0 || vlan.inner in [1, 2, 3]\' | tcpdump -r - -nl\\n```\\n\\nVLAN tags occur in many variations, and VAST extracts them in case of\\nsingle-tagging and  [QinQ\\ndouble-tagging](https://en.wikipedia.org/wiki/IEEE_802.1ad). Consult the [PCAP\\ndocumentation](/docs/use/ingest#pcap) for details on this feature.\\n\\nInternally, the packet decapsulation logic has been rewritten to follow a\\nlayered approach: frames, packets, and segments are the building blocks. The\\nplan is to reuse this architecture when switching to kernel-bypass packet\\nacquisition using DPDK. If you would like to see more work on the front of\\nhigh-performance packet recording, please reach out.\\n\\n## Breaking Changes\\n\\nThe `--verbosity` command-line option is now called `--console-verbosity`. The\\nshorthand options `-v`, `-vv`, `-vvv`, `-q`, `-qq`, and  `-qqq`  are unchanged.\\nThis aligns the command-line option with the configuration option\\n`vast.console-verbosity`, and disambiguates from the `vast.file-verbosity`\\noption.\\n\\nThe _Meta Index_ is now called the _Catalog_. This affects multiple status and\\nmetrics keys. We plan to extend the functionality of the Catalog in a future\\nrelease, turning it into a more powerful first instance for lookups.\\n\\nTransform steps that add or modify columns now add or modify the columns\\nin-place rather than at the end, preserving the nesting structure of the\\noriginal data.\\n\\n## Changes for Developers\\n\\nThe `vast get` command no longer exists. The command allowed for retrieving\\nevents by their internal unique ID, which we are looking to remove entirely in\\nthe future.\\n\\nChanges to the internal data representation of VAST require all transform step\\nplugins to be updated. The output format of the vast export arrow command\\nchanged for the address, subnet, pattern, and enumeration types, which are now\\nmodeled as [Arrow Extension\\nTypes](https://arrow.apache.org/docs/format/Columnar.html#extension-types). The\\nrecord type is no longer flattened. The mapping of VAST types to Apache Arrow\\ndata types  is now considered stable.\\n\\n## Smaller Things\\n\\n- VAST client commands now start much faster and use less memory.\\n- The `vast count --estimate \'<query>\'` feature no longer unnecessarily causes\\n  stores to load from disk, resulting in major speedups for larger databases and\\n  broad queries.\\n- The [tenzir/vast](https://github.com/tenzir/vast) repository now contains\\n  experimental Terraform scripts for deploying VAST to AWS Fargate and Lambda."},{"id":"/vast-v1.1.2","metadata":{"permalink":"/blog/vast-v1.1.2","source":"@site/blog/vast-v1.1.2/index.md","title":"VAST v1.1.2","description":"VAST v1.1.2 - Compaction & Query Language Frontends","date":"2022-03-29T00:00:00.000Z","formattedDate":"March 29, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"compaction","permalink":"/blog/tags/compaction"},{"label":"query","permalink":"/blog/tags/query"}],"readingTime":0.33,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"}],"frontMatter":{"title":"VAST v1.1.2","description":"VAST v1.1.2 - Compaction & Query Language Frontends","authors":"lava","date":"2022-03-29T00:00:00.000Z","tags":["release","compaction","query"]},"prevItem":{"title":"VAST v2.0","permalink":"/blog/vast-v2.0"},"nextItem":{"title":"VAST v1.1.1","permalink":"/blog/vast-v1.1.1"}},"content":"Dear community, we are happy to announce the release of [VAST\\nv1.1.2](https://github.com/tenzir/vast/releases/tag/v1.1.2), the latest release\\non the VAST v1.1 series. This release contains a fix for a race condition that\\ncould lead to VAST eventually becoming unresponsive to queries in large\\ndeployments.\\n\\n\x3c!--truncate--\x3e\\n\\nFixed a race condition that would cause queries to become stuck when an exporter\\nwould time out during the meta index lookup.\\n[#2165](https://github.com/tenzir/vast/pull/2165)"},{"id":"/vast-v1.1.1","metadata":{"permalink":"/blog/vast-v1.1.1","source":"@site/blog/vast-v1.1.1/index.md","title":"VAST v1.1.1","description":"VAST v1.1.1 - Compaction & Query Language Frontends","date":"2022-03-25T00:00:00.000Z","formattedDate":"March 25, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"compaction","permalink":"/blog/tags/compaction"},{"label":"query","permalink":"/blog/tags/query"}],"readingTime":0.635,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v1.1.1","description":"VAST v1.1.1 - Compaction & Query Language Frontends","authors":"dominiklohmann","date":"2022-03-25T00:00:00.000Z","tags":["release","compaction","query"]},"prevItem":{"title":"VAST v1.1.2","permalink":"/blog/vast-v1.1.2"},"nextItem":{"title":"VAST v1.1","permalink":"/blog/vast-v1.1"}},"content":"Dear community, we are excited to announce [VAST\\nv1.1.1][github-vast-release-new].\\n\\nThis release contains some important bug fixes on top of everything included in\\nthe [VAST v1.1][github-vast-release-old] release.\\n\\n[github-vast-release-new]: https://github.com/tenzir/vast/releases/tag/v1.1.1\\n[github-vast-release-old]: https://github.com/tenzir/vast/releases/tag/v1.1.0\\n\\n\x3c!--truncate--\x3e\\n\\n- The disk monitor now correctly continues deleting until below the low water\\n  mark after a partition failed to delete.\\n- We fixed a rarely occurring race condition that caused query workers to become\\n  stuck after delivering all results until the corresponding client process\\n  terminated.\\n- Queries that timed out or were externally terminated while in the query\\n  backlog that had more unhandled candidate than taste partitions no longer\\n  permanently get stuck. This critical bug caused VAST to idle permanently on\\n  the export path once all workers were stuck.\\n\\nThanks to [@norg](https://github.com/norg) for reporting the issues."},{"id":"/vast-v1.1","metadata":{"permalink":"/blog/vast-v1.1","source":"@site/blog/vast-v1.1/index.md","title":"VAST v1.1","description":"VAST v1.1 - Compaction & Query Language Frontends","date":"2022-03-03T00:00:00.000Z","formattedDate":"March 3, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"compaction","permalink":"/blog/tags/compaction"},{"label":"query","permalink":"/blog/tags/query"}],"readingTime":5.985,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v1.1","description":"VAST v1.1 - Compaction & Query Language Frontends","authors":"dominiklohmann","date":"2022-03-03T00:00:00.000Z","last_updated":"2022-07-15T00:00:00.000Z","tags":["release","compaction","query"]},"prevItem":{"title":"VAST v1.1.1","permalink":"/blog/vast-v1.1.1"},"nextItem":{"title":"VAST v1.0","permalink":"/blog/vast-v1.0"}},"content":"Dear community, we are excited to announce [VAST v1.1][github-vast-release],\\nwhich ships with exciting new features: *query language plugins* to exchange the\\nquery expression frontend, and *compaction* as a mechanism for expressing\\nfine-grained data retention policies and gradually aging out data instead of\\nsimply deleting it.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v1.1.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Query Language Plugins\\n\\nVAST features [a new query language plugin\\ntype](https://vast.io/docs/understand/architecture/plugins#query-language)\\nthat makes it possible to exchange the querying frontend, that is, replace the\\nlanguage in which the user writes queries. This makes it easier to integrate\\nVAST into specific domains without compromising the policy-neutral system core.\\n\\nThe first instance of the query language plugin is the [`sigma`\\nplugin](https://github.com/tenzir/vast/tree/master/plugins/sigma), which make it\\npossible to pass [Sigma\\nrules](https://vast.io/docs/understand/query-language/frontends/sigma) as\\ninput instead of a standard VAST query expression. Prior to this plugin, VAST\\nattempted to parse a query as Sigma rule first, and if that failed, tried to\\nparse it as a VAST expression. The behavior changed in that VAST now always\\ntries to interpret user input as VAST expression, and if that fails, goes\\nthrough all other loaded query language plugins.\\n\\nMoving forward, we will make it easier for integrators to BYO query language and\\nleverage VAST as an execution engine. We have already\\n[experimented](https://github.com/tenzir/vast/pull/2075) with\\n[Substrait](https://substrait.io), a cross-language protobuf spec for query\\nplans. The vision is that users can easily connect *any* query language that\\ncompiles into Substrait, and VAST takes the query plan as binary substrait blob.\\nSubstrait is still a very young project, but if the Arrow integration starts to\\nmature, it has the potential to enable very powerful types of queries without\\nmuch heavy lifting on our end. We already use the Arrow Compute API to implement\\ngeneric grouping and aggregation during compaction, which allows us to avoid\\nhand-roll and optimize compute kernels for standard functions.\\n\\n## Compaction Plugin\\n\\nCompaction is a feature to perform fine-grained transformation of historical\\ndata to manage a fixed storage budget. This gives operators full control over\\nshrinking data gradually\u2014both from a temporal and spatial angle:\\n\\n**Spatial**: Traditionally, reaching a storage budget triggers deletion of the\\noldest (or least-recently-used) data. This is a binary decision to throw away a\\nsubset of events. It does not differentiate the utility of data within an event.\\nWhat if you could only throw away the irrelevant parts and keep the information\\nthat might still be useful for longitudinal investigations? What if you could\\naggregate multiple events into a single one that captures valuable information?\\nImagine, for example, halving the space utilization of events with network flow\\ninformation and keeping them 6 months longer; or imagine you could roll up a set\\nof flows into a traffic matrix that only captures who communicated with whom in\\na given timeframe.\\n\\nBy incrementally elevating data into more space-efficient representations,\\ncompaction gives you a much more powerful mechanism to achieve long retention\\nperiods while working with high-volume telemetry.\\n\\n**Temporal**: data residency regulations often come with compliance policies\\nwith maximum retention periods, e.g., data containing personal data. For\\nexample, a policy may dictate a maximum retention of 1 week for events\\ncontaining URIs and 3 months for events containing IP addresses related to\\nnetwork connections. However, these retention windows could be broadened when\\npseudonomyzing or anonymizing the relevant fields.\\n\\nCompaction has a policy-based approach to specify these temporal constraints in\\na clear, declarative fashion.\\n\\nCompaction supersedes both the disk monitor and aging, being able to cover the\\nentire functionality of their behaviors in a more configurable way. The disk\\nmonitor remains unchanged and the experimental aging feature is deprecated (see\\nbelow).\\n\\n## Updates to Transform Steps\\n\\n### Aggregate Step\\n\\n:::info Transforms \u2192 Pipelines\\nIn [VAST v2.2](/blog/vast-v2.2), we renamed *transforms* to *pipelines*, and\\n*transform steps* to *pipeline operators*. This caused several configuration key\\nchanges. Additionally, we renamed the `aggregate` operator to\\n[`summarize`][summarize]. Please keep this in mind when reading the example\\nbelow and consult the\\n[documentation](/docs/understand/query-language/pipelines) for the\\nup-to-date syntax.\\n[summarize]: /docs/understand/query-language/operators/summarize\\n:::\\n\\nThe new `aggregate` transform step plugin allows for reducing data with an\\naggregation operation over a group of columns.\\n\\nAggregation is a two-step process of first bucketing data in groups of values,\\nand then executing an aggregation function that computes a single value over the\\nbucket. The functionality is in line with what standard execution engines offer\\nvia \\"group-by\\" and \\"aggregate\\".\\n\\nBased on how the transformation is invoked in VAST, the boundary for determining\\nwhat goes into a grouping can be a table slice (e.g., during import/export) or\\nan entire partition (during compaction).\\n\\nHow this works is best shown on example data. Consider the following events\\nrepresenting flow data that contain a source IP address, a start and end\\ntimestamp, the number of bytes per flow, a boolean flag whether there is an\\nassociated alert, and a unique identifier.\\n\\n```json\\n{\\"source_ip\\": \\"10.0.0.1\\", \\"num_bytes\\": 87122, \\"start\\": \\"2022-02-22T10:36:40\\", \\"end\\": \\"2022-02-22T10:36:47\\", \\"alerted\\": false, \\"unique_id\\": 1}\\n{\\"source_ip\\": \\"10.0.0.2\\", \\"num_bytes\\": 62335, \\"start\\": \\"2022-02-22T10:36:43\\", \\"end\\": \\"2022-02-22T10:36:48\\", \\"alerted\\": false, \\"unique_id\\": 2}\\n{\\"source_ip\\": \\"10.0.0.1\\", \\"num_bytes\\": 640, \\"start\\": \\"2022-02-22T10:36:46\\", \\"end\\": \\"2022-02-22T10:36:47\\", \\"alerted\\": true, \\"unique_id\\": 3}\\n{\\"source_ip\\": \\"10.0.0.1\\", \\"num_bytes\\": 2162, \\"start\\": \\"2022-02-22T10:36:49\\", \\"end\\": \\"2022-02-22T10:36:51\\", \\"alerted\\": false, \\"unique_id\\": 4}\\n```\\n\\nWe can now configure a transformation that groups the events by their source IP\\naddress, takes the sum of the number of bytes, the minimum of the start\\ntimestamp, the maximum of the end timestamp, and the disjunction of the alerted\\nflag. Since the unique identifier cannot be aggregated in a meaningful manner,\\nit  is discarded.\\n\\n```yaml\\nvast:\\n  transforms:\\n    example-aggregation:\\n      - aggregate:\\n          group-by:\\n            - source_ip\\n          sum:\\n            - num_bytes\\n          min:\\n            - start\\n          max:\\n            - end\\n          any:\\n            - alerted\\n```\\n\\nAfter applying the transform, the resulting events will look like this:\\n\\n```json\\n{\\"source_ip\\": \\"10.0.0.1\\", \\"num_bytes\\": 89924, \\"start\\": \\"2022-02-22T10:36:40\\", \\"end\\": \\"2022-02-02T10:36:51\\", \\"alerted\\": true}\\n{\\"source_ip\\": \\"10.0.0.2\\", \\"num_bytes\\": 62335, \\"start\\": \\"2020-11-06T10:36:43\\", \\"end\\": \\"2020-02-22T10:36:48\\", \\"alerted\\": false}\\n```\\n\\nUnlike the built-in transform steps, `aggregate` is a separate open-source\\nplugin that needs to be manually enabled in your `vast.yaml` configuration to be\\nusable:\\n\\n```yaml\\nvast:\\n  plugins:\\n    - aggregate\\n```\\n\\n### Rename Step\\n\\nThe new `rename` transform step is a built-in that allows for changing the name\\nof the schema of data. This is particularly useful when a transformation changes\\nthe shape of the data. E.g., an aggregated `suricata.flow` should likely be\\nrenamed because it is of a different layout.\\n\\nThis is how you configure the transform step:\\n\\n```yaml\\nrename:\\n  layout-names:\\n    - from: suricata.flow\\n      to: suricata.aggregated_flow\\n```\\n\\n### Project and Select Steps\\n\\nThe built-in `project` and `select` transform steps now drop table slices where\\nno columns and rows match the configuration respectively instead of leaving the\\ndata untouched.\\n\\n## Deprecations\\n\\nThe `msgpack` encoding no longer exists. As we integrate deeper with Apache\\nArrow, the `arrow` encoding is now the only option. Configuration options for\\n`msgpack` will be removed in an upcoming major release. On startup, VAST now\\nwarns if any of the deprecated options are in use.\\n\\nVAST\u2019s *aging* feature never made it out of the experimental stage: it only\\nerased data without updating the index correctly, leading to unnecessary lookups\\ndue to overly large candidate sets and miscounts in the statistics. Because\\ntime-based compaction is a superset of the aging functionality (that also\\nupdates the index correctly), we will remove aging in a future release. VAST now\\nwarns on startup if it\u2019s configured to run aging."},{"id":"/vast-v1.0","metadata":{"permalink":"/blog/vast-v1.0","source":"@site/blog/vast-v1.0/index.md","title":"VAST v1.0","description":"VAST v1.0 \u2013 New Year, New Versioning Scheme","date":"2022-01-27T00:00:00.000Z","formattedDate":"January 27, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"transforms","permalink":"/blog/tags/transforms"},{"label":"query","permalink":"/blog/tags/query"}],"readingTime":3.195,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v1.0","description":"VAST v1.0 \u2013 New Year, New Versioning Scheme","authors":"dominiklohmann","date":"2022-01-27T00:00:00.000Z","last_updated":"2022-07-15T00:00:00.000Z","tags":["release","transforms","query"]},"prevItem":{"title":"VAST v1.1","permalink":"/blog/vast-v1.1"}},"content":"We are happy to announce [VAST v1.0][github-vast-release]!\\n\\nThis release brings a new approach to software versioning for Tenzir. We laid\\nout the semantics in detail in a new [VERSIONING][github-versioning-md]\\ndocument.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v1.0.0\\n[github-versioning-md]: https://github.com/tenzir/vast/blob/v1.0.0/VERSIONING.md\\n\\n\x3c!--truncate--\x3e\\n\\n## Query events based on their import time\\n\\nThe new [`#import_time` extractor][docs-meta-extractor] allows for exporting\\nevents based on the time they arrived at VAST. Most of the time, this timestamp\\nis not far away from the timestamp of when the event occurred, but in certain\\ncases the two may deviate substantially, e.g., when ingesting historical events\\nfrom several years ago.\\n\\nFor example, to export all Suricata alerts that arrived at VAST on New Years Eve\\nas JSON, run this command:\\n\\n```bash\\nvast export json \'#type == \\"suricata.alert\\" && #import_time >= 2021-12-31 && #import_time < 2022-01-01\'\\n```\\n\\nThis differs from the [`:timestamp` type extractor][docs-type-extractor] that\\nqueries all events that contain a type `timestamp`, which is an alias for the\\n`time` type.  By convention, the `timestamp` type represents the event time\\nembedded in the data itself. However, the import time  is not part of the event\\ndata itself, but rather part of metadata of every batch of events that VAST\\ncreates.\\n\\n[docs-meta-extractor]: https://vast.io/docs/understand/query-language/expressions#meta-extractor\\n[docs-type-extractor]: https://vast.io/docs/understand/query-language/expressions#type-extractor\\n\\n## Omit `null` fields in the JSON export\\n\\nVAST renders all fields defined in the schema when exporting events as JSON. A\\ncommon option for many tools that handle JSON is to skip rendering `null`\\nfields, and the new `--omit-nulls` option to the JSON export does exactly that.\\n\\nTo use it on a case-by-case basis, add this flag to any JSON export.\\n\\n```bash\\nvast export json --omit-nulls \'<query>\'\\n\\n# This also works when attaching to a matcher.\\nvast matcher attach json --omit-nulls <matcher>\\n```\\n\\nTo always enable it, add this to your `vast.yaml` configuration file:\\n\\n```yaml\\nvast:\\n  import:\\n    omit-nulls: true\\n```\\n\\n## Selection and Projection Transform Steps\\n\\n:::info Transforms \u2192 Pipelines\\nIn [VAST v2.2](/blog/vast-v2.2), we renamed *transforms* to *pipelines*, and\\n*transform steps* to *pipeline operators*. This caused several configuration key\\nchanges. Please keep this in mind when reading the example below and consult the\\n[documentation](/docs/understand/query-language/pipelines) for the\\nup-to-date syntax.\\n:::\\n\\nReshaping data during import and export is a common use case that VAST now\\nsupports. The two new built-in transform steps allow for filtering columns and\\nrows. Filtering columns (*projection*) takes a list of column names as input,\\nand filtering rows (*selection*)  works with an arbitrary query expression.\\n\\nHere\u2019s a usage example that sanitizes data leaving VAST during a query. If any\\nstring field in an event contains the value `tenzir` or `secret-username`, VAST\\nwill not include the event in the result set. The example below applies this\\nsanitization only to the events  `suricata.dns` and `suricata.http`, as defined\\nin the section `transform-triggers`.\\n\\n```yaml\\nvast:\\n  # Specify and name our transforms, each of which are a list of configured\\n  # transform steps. Transform steps are plugins, enabling users to write more\\n  # complex transformations in native code using C++ and Apache Arrow.\\n  transforms:\\n     # Prevent events with certain strings to be exported, e.g., \\"tenzir\\" or\\n     # \\"secret-username\\".\\n     remove-events-with-secrets:\\n       - select:\\n           expression: \':string !in [\\"tenzir\\", \\"secret-username\\"]\'\\n\\n  # Specify whether to trigger each transform at server- or client-side, on\\n  # import or export, and restrict them to a list of event types.\\n  transform-triggers:\\n    export:\\n      # Apply the remove-events-with-secrets transformation server-side on\\n      # export to the suricata.dns and suricata.http event types.\\n      - transform: remove-events-with-secrets\\n        location: server\\n        events:\\n          - suricata.dns\\n          - suricata.http\\n```\\n\\n## Threat Bus 2022.01.27\\n\\nThanks to a contribution from Sascha Steinbiss\\n([@satta](https://github.com/satta)), Threat Bus only reports failure when\\ntransforming a sighting context if the return code of the transforming program\\nindicates failure.\\n\\nA small peek behind the curtain: We\u2019re building the next generation of Threat\\nBus as part of VAST. We will continue to develop and maintain Threat Bus and its\\napps for the time being.\\n\\nThreat Bus 2022.01.27 is available [\ud83d\udc49\\nhere](https://github.com/tenzir/threatbus/releases/tag/2022.01.27)."}]}')}}]);