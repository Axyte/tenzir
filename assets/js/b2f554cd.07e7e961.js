"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[11477],{30010:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/introducing-tenzir-security-data-pipelines","metadata":{"permalink":"/blog/introducing-tenzir-security-data-pipelines","source":"@site/blog/introducing-tenzir-security-data-pipelines/index.md","title":"Introducing Tenzir Security Data Pipelines","description":"We\'re overjoyed to announce our highly-anticipated security data pipeline","date":"2023-08-09T00:00:00.000Z","formattedDate":"August 9, 2023","tags":[{"label":"tenzir","permalink":"/blog/tags/tenzir"},{"label":"pipelines","permalink":"/blog/tags/pipelines"}],"readingTime":4.3,"hasTruncateMarker":true,"authors":[{"name":"Oliver Rochford","title":"Chief Futurist","url":"https://github.com/oliverrochford","email":"oliver@tenzir.com","imageURL":"https://github.com/oliverrochford.png","key":"oliverrochford"}],"frontMatter":{"title":"Introducing Tenzir Security Data Pipelines","authors":"oliverrochford","date":"2023-08-09T00:00:00.000Z","tags":["tenzir","pipelines"],"comments":true},"nextItem":{"title":"Tenzir for Splunk Users","permalink":"/blog/tenzir-for-splunk-users"}},"content":"We\'re overjoyed to [announce][pr] our highly-anticipated security data pipeline\\nplatform at the renowned BlackHat conference in Las Vegas. The launch marks a\\nmilestone in our journey to bring simplicity to data engineering for\\ncybersecurity operations, and to bring a cost-efficient way to tackle the\\nincreasingly complex data engineering challenges that security teams confront\\ndaily.\\n\\n[pr]: https://tenzir.com/press/tenzir-launches-security-data-pipeline-platform?utm_source=Blog&utm_campaign=launch\\n\\n![Tenzir Launch](tenzir-launch.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Security Data Operations for the Automation Age\\n\\nThe volume of data that needs to be collected, analyzed, and stored by security\\nteams has skyrocketed. Traditional security operations tools are increasingly\\noverwhelmed, leading to an urgent need for more efficient and effective\\nsolutions. Tenzir addresses this challenge head-on, simplifying data management\\nso that security teams can focus more on identifying and mitigating threats.\\n\\nIn the words of our CEO and founder, [Matthias\\nVallentin](https://www.linkedin.com/in/matthias-vallentin/), \\"To survive in\\ntoday\'s unforgiving threat landscape you need fast, near real-time *and*\\nextensive historical data. Tenzir pipelines help security teams speed up and\\nsimplify managing the data they need, so that they can spend more time doing\\nwhat is most crucial\u2014hunting threats.\\"\\n\\n## Why Data Pipelines for Security?\\n\\nCybersecurity has become an increasingly data-driven field. From network traffic\\nto cloud telemetry, the amount of information that security teams need to\\nanalyze is staggering. A single security incident can generate billions of data\\npoints that need to be reviewed, analyzed, and actioned. Traditional methods of\\ncollecting, aggregating, and analyzing this data are not just insufficient, they\\nare obsolete, leading to security gaps and cost inefficiencies.\\n\\nNavigating the modern security data stack is no small feat. Today\'s security\\nteams are faced not just with the management of a horde of advanced security\\nsolutions including SIEM, SOAR, UEBA, and threat intelligence platforms, but\\nalso the challenge of integrating these systems with diverse data technologies,\\nsuch as databases, data lakes, and data warehouses. Compounding this complexity\\nis a growing reliance on cloud microservices and increasingly AI services. This\\nhas made security operations less of a routine process and more a strategic\\nexercise in continuously mastering emergent complexities and optimizing\\nperformance.\\n\\nThis is where Tenzir\'s security data pipelines come in. Our unique platform\\ninstigates a shift from centralized security information and event management to\\na more adaptive and decentralized operating model more aligned with DevOps and\\ndata engineering principles: security data operations\\n([SecDataOps](https://tenzir.com/secdataops?utm_source=Blog)). It transcends\\nmere collection of events and logs, instead building resilient and robust data\\nflows that optimize data for further use, whether for detection and correlation,\\nthreat hunting, or machine learning. Data pipelines are already common in data\\nengineering and DevOps. They are designed to provide a seamless, efficient, and\\nflexible way to manage and move data. But there are a number of reasons why data\\npipelines are also the ideal solution for today\'s cybersecurity challenges.\\n\\n- Firstly, security data pipelines optimize and formalize data management. They\\n  allow for the standardized collection, shaping, enrichment, and routing of\\n  data between any security and data technology. They also provide a measurable,\\n  repeatable and more cost-effective approach to solve the growing data\\n  engineering challenges typically faced by security teams.  As they are\\n  designed specifically for security use-cases, they also allow security teams\\n  to meet their own data needs.\\n- Secondly, and in today\'s economic climate more crucially, security data\\n  pipelines reduce consumption-based costs. By moving only the right data to the\\n  right place at the right time in the most efficient way, and by pushing\\n  detection and enrichment workloads to the network edge, businesses can\\n  drastically reduce their SIEM, cloud, and other data costs. Security\\n  operations become more efficient and cost-effective, ultimately allowing more\\n  data to be collected, and scarce money to be reallocated.\\n- Thirdly, security data pipelines help avoid vendor lock-in. Tenzir is built on\\n  open data and security standards, making data exchange between different\\n  technologies trivial. Pipelines also connect diverse tools and solutions as\\n  needed, enabling organizations to choose whatever solutions fit best for them,\\n  and to better adapt to evolving.\\n- Finally, the flexibility and scalability of security data pipelines are\\n  unmatched. They can scale up or down according to need. They also make it easy\\n  to support new data types and security scenarios, helping to future-proof your\\n  security architecture, and providing operational plasticity and resilience.\\n\\nSecurity data pipelines are transforming the security operations landscape by\\nproviding a more effective and efficient way to manage the ever-growing volumes\\nof security data. As the volume, variety, and velocity of security data continue\\nto increase, the need for more effective data management and analysis tools will\\nonly grow as well.\\n\\nAt Tenzir, we are leading this transformation, building an open platform that\\nempowers security teams to build and deploy efficient security data pipelines\\nusing plug-and-play building blocks. Our goal is simple\u2014more time for threat\\nhunting, less time and money on data engineering, and a more robust\\ncybersecurity posture overall.\\n\\nIn today\'s complex cybersecurity landscape, data pipelines are not just for data\\nengineers anymore. They have become indispensable for security teams. The era\\nfor security data pipelines isn\'t on the horizon, it\'s already here.\\n\\nJoin us on this exciting journey to revamp cybersecurity operations.\\n\\nStart using Tenzir by visiting our website at\\n[https://tenzir.com](https://tenzir.com?utm_source=Blog), or get in touch with\\nus at [info@tenzir.com](mailto:info@tenzir.com)."},{"id":"/tenzir-for-splunk-users","metadata":{"permalink":"/blog/tenzir-for-splunk-users","source":"@site/blog/tenzir-for-splunk-users/index.md","title":"Tenzir for Splunk Users","description":"Our Tenzir Query Language (TQL) is a pipeline language that works","date":"2023-08-03T00:00:00.000Z","formattedDate":"August 3, 2023","tags":[{"label":"zeek","permalink":"/blog/tags/zeek"},{"label":"threat hunting","permalink":"/blog/tags/threat-hunting"},{"label":"pipelines","permalink":"/blog/tags/pipelines"},{"label":"tql","permalink":"/blog/tags/tql"},{"label":"splunk","permalink":"/blog/tags/splunk"},{"label":"spl","permalink":"/blog/tags/spl"}],"readingTime":8.06,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Tenzir for Splunk Users","authors":"mavam","date":"2023-08-03T00:00:00.000Z","tags":["zeek","threat hunting","pipelines","tql","splunk","spl"],"comments":true},"prevItem":{"title":"Introducing Tenzir Security Data Pipelines","permalink":"/blog/introducing-tenzir-security-data-pipelines"},"nextItem":{"title":"Native Zeek Log Rotation & Shipping","permalink":"/blog/native-zeek-log-rotation-and-shipping"}},"content":"Our [Tenzir Query Language (TQL)](/language) is a pipeline language that works\\nby chaining operators into data flows. When we designed TQL, we specifically\\nstudied Splunk\'s [Search Processing Language (SPL)][spl], as it generally leaves\\na positive impression for security analysts that are not data engineers. Our\\ngoal was to take all the good things of SPL, but provide a more powerful\\nlanguage without compromising simplicity. In this blog post, we explain how the\\ntwo languages differ using concrete threat hunting examples.\\n\\n[spl]: https://docs.splunk.com/Documentation/SplunkCloud/latest/Search/Aboutthesearchlanguage\\n\\n![SPL versus TQL](spl-vs-tql.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Why not SQL?\\n\\nSplunk was the first tool that provided an integrated solution from interactive\\ndata exploration to management-grade dashboards\u2014all powered by dataflow\\npipelines. The success of Splunk is not only resulting from their first-mover\\nadvantage in the market, but also because their likable user experience: it is\\n*easy* to get things done.\\n\\nAt Tenzir, we have a very clear target audience: security practitioners. They\\nare not necessarily data engineers and fluent in SQL and low-level data tools,\\nbut rather identify as blue teamers, incident responders, threat hunters,\\ndetection engineers, threat intelligence analysts, and other domain experts. Our\\ngoal is cater to these folks, without requiring them to have deep understanding\\nof relational algebra.\\n\\nWe opted for a dataflow language because it simplifies reasoning\u2014one step at a\\ntime. At least conceptually, because a smart system optimizes the execution\\nunder the hood. As long as the observable behavior remains the same, the\\nunderlying implementation can optimize the actual computation at will. This is\\nespecially noticeable with declarative languages, such as SQL, where the user\\ndescribes the *what* instead of the *how*. A dataflow language is a bit more\\nconcrete in that it\'s closer to the *how*, but that\'s precisely the trade-off\\nthat simplifies the reasoning: the focus is on a single operation at a time as\\nopposed to an entire large expression.\\n\\nThis dataflow pipeline style is becoming more and more popular. Most SIEMs have\\na language of their own, like Splunk. [Kusto][kusto] is another great example\\nwith a wide user base in security. Even in the data space,\\n[PRQL](https://prql-lang.org) witnesses a strong support for this way of\\nthinking.\\n\\n[kusto]: https://learn.microsoft.com/en-us/azure/data-explorer/kusto/query/\\n\\nIn fact, for a given dataflow pipeline there\'s often an equivalent SQL\\nexpression, because the underlying engines frequently map to the same execution\\nmodel. This gives rise to [transpiling dataflow languages to other execution\\nplatforms][splunk-transpiler]. Ultimately, our goal is that security\\npractitioners do not have to think about *any* of this and stay in their happy\\nplace, which means avoiding context switches to lower-level data primitives.\\n\\n[splunk-transpiler]: https://www.databricks.com/blog/2022/12/16/accelerating-siem-migrations-spl-pyspark-transpiler.html\\n\\nNow that we got the SQL topic out of the way, let\'s dive into some hands-on\\nexamples that illustrate the similarities and differences between SPL and TQL.\\n\\n## Examples\\n\\nBack in 2020, Eric Ooi wrote about [threat hunting with\\nZeek](https://www.ericooi.com/zeekurity-zen-part-iv-threat-hunting-with-zeek/),\\nproviding a set of Splunk queries that are corner stones for threat hunting.\\n\\n### Connections to destination port > 1024\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=zeek_conn id.resp_p > 1024\\n| chart count over service by id.resp_p\\n```\\n\\nTenzir:\\n\\n```\\nexport\\n| where #schema == \\"zeek.conn\\" && id.resp_p > 1024\\n| summarize count(.) by service, id.resp_p\\n```\\n\\nAnalysis:\\n\\n- In SPL, you typically start with an `index=X` to specify your dataset. In\\n  TQL, you start with a [source operator](/operators/sources). To run a query\\n  over historical data, we use the [`export`](/operators/sources/export)\\n  operator.\\n\\n- The subsequent [`where`](/operators/transformations/where) operator is a\\n  transformation to filter the stream of events with the\\n  [expression](/language/expressions) `#schema == \\"zeek.conn\\" && id.resp_p >\\n  1024`. In SPL, you write that expression directly into `index`. In TQL, we\\n  logically separate this because one operator should have exactly one purpose.\\n  Under the hood, the TQL optimizer does predicate pushdown to avoid first\\n  exporting the entire database and only then applying the filter.\\n\\n  Why does this single responsibility principle matter? Because it\'s critical\\n  for *composition*: we can now replace `export` with another data source, like\\n  [`from`](/operators/sources/from) [`kafka`](/connectors/kafka), and the rest\\n  of the pipeline stays the same.\\n\\n- TQL\'s `#schema` is an expression that is responsible for filtering the data\\n  sources. This is because all TQL pipelines are *multi-schema*, i.e., they can\\n  process more than a single type of data. The ability to specify a regular\\n  expression makes for a powerful way to select the desired input.\\n\\n- SPL\'s [`chart X by Y, Z`][chart] (or equivalently `chart X over Y by Z`)\\n  performs an implicit\\n  [pivot-wider](https://epirhandbook.com/en/pivoting-data.html) operation on\\n  `Z`. This different tabular format has the same underlying data produced by\\n  `summarize X by Y, Z`, which is why we are replacing it accordingly in our\\n  examples.\\n\\n[chart]: https://www.splunk.com/en_us/blog/tips-and-tricks/search-commands-stats-chart-and-timechart.html\\n\\n### Top 10 sources by number of connections\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=zeek_conn\\n| top id.orig_h\\n| head 10\\n```\\n\\nTenzir:\\n\\n```splunk-spl\\nexport\\n| where #schema == \\"zeek.conn\\"\\n| top id.orig_h\\n| head 10\\n```\\n\\nNote the similarity. We opted to add [`top`](/operators/transformations/top) and\\n[`rare`](/operators/transformations/rare) to make SPL users feel at home.\\n\\n### Top 10 sources by bytes sent\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=zeek_conn\\n| stats values(service) as Services sum(orig_bytes) as B by id.orig_h\\n| sort -B\\n| head 10\\n| eval MB = round(B/1024/1024,2)\\n| eval GB = round(MB/1024,2)\\n| rename id.orig_h as Source\\n| fields Source B MB GB Services\\n```\\n\\nTenzir:\\n\\n```splunk-spl\\nexport\\n| where #schema == \\"zeek.conn\\"\\n| summarize Services=distinct(service), B=sum(orig_bytes) by id.orig_h\\n| sort B desc\\n| head 10\\n| extend MB=round(B/1024/1024,2)\\n| extend GB=round(MB/1024,2)\\n| put Source=id.orig_h, B, MB, GB, Services\\n```\\n\\nAnalysis:\\n\\n- We opted for Kusto\'s syntax of sorting (for technical reasons), by appending\\n  an `asc` or `desc` qualifier after the field name. `sort -B` translates into\\n  `sort B desc`, whereas `sort B` into `sort B asc`. However, we want to adopt\\n  the SPL syntax in the future.\\n\\n- SPL\'s `eval` maps to [`extend`](/operators/transformations/extend).\\n\\n- The difference between `extend` and [`put`](/operators/transformations/put) is\\n  that `extend` keeps all fields as is, whereas `put` reorders fields and\\n  performs an explicit projection with the provided fields.\\n\\n- We don\'t have functions in TQL. *Yet*. It\'s one of our most important roadmap\\n  items at the time of writing, so stay tuned.\\n\\n### Bytes transferred over time by service\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=\\"zeek_conn\\" OR sourcetype=\\"zeek_conn_long\\"\\n| eval orig_megabytes = round(orig_bytes/1024/1024,2)\\n| eval resp_megabytes = round(resp_bytes/1024/1024,2)\\n| eval orig_gigabytes = round(orig_megabytes/1024,2)\\n| eval resp_gigabytes = round(resp_megabytes/1024,2)\\n| timechart sum(orig_gigabytes) AS \'Outgoing\',sum(resp_gigabytes) AS \'Incoming\' by service span=1h\\n```\\n\\nTenzir:\\n\\n```splunk-spl\\nexport\\n| where #schema == /zeek\\\\.conn.*/\\n| extend orig_megabytes=round(orig_bytes/1024/1024, 2)\\n| extend resp_megabytes=round(orig_bytes/1024/1024, 2)\\n| extend orig_gigabytes=round(orig_megabytes/1024, 2)\\n| extend resp_gigabytes=round(orig_megabytes/1024, 2)\\n| summarize Outgoing=sum(orig_gigabytes), Incoming=sum(resp_gigabytes) by ts, service resolution 1h\\n```\\n\\nAnalysis:\\n\\n- SPL\'s `timechart` does an implicit group by timestamp. As we use TQL\'s\\n  `summarize` operator, we need to explicitly provide the grouping field `ts`.\\n  In the future, you will be able to use `:timestamp` in a grouping expression,\\n  i.e., group by the field with the type named `timestamp`.\\n\\n- This query spreads over two data sources: the event `zeek.conn` and\\n  `zeek.conn_long`. The latter tracks long-running connections and is available\\n  as [separate package](https://github.com/corelight/zeek-long-connections).\\n\\n### Rare JA3 hashes\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=zeek_ssl\\n| rare ja3\\n```\\n\\nTenzir:\\n\\n```splunk-spl\\nexport\\n| where #schema == \\"zeek.ssl\\"\\n| rare ja3\\n| head 10\\n```\\n\\nAnalysis:\\n\\n- This example shows again how to select a specific data source and perform\\n  \\"stack counting\\". Unlike SPL, our version of `rare` does not limit the output\\n  to 10 events by default, which is why add `head 10`. This goes back to the\\n  single responsibility principle: one operator should do exactly one thing. The\\n  act of limiting the output should always be associated with\\n  [`head`](/operators/transformations/head).\\n\\n### Expired certificates\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=zeek_x509\\n| convert num(certificate.not_valid_after) AS cert_expire\\n| eval current_time = now(), cert_expire_readable = strftime(cert_expire,\\"%Y-%m-%dT%H:%M:%S.%Q\\"), current_time_readable=strftime(current_time,\\"%Y-%m-%dT%H:%M:%S.%Q\\")\\n| where current_time > cert_expire\\n```\\n\\nTenzir:\\n\\n```splunk-spl\\nexport\\n| where certificate.not_valid_after > now()\\n```\\n\\nAnalysis:\\n\\n- This example shows the benefit of native time types (and Tenzir\'s rich type\\n  system in general).\\n\\n- TQL\'s [type system](/data-model/type-system) has first-class support for times\\n  and durations.\\n\\n- TQL\'s [`zeek-tsv`](/formats/zeek-tsv) parser preserves `time` types natively,\\n  so you don\'t have to massage strings at query-time.\\n\\n### Large DNS queries\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=zeek_dns\\n| eval query_length = len(query)\\n| where query_length > 75\\n| table _time id.orig_h id.resp_h proto query query_length answer\\n```\\n\\nTenzir:\\n\\n```splunk-spl\\nexport\\n| where #schema == \\"zeek.dns\\"\\n| extend query_length = length(query)\\n| where query_length > 75\\n| select :timestamp, id.orig_h, id.resp_h, proto, query, query_length, answer\\n```\\n\\nAnalysis:\\n\\n- As mentioned above, we don\'t have functions in TQL yet. Once we have them,\\n  SPL\'s `len` will map to `length` in TQL.\\n\\n- The SPL-generated `_time` field maps to the `:timestamp` type extractor in\\n  TQL.\\n\\n### Query responses with NXDOMAIN\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=zeek_dns rcode_name=NXDOMAIN\\n| table _time id.orig_h id.resp_h proto query\\n```\\n\\nTenzir:\\n\\n```splunk-spl\\nexport\\n| where #schema == \\"zeek.dns\\" && rcode_name == \\"NXDOMAIN\\"\\n| select :timestamp, id.orig_h, id.resp_h, proto, query\\n```\\n\\nAnalysis:\\n\\n- The `table` operator in splunk outputs the data in tabular form. This is the\\n  default for our [app](/setup-guides/use-the-app).\\n\\n- There\'s also an [upcoming](https://github.com/tenzir/tenzir/pull/3113) `write\\n  table` format to generate a tabular representation outside the app.\\n\\n## Summary\\n\\nIn this blog post we\'ve juxtaposed the languages of Splunk (SPL) and Tenzir\\n(TQL). They are remarkably similar\u2014and that\'s not accidental. When we talked to\\nsecurity analysts we often heard that Splunk has a great UX. Even our own\\nengineers that live on the command line find this mindset natural. But Splunk\\nwas not our only inspiration, we also drew inspiration from Kusto and others.\\n\\nAs we created TQL, we wanted to learn from missed opportunities while doubling\\ndown on SPL\'s great user experience.\\n\\nIf you\'d like to give Tenzir a spin, [try our community edition](/get-started)\\nfor free. A demo node with example pipelines is waiting for you. For more\\ndetails about TQL, head over to the [language\\ndocumentation](/language/pipelines)."},{"id":"/native-zeek-log-rotation-and-shipping","metadata":{"permalink":"/blog/native-zeek-log-rotation-and-shipping","source":"@site/blog/native-zeek-log-rotation-and-shipping/index.md","title":"Native Zeek Log Rotation & Shipping","description":"Did you know that Zeek supports log rotation triggers, so","date":"2023-07-27T00:00:00.000Z","formattedDate":"July 27, 2023","tags":[{"label":"zeek","permalink":"/blog/tags/zeek"},{"label":"logs","permalink":"/blog/tags/logs"},{"label":"shipping","permalink":"/blog/tags/shipping"},{"label":"rotation","permalink":"/blog/tags/rotation"},{"label":"pipelines","permalink":"/blog/tags/pipelines"}],"readingTime":4.875,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Native Zeek Log Rotation & Shipping","authors":"mavam","date":"2023-07-27T00:00:00.000Z","tags":["zeek","logs","shipping","rotation","pipelines"],"comments":true},"prevItem":{"title":"Tenzir for Splunk Users","permalink":"/blog/tenzir-for-splunk-users"},"nextItem":{"title":"Shell Yeah! Supercharging Zeek and Suricata with Tenzir","permalink":"/blog/shell-yeah-supercharging-zeek-and-suricata-with-tenzir"}},"content":"Did you know that [Zeek](http://zeek.org) supports log rotation triggers, so\\nthat you can do anything you want with a newly rotated batch of logs?\\n\\n![Zeek Log Rotation](zeek-log-rotation.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\nThis blog post shows you how to use Zeek\'s native log rotation feature to\\nconveniently invoke any post-processor, such as a log shipper. In our examples\\nwe show how to to ingest data into Tenzir, but you can plug in any downstream\\ntooling.\\n\\n## External Log Shipping (pull)\\n\\nIn case you\'re not using Zeek\'s native log rotation trigger, you may observe a\\ndirectory to which Zeek periodically writes files. For example, the utility\\n[zeek-archiver](https://github.com/zeek/zeek-archiver) does that.\\n\\nGeneric log shippers can take care of that as well. Your mileage may vary. For\\nexample, [Filebeat][filebeat] works for stock Zeek only. The parsing logic is\\nhard-coded for every log type. If you have custom scripts or extend some logs,\\nyou\'re left alone. Filebeat also uses the stock Zeek JSON output, which has no\\ntype information. Filebeat then brings the typing back manually later as it\\nconverts the logs to the Elastic Common Schema (ECS).\\n\\n[filebeat]: https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-zeek.html\\n\\n## Native Log Shipping (push)\\n\\nThere\'s also a lesser known, push-based option using [Zeek\'s logging\\nframework](https://docs.zeek.org/en/master/frameworks/logging.html). You can\\nprovide a shell script that Zeek invokes *whenever it rotates a file*. The shell\\nscript receives the filename of the rotated file plus some additional metadata\\nas arguments.\\n\\nFirst, to activate log rotation, you need to set\\n`Log::default_rotation_interval` to a non-zero value. The default of `0 secs`\\nmeans that log rotation is disabled.\\n\\nSecond, to customize what\'s happening on rotation you can redefine\\n[`Log::default_rotation_postprocessor_cmd`](https://docs.zeek.org/en/master/scripts/base/frameworks/logging/main.zeek.html#id-Log::default_rotation_postprocessor_cmd)\\nto point to a shell script.\\n\\nFor example, to rotate all log files every 10 minutes with a custom `ingest`\\nscript, you can invoke Zeek as follows:\\n\\n```bash\\nzeek -r trace.pcap \\\\\\n  Log::default_rotation_postprocessor_cmd=ingest \\\\\\n  Log::default_rotation_interval=10mins\\n```\\n\\nLet\'s take a look at this `ingest` shell script in more detail. Zeek always\\npasses 6 arguments to the post-processing script:\\n\\n1. The filename of the log, e.g., `/path/to/conn.log`\\n2. The type of the log (aka. `path`), such as `conn` or `http`\\n3. Timestamp when Zeek opened the log file\\n4. Timestamp when Zeek closed (= rotated) the log file\\n5. A flag that is true when rotation occurred due to Zeek terminating\\n6. The format of the log, which is either `ascii` (=\\n   [`zeek-tsv`](/formats/zeek-tsv)) or [`json`](/formats/json)\\n\\nHere\'s a complete example that uses (1), (2), and (6):\\n\\n```bash title=\\"ingest\\"\\n#!/bin/sh\\n\\nfile_name=\\"$1\\"\\nbase_name=\\"$2\\"\\nfrom=\\"$3\\"\\nto=\\"$4\\"\\nterminating=\\"$5\\"\\nwriter=\\"$6\\"\\n\\nif [ \\"$writer\\" = \\"ascii\\" ]; then\\n  format=\\"zeek-tsv\\"\\nelif [ \\"$writer\\" = \\"json\\" ]; then\\n  format=\\"json --schema zeek.$base_name\\"\\nelse\\n  echo \\"unsupported Zeek writer: $writer\\"\\n  exit 1\\nfi\\n\\npipeline=\\"from file $file_name read $format | import\\"\\n\\ntenzir \\"$pipeline\\"\\n```\\n\\n### Post-processing with Tenzir pipelines\\n\\nWhen you run Zeek as above, the `ingest` script dynamically constructs an\\ningestion pipeline based on the type of the Zeek log at hand. Given your logging\\nformat (TSV or JSON), the pipelines for a rotated `conn.log` file may look like\\nthis:\\n\\n```\\nfrom file /path/to/conn.log read zeek-tsv | import\\nfrom file /path/to/conn.log read json --schema zeek.conn | import\\n```\\n\\nThis pipeline reads the Zeek log and pipes it to the\\n[`import`](/operators/sinks/import) operator, which stores all your logs at a\\nrunning Tenzir node. You could also use the\\n[`extend`](/operators/transformations/extend) operator to include the\\nfilename in the data:\\n\\n```bash\\npipeline=\\"from file $file_name read $format \\\\\\n          | extend filename=$file_name \\\\\\n          | import\\"\\n```\\n\\nTake a look at the [list of operators](/operators) for further inspiration on\\nthings you can do, or check out the [user guides](/user-guides) for concrete\\nideas.\\n\\n### Zeek package\\n\\nIf you want post-processing with Tenzir pipelines out of the box, use our\\nofficial [Zeek package](https://github.com/tenzir/zeek-tenzir):\\n\\n```bash\\nzkg install zeek-tenzir\\n```\\n\\nAfter installing the package, you have two options to run pipelines on rotated\\nZeek logs:\\n\\n1. Load the `tenzir-import` Zeek script to ship logs to a local Tenzir node\\n\\n   ```bash\\n   # Start a node.\\n   tenzir-node\\n   # Ship logs to it and delete the original files.\\n   zeek -r trace.pcap tenzir/import\\n   ```\\n\\n  Pass `Tenzir::delete_after_postprocesing=F` to `zeek` to keep the original\\n  logs.\\n\\n2. Write Zeek scripts to register pipelines manually:\\n\\n   ```zeek\\n   # Activate log rotation by setting a non-zero value.\\n   redef Log::default_rotation_interval = 10 mins;\\n \\n   event zeek_init()\\n     {\\n     Tenzir::postprocess(\\"import\\");\\n     Tenzir::postprocess(\\"to directory /tmp/logs write parquet\\");\\n     }\\n   ```\\n\\n   The above Zeek script hooks up two pipelines via the function\\n   `Tenzir::postprocess`. Each pipeline executes upon log rotation and receives\\n   the Zeek log file as input. The first imports all data via\\n   [`import`](/operators/sinks/import) and the second writes the logs as\\n   [`parquet`](/formats/parquet) files using [`to`](/operators/sinks/to).\\n\\n## Reliability\\n\\nZeek implements the log rotation logic by spawning a separate child process.\\nWhen the (parent) Zeek process dies, the children become orphaned and keep\\nrunning until completion.\\n\\nThe implication is that Zeek cannot re-trigger a failed post-processing command.\\nSo you have exactly one shot. This may not be a problem for trace file analysis,\\nbut live deployments may require higher reliability guarantees. For such\\nscenarios, we recommend to use the post-processing script as a notifier, e.g.,\\nto signal another tool that it can now process a file.\\n\\nFor ultimate control over logging, you can always develop your own [writer\\nplugin](/blog/mobilizing-zeek-logs#writer-plugin) that immediately ship logs\\ninstead of going through the file system.\\n\\n## Conclusion\\n\\nThis blog post shows how you can use Zeek\'s native log rotation feature to\\ninvoke an arbitrary command as soon as a log file gets rotated. This approach\\nprovides an attractive alternative that turns pull-based file monitoring into\\nmore flexible push-based delivery.\\n\\n|              |   Push   |     Pull     |\\n| ------------ |:--------:|:------------:|\\n| Trigger      | rotation | new file/dir |\\n| Complexity   |   low    |    medium    |\\n| Reliability  |   low    |    high      |\\n\\nIf you are looking for an efficient way to get your Zeek logs flowing, [give\\nTenzir a try](/get-started). [Our Zeek\\npackage](https://github.com/tenzir/zeek-tenzir) makes it easy to launch\\npost-processing pipelines natively from Zeek. And don\'t forget to check out our\\n[other Zeek blogs](/blog/tags/zeek)."},{"id":"/shell-yeah-supercharging-zeek-and-suricata-with-tenzir","metadata":{"permalink":"/blog/shell-yeah-supercharging-zeek-and-suricata-with-tenzir","source":"@site/blog/shell-yeah-supercharging-zeek-and-suricata-with-tenzir/index.md","title":"Shell Yeah! Supercharging Zeek and Suricata with Tenzir","description":"As an incident responder, threat hunter, or detection engineer, getting quickly","date":"2023-07-20T00:00:00.000Z","formattedDate":"July 20, 2023","tags":[{"label":"zeek","permalink":"/blog/tags/zeek"},{"label":"suricata","permalink":"/blog/tags/suricata"},{"label":"logs","permalink":"/blog/tags/logs"},{"label":"shell","permalink":"/blog/tags/shell"}],"readingTime":4.48,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Shell Yeah! Supercharging Zeek and Suricata with Tenzir","authors":"mavam","date":"2023-07-20T00:00:00.000Z","last_updated":"2023-07-22T00:00:00.000Z","tags":["zeek","suricata","logs","shell"],"comments":true},"prevItem":{"title":"Native Zeek Log Rotation & Shipping","permalink":"/blog/native-zeek-log-rotation-and-shipping"},"nextItem":{"title":"Zeek and Ye Shall Pipe","permalink":"/blog/zeek-and-ye-shall-pipe"}},"content":"As an incident responder, threat hunter, or detection engineer, getting quickly\\nto your analytics is key for productivity. For network-based visibility and\\ndetection, [Zeek](https://zeek.org) and [Suricata](https://suricata.io) are the\\nbedrock for many security teams. But operationalizing these tools can take a\\ngood chunk of time.\\n\\nSo we asked ourselves: **How can we make it super easy to work with Zeek and\\nSuricata logs?**\\n\\n![Shell Operator](shell-operator.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Purring like a Suricat\\n\\nIn our [previous blog post](/blog/zeek-and-ye-shall-pipe/) we adapted Zeek to\\nbehave like a good ol\' Unix tool, taking input via stdin and producing output\\nvia stdout. Turns out you can do the same fudgery with Suricata:[^1]\\n\\n[^1]: Suricata outputs [EVE\\nJSON](https://suricata.readthedocs.io/en/latest/output/eve/eve-json-output.html)\\nby default, which is equivalent to Zeek\'s streaming JSON output, with the\\ndifference being that Zeek\'s `_path` field is called `event_type` in Suricata\\nlogs.\\n\\n```bash title=suricatify\\n#!/bin/sh\\nsuricata -r /dev/stdin \\\\\\n  --set outputs.1.eve-log.filename=/dev/stdout \\\\\\n  --set logging.outputs.0.console.enabled=no\\n```\\n\\nLet\'s break this down:\\n\\n- The `--set` option take a `name=value` parameter that overrides the settings\\n  in your `suricata.yaml` config file.\\n- The key `outputs.1.eve-log.filename` refers to the `outputs` array, takes\\n  element at index `1`, treats that as object and goes to the nested field\\n  `eve-log.filename`. Setting `/dev/stdout` as filename makes Suricata write to\\n  stdout.\\n- We must set `logging.outputs.0.console.enabled` to `no` because Suricata\\n  writes startup log messages to stdout. Since they are not valid JSON, we\\n  would otherwise create an invalid JSON output stream.\\n\\n## User-defined Operators\\n\\nNow that we have both Zeek and Suricata at our fingertips, how can we work with\\ntheir output more easily? This is where Tenzir comes into play\u2014easy\\n[pipelines](/language/pipelines) for security teams to acquire,\\n[reshape](/user-guides/reshape-data), and route event data.\\n\\nHere are two examples that count the number of unique source IP addresses per\\ndestination IP address, on both Zeek and Suricata data:\\n\\n```bash\\n# Zeek\\nzcat pcap.gz | zeekify | tenzir \\\\\\n  \'read zeek-json\\n   | where #schema == \\"zeek.conn\\"\\n   | summarize n=count_distinct(id.orig_h) by id.resp_h\\n   | sort n desc\'\\n# Suricata\\nzcat pcap.gz | suricatify | tenzir \\\\\\n  \'read suricata\\n   | where #schema == \\"suricata.flow\\"\\n   | summarize n=count_distinct(src_ip) by dst_ip\\n   | sort n desc\'\\n```\\n\\nIt\'s a bit unwieldy to write such a command line that requires an external shell\\nscript to work. This is where [user-defined operators](/operators/user-defined)\\ncome into play. In combination with the [`shell`](/operators/sources/shell)\\noperator, you can write a custom `zeek` and `suricata` operator and ditch the\\nshell script:\\n\\n```yaml title=\\"tenzir.yaml\\"\\ntenzir:\\n  operators:\\n    zeek:\\n     shell \\"zeek -r - LogAscii::output_to_stdout=T\\n            JSONStreaming::disable_default_logs=T\\n            JSONStreaming::enable_log_rotation=F\\n            json-streaming-logs\\"\\n     | read zeek-json\\n    suricata:\\n     shell \\"suricata -r /dev/stdin\\n            --set outputs.1.eve-log.filename=/dev/stdout\\n            --set logging.outputs.0.console.enabled=no\\"\\n     | read suricata\\n```\\n\\nThe difference stands out when you look now at the pipeline definition:\\n\\n```text title=Zeek\\nzeek\\n| where #schema == \\"zeek.conn\\"\\n| summarize n=count_distinct(id.orig_h) by id.resp_h\\n| sort n desc\\n```\\n\\n```text bash title=Suricata\\nsuricata\\n| where #schema == \\"suricata.flow\\"\\n| summarize n=count_distinct(src_ip) by dst_ip\\n| sort n desc\\n```\\n\\nIt\'s pretty convenient to drop packets into a Tenzir pipeline, process them with\\nour favorite tools, and then perform fast in-situ analytics on them. The nice\\nthing is that operators compose: a new operator automatically works with all\\nexisting ones.\\n\\n## How does it work?\\n\\nFirst, let\'s take a look at the standard approach where one process pipes the\\noutput into the next:\\n\\n![Piping Zeek to Tenzir](zeek-to-tenzir-pipe.excalidraw.svg)\\n\\nWhen using the `shell` operator, the `tenzir` process spawns `zeek` or\\n`suricata` as child process. The operator then forwards the bytes from stdin of\\nthe `tenzir` process to the child\'s stdin, and uses the child\'s stdout as input\\nto the subsequent [`read`](/next/operators/transformations/read) operator.\\n\\n![Shelling out to Zeek](zeek-to-tenzir-shell.excalidraw.svg)\\n\\nIn the above example, `shell` acts as a *source* operator, i.e., it does not\\nconsume input and only produces output. The `shell` operator can also act as\\n*transformation*, i.e., additionally accept input. This makes it possible to use\\nit more flexibly in combination with other operators, e.g., the\\n[`load`](/operators/sources/load) operator emitting bytes from a\\n[loader](/connectors):\\n\\n```\\nload file trace.pcap\\n| zeek\\n| where 6.6.6.6\\n| write json\\n```\\n\\nGot a PCAP trace via Kafka? Just exchange the `file` loader with the\\n[`kafka`](/connectors/kafka) loader:\\n\\n```\\nload kafka -t artifact\\n| zeek\\n| where 6.6.6.6\\n| write json\\n```\\n\\nYou may not always sit in front of a command line and are able to pipe data from\\na Unix tool into a Tenzir pipeline. For example, when you use our\\n[app](/setup-guides/use-the-app) or the [REST API](/rest-api). This is where the\\n`shell` operator shines. The diagram above shows how `shell` shifts the entry\\npoint of data from a tool to the Tenzir process. You can consider `shell` your\\nescape hatch to reach deeper into a specific Tenzir node, as if you had a native\\nshell.\\n\\n:::info Unsafe operators in Nodes\\nNaturally, we must add that this is also a security risk and can lead to\\narbitrary shell access. This is why the `shell` operator and a few other ones\\nwith far-reaching side effects are disabled by default *when running in a node*.\\nIf you are aware of the implications, you can remove this restriction by setting\\n`tenzir.allow-unsafe-pipelines: true` in the `tenzir.yaml` of the respective\\nnode.\\n:::\\n\\n## Conclusion\\n\\nIn this blog post we showed you the [`shell`](/operators/sources/shell) operator\\nand how you can use it to integrate third-party tooling into a Tenzir pipeline\\nwhen coupled with [user-defined operators](/operators/user-defined).\\n\\nUsing Zeek or Suricata? Tenzir makes \'em fun to work with. Check out our other\\nblogs tagged with [`#zeek`](/blog/tags/zeek) and\\n[`#suricata`](/blog/tags/suricata), and [give it a shot](/get-started) yourself."},{"id":"/zeek-and-ye-shall-pipe","metadata":{"permalink":"/blog/zeek-and-ye-shall-pipe","source":"@site/blog/zeek-and-ye-shall-pipe/index.md","title":"Zeek and Ye Shall Pipe","description":"Zeek turns packets into structured logs. By default, Zeek","date":"2023-07-13T00:00:00.000Z","formattedDate":"July 13, 2023","tags":[{"label":"zeek","permalink":"/blog/tags/zeek"},{"label":"logs","permalink":"/blog/tags/logs"},{"label":"json","permalink":"/blog/tags/json"},{"label":"pipelines","permalink":"/blog/tags/pipelines"}],"readingTime":2.46,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Zeek and Ye Shall Pipe","authors":"mavam","date":"2023-07-13T00:00:00.000Z","tags":["zeek","logs","json","pipelines"],"comments":true},"prevItem":{"title":"Shell Yeah! Supercharging Zeek and Suricata with Tenzir","permalink":"/blog/shell-yeah-supercharging-zeek-and-suricata-with-tenzir"},"nextItem":{"title":"Mobilizing Zeek Logs","permalink":"/blog/mobilizing-zeek-logs"}},"content":"[Zeek](https://zeek.org) turns packets into structured logs. By default, Zeek\\ngenerates one file per log type and per rotation timeframe. If you don\'t want to\\nwrangle files and directly process the output, this short blog post is for you.\\n\\n![Zeek as Pipeline](zeek-as-pipeline.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\nZeek requires a bit of adaptation to fit in the Unix pipeline model, by which we\\nmean *take your input on stdin and produce your output to stdout*:\\n\\n```\\n<upstream> | zeek | <downstream>\\n```\\n\\nIn this example, `<upstream>` produces packets in PCAP format and `<downstream>`\\nprocesses the Zeek logs. Let\'s work towards this.\\n\\nSolving the upstream part is easy: just use `zeek -r -` to read from stdin. So\\nlet\'s focus on the logs downstream. [Our last blog](/blog/mobilizing-zeek-logs)\\nintroduced the various logging formats, such as tab-separated values (TSV),\\nJSON, and Streaming JSON with an extra `_path` discriminator field. The only\\nformat conducive to multiplexing different log types is Streaming JSON.\\n\\nLet\'s see what we get:\\n\\n```bash\\nzcat < trace.pcap | zeek -r - json-streaming-logs\\n```\\n\\n```\\n\u276f ls\\njson_streaming_analyzer.1.log       json_streaming_packet_filter.1.log\\njson_streaming_conn.1.log           json_streaming_pe.1.log\\njson_streaming_dce_rpc.1.log        json_streaming_reporter.1.log\\njson_streaming_dhcp.1.log           json_streaming_sip.1.log\\njson_streaming_dns.1.log            json_streaming_smb_files.1.log\\njson_streaming_dpd.1.log            json_streaming_smb_mapping.1.log\\njson_streaming_files.1.log          json_streaming_snmp.1.log\\njson_streaming_http.1.log           json_streaming_ssl.1.log\\njson_streaming_kerberos.1.log       json_streaming_tunnel.1.log\\njson_streaming_ntlm.1.log           json_streaming_weird.1.log\\njson_streaming_ntp.1.log            json_streaming_x509.1.log\\njson_streaming_ocsp.1.log\\n```\\n\\nThe `json-streaming-package` prepends a distinguishing prefix to the filename.\\nThe `*.N.log` suffix counts the rotations, e.g., `*.1.log` means the logs from\\nthe first batch.\\n\\nLet\'s try to avoid the files altogether and send the contents of these file to\\nstdout. This requires a bit of option fiddling to achieve the desired result:\\n\\n```bash\\nzcat < trace.pcap |\\n  zeek -r - \\\\\\n    LogAscii::output_to_stdout=T \\\\\\n    JSONStreaming::disable_default_logs=T \\\\\\n    JSONStreaming::enable_log_rotation=F \\\\\\n    json-streaming-logs\\n```\\n\\nThis requires a bit explanation:\\n\\n- `LogAscii::output_to_stdout=T` redirects the log output to stdout.\\n- `JSONStreaming::disable_default_logs=T` disables the default TSV logs.\\n  Without this option, Zeek will print *both* TSV and NDJSON to stdout.\\n- `JSONStreaming::enable_log_rotation=F` disables log rotation. This is needed\\n  because the option `output_to_stdout=T` sets the internal filenames to\\n  `/dev/stdout`, which Zeek then tries to rotate away. Better not.\\n\\nHere\'s the result you\'d expect, which is basically a `cat *.log`:\\n\\n```json\\n{\\"_path\\":\\"files\\",\\"_write_ts\\":\\"2021-11-17T13:32:43.250616Z\\",\\"ts\\":\\"2021-11-17T13:32:43.250616Z\\",\\"fuid\\":\\"FhEFqzHx1hVpkhWci\\",\\"uid\\":\\"CHhfpE1dTbPgBTR24\\",\\"id.orig_h\\":\\"128.14.134.170\\",\\"id.orig_p\\":57468,\\"id.resp_h\\":\\"198.71.247.91\\",\\"id.resp_p\\":80,\\"source\\":\\"HTTP\\",\\"depth\\":0,\\"analyzers\\":[],\\"mime_type\\":\\"text/html\\",\\"duration\\":0.0,\\"is_orig\\":false,\\"seen_bytes\\":51,\\"total_bytes\\":51,\\"missing_bytes\\":0,\\"overflow_bytes\\":0,\\"timedout\\":false}\\n{\\"_path\\":\\"http\\",\\"_write_ts\\":\\"2021-11-17T13:32:43.250616Z\\",\\"ts\\":\\"2021-11-17T13:32:43.249475Z\\",\\"uid\\":\\"CHhfpE1dTbPgBTR24\\",\\"id.orig_h\\":\\"128.14.134.170\\",\\"id.orig_p\\":57468,\\"id.resp_h\\":\\"198.71.247.91\\",\\"id.resp_p\\":80,\\"trans_depth\\":1,\\"method\\":\\"GET\\",\\"host\\":\\"198.71.247.91\\",\\"uri\\":\\"/\\",\\"version\\":\\"1.1\\",\\"user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36 \\",\\"request_body_len\\":0,\\"response_body_len\\":51,\\"status_code\\":200,\\"status_msg\\":\\"OK\\",\\"tags\\":[],\\"resp_fuids\\":[\\"FhEFqzHx1hVpkhWci\\"],\\"resp_mime_types\\":[\\"text/html\\"]}\\n{\\"_path\\":\\"packet_filter\\",\\"_write_ts\\":\\"1970-01-01T00:00:00.000000Z\\",\\"ts\\":\\"2023-07-11T03:30:17.189787Z\\",\\"node\\":\\"zeek\\",\\"filter\\":\\"ip or not ip\\",\\"init\\":true,\\"success\\":true}\\n{\\"_path\\":\\"conn\\",\\"_write_ts\\":\\"2021-11-17T13:33:01.457108Z\\",\\"ts\\":\\"2021-11-17T13:32:46.565338Z\\",\\"uid\\":\\"CD868huwhDP636oT\\",\\"id.orig_h\\":\\"89.248.165.145\\",\\"id.orig_p\\":43831,\\"id.resp_h\\":\\"198.71.247.91\\",\\"id.resp_p\\":52806,\\"proto\\":\\"tcp\\",\\"conn_state\\":\\"S0\\",\\"missed_bytes\\":0,\\"history\\":\\"S\\",\\"orig_pkts\\":1,\\"orig_ip_bytes\\":40,\\"resp_pkts\\":0,\\"resp_ip_bytes\\":0}\\n{\\"_path\\":\\"tunnel\\",\\"_write_ts\\":\\"2021-11-17T13:40:34.891453Z\\",\\"ts\\":\\"2021-11-17T13:40:34.891453Z\\",\\"uid\\":\\"CsqzCG2F8VDR4gM3a8\\",\\"id.orig_h\\":\\"49.213.162.198\\",\\"id.orig_p\\":0,\\"id.resp_h\\":\\"198.71.247.91\\",\\"id.resp_p\\":0,\\"tunnel_type\\":\\"Tunnel::GRE\\",\\"action\\":\\"Tunnel::DISCOVER\\"}\\n```\\n\\nNobody can remember this invocation. Especially during firefighting when you\\nquickly need to plow through a trace to understand it. So we want to wrap this\\nsomehow:\\n\\n```bash title=zeekify\\n#!/bin/sh\\nzeek -r - \\\\\\n  LogAscii::output_to_stdout=T \\\\\\n  JSONStreaming::disable_default_logs=T \\\\\\n  JSONStreaming::enable_log_rotation=F \\\\\\n  json-streaming-logs \\\\\\n  \\"$@\\"\\n```\\n\\nNow we\'re in pipeline land:\\n\\n```bash\\nzcat pcap.gz | zeekify | head | jq -r ._path\\n```\\n\\n```\\npacket_filter\\nfiles\\nntp\\ntunnel\\nconn\\nntp\\nhttp\\nconn\\nntp\\nconn\\n```\\n\\nOkay, we got Zeek as a Unix pipe. But now you have to wrangle the JSON with\\n`jq`. Unless you\'re a die-hard fan, even simple analytics, like filtering or\\naggregating, have a steep learning curve. In the next blog post, we\'ll double\\ndown on the elegant principle of pipelines and show how you can take do easy\\nin-situ analytics with Tenzir."},{"id":"/mobilizing-zeek-logs","metadata":{"permalink":"/blog/mobilizing-zeek-logs","source":"@site/blog/mobilizing-zeek-logs/index.md","title":"Mobilizing Zeek Logs","description":"Zeek offers many ways to produce and consume logs. In this","date":"2023-07-06T00:00:00.000Z","formattedDate":"July 6, 2023","tags":[{"label":"tenzir","permalink":"/blog/tags/tenzir"},{"label":"zeek","permalink":"/blog/tags/zeek"},{"label":"logs","permalink":"/blog/tags/logs"},{"label":"json","permalink":"/blog/tags/json"},{"label":"kafka","permalink":"/blog/tags/kafka"}],"readingTime":7.695,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Mobilizing Zeek Logs","authors":"mavam","date":"2023-07-06T00:00:00.000Z","tags":["tenzir","zeek","logs","json","kafka"],"comments":true},"prevItem":{"title":"Zeek and Ye Shall Pipe","permalink":"/blog/zeek-and-ye-shall-pipe"},"nextItem":{"title":"Migrating from VAST to Tenzir","permalink":"/blog/migrating-from-vast-to-tenzir"}},"content":"[Zeek](https://zeek.org) offers many ways to produce and consume logs. In this\\nblog, we explain the various Zeek logging formats and show how you can get the\\nmost out of Zeek with Tenzir. We conclude with recommendations for when to use\\nwhat Zeek format based on your use case.\\n\\n![Packet to Logs](zeek-packets-to-logs.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Zeek Logging 101\\n\\nZeek\'s main function is turning live network traffic or trace files into\\nstructured logs.[^1] Zeek logs span the entire network stack, including\\nlink-layer analytics with MAC address to application-layer fingerprinting of\\napplications. These rich logs are invaluable for any network-based detection and\\nresponse activities. Many users also simply appreciate the myriad of protocol\\nanalyzers, each of which generates a dedicated log file, like `smb.log`,\\n`http.log`, `x509.log`, and others.\\n\\n[^1]: Zeek also comes with a Turing-complete scripting language for executing\\narbitrary logic. The event-based language resembles Javascript and is\\nespecially useful for performing in-depth protocol analysis and engineering\\ndetections.\\n\\nIn the default configuration, Zeek writes logs into the current directory, one\\nfile per log type. There are various file formats to choose from, such as TSV,\\nJSON, and others. Let\'s take a look.\\n\\n### Tab-Separated Values (TSV)\\n\\nZeek\'s custom tab-separated value (TSV) format is variant of CSV with additional\\nmetadata, similar to a data frame.\\n\\nHere\'s how you create TSV logs from a trace:\\n\\n```bash\\nzeek -C -r trace.pcap [scripts]\\n```\\n\\n:::info disable checksumming\\nWe add `-C` to disable checksumming, telling Zeek to ignore mismatches and use\\nall packets in the trace. This is good practice to process all packets in a\\ntrace, as some capturing setups may perturb the checksums.\\n:::\\n\\nAnd here\'s a snippet of the corresponding `conn.log`:\\n\\n```\\n#separator \\\\x09\\n#set_separator\\t,\\n#empty_field\\t(empty)\\n#unset_field\\t-\\n#path\\tconn\\n#open\\t2019-06-07-14-30-44\\n#fields\\tts\\tuid\\tid.orig_h\\tid.orig_p\\tid.resp_h\\tid.resp_p\\tproto\\tservice\\tduration\\torig_bytes\\tresp_bytes\\tconn_state\\tlocal_orig\\tlocal_resp\\tmissed_bytes\\thistory\\torig_pkts\\torig_ip_bytes\\tresp_pkts\\tresp_ip_bytes\\ttunnel_parents\\tcommunity_id\\n#types\\ttime\\tstring\\taddr\\tport\\taddr\\tport\\tenum\\tstring\\tinterval\\tcount\\tcount\\tstring\\tbool\\tbool\\tcount\\tstring\\tcount\\tcount\\tcount\\tcount\\tset[string]\\tstring\\n1258531221.486539\\tCz8F3O3rmUNrd0OxS5\\t192.168.1.102\\t68\\t192.168.1.6\\t7\\tudp\\tdhcp\\t0.163820\\t301\\t300\\tSF\\t-\\t-\\t0\\tDd\\t1\\t329\\t1\\t328\\t-\\t1:aWZfLIquYlCxKGuJ62fQGlgFzAI=\\n1258531680.237254\\tCeJFOE1CNssyQjfJo1\\t192.168.1.103\\t137\\t192.168.1.255\\t137\\tudp\\tdns\\t3.780125\\t350\\t0\\tS0\\t-\\t-\\t546\\t0\\t0\\t-\\t1:fLbpXGtS1VgDhqUW+WYaP0v+NuA=\\n```\\n\\nAnd here\'s a `http.log` with a different header:\\n\\n```\\n#separator \\\\x09\\n#set_separator\\t,\\n#empty_field\\t(empty)\\n#unset_field\\t-\\n#path\\thttp\\n#open\\t2019-06-07-14-30-44\\n#fields\\tts\\tuid\\tid.orig_h\\tid.orig_p\\tid.resp_h\\tid.resp_p\\ttrans_depth\\tmethod\\thost\\turi\\treferrer\\tversion\\tuser_agent\\trequest_body_len\\tresponse_body_len\\tstatus_code\\tstatus_msg\\tinfo_code\\tinfo_msg\\ttags\\tusername\\tpassword\\tproxied\\torig_fuids\\torig_filenames\\torig_mime_types\\tresp_fuids\\tresp_filenames\\tresp_mime_types\\n#types\\ttime\\tstring\\taddr\\tport\\taddr\\tport\\tcount\\tstring\\tstring\\tstring\\tstring\\tstring\\tstring\\tcount\\tcount\\tcount\\tstring\\tcount\\tstring\\tset[enum]\\tstring\\tstring\\tset[string]\\tvector[string]\\tvector[string]\\tvector[string]\\tvector[string]\\tvector[string]\\tvector[string]\\n1258535653.087137\\tCUk3vSsgfU9oCghL4\\t192.168.1.104\\t1191\\t65.54.95.680\\t1\\tHEAD\\tdownload.windowsupdate.com\\t/v9/windowsupdate/redir/muv4wuredir.cab?0911180916\\t-\\t1.1\\tWindows-Update-Agent\\t0\\t0\\t20OK\\t-\\t-\\t(empty)\\t-\\t-\\t-\\t-\\t-\\t-\\t-\\t-\\n1258535655.525107\\tCc6Alh3FtTOAqNSIx2\\t192.168.1.104\\t1192\\t65.55.184.16\\t80\\t1\\tHEAD\\twww.update.microsoft.com\\t/v9/windowsupdate/selfupdate/wuident.cab?0911180916\\t-\\t1.1\\tWindows-Update-Agent\\t0\\t200\\tOK\\t-\\t-\\t(empty)\\t-\\t-\\t-\\t-\\t-\\t-\\t-\\n```\\n\\nMany Zeek users would now resort to their downstream log management tool,\\nassuming it supports the custom TSV format. Zeek also comes with small helper\\nutility `zeek-cut` for light-weight reshaping of this TSV format. For example:\\n\\n```bash\\nzeek-cut id.orig_h id.resp_h < conn.log\\n```\\n\\nThis selects the columns `id.orig_h` and `id.resp_h`. Back in the days, many\\nfolks used `awk` to extract fields by their position, e.g., with `$4`, `$7`,\\n`$9`. This is not only difficult to understand, but also brittle, since Zeek\\nschemas can change based on configuration. With `zeek-cut`, it\'s at least a bit\\nmore robust.\\n\\nTenzir\'s data pipelines make it easy to process Zeek logs. The native\\n[`zeek-tsv`](/next/formats/zeek-tsv) parser converts them into data frames, so\\nthat you can process them with a wide range of [operators](/next/operators):\\n\\n```bash\\ncat *.log | tenzir \'read zeek-tsv | select id.orig_h, id.resp_h\'\\n```\\n\\nTenzir takes care of parsing the type information properly and keeps IP\\naddresses and timestamps as native data types. You can also see in the examples\\nthat Tenzir handles multiple concatenated TSV logs of different schemas as you\'d\\nexpect.\\n\\nNow that Zeek logs are flowing, you can do a lot more than selecting specific\\ncolumns. Check out the [reshaping guide](/next/user-guides/reshape-data) for\\nfiltering rows, performing aggregations, and routing them elsewhere. Or [store\\nthe logs](/next/user-guides/import-into-a-node) locally at a Tenzir node in\\n[Parquet](https://parquet.apache.org) to process them with other data tools.\\n\\n### JSON\\n\\nZeek can also render logs as JSON by setting\\n[`LogAscii::use_json=T`](https://docs.zeek.org/en/master/frameworks/logging.html):\\n\\n```bash\\nzeek -r trace.pcap LogAscii::use_json=T\\n```\\n\\nAs with TSV, this generates one file per log type containing the NDJSON records.\\nHere are the same two entries from above:\\n\\n```json\\n{\\"ts\\":1258531221.486539,\\"uid\\":\\"C8b0xF1gjm7rOZXemg\\",\\"id.orig_h\\":\\"192.168.1.102\\",\\"id.orig_p\\":68,\\"id.resp_h\\":\\"192.168.1.1\\",\\"id.resp_p\\":67,\\"proto\\":\\"udp\\",\\"service\\":\\"dhcp\\",\\"duration\\":0.1638200283050537,\\"orig_bytes\\":301,\\"resp_bytes\\":300,\\"conn_state\\":\\"SF\\",\\"missed_bytes\\":0,\\"history\\":\\"Dd\\",\\"orig_pkts\\":1,\\"orig_ip_bytes\\":329,\\"resp_pkts\\":1,\\"resp_ip_bytes\\":328}\\n{\\"ts\\":1258531680.237254,\\"uid\\":\\"CMsxKW3uTZ3tSLsN0g\\",\\"id.orig_h\\":\\"192.168.1.103\\",\\"id.orig_p\\":137,\\"id.resp_h\\":\\"192.168.1.255\\",\\"id.resp_p\\":137,\\"proto\\":\\"udp\\",\\"service\\":\\"dns\\",\\"duration\\":3.780125141143799,\\"orig_bytes\\":350,\\"resp_bytes\\":0,\\"conn_state\\":\\"S0\\",\\"missed_bytes\\":0,\\"history\\":\\"D\\",\\"orig_pkts\\":7,\\"orig_ip_bytes\\":546,\\"resp_pkts\\":0,\\"resp_ip_bytes\\":0}\\n```\\n\\nAnd `http.log`:\\n\\n```json\\n{\\"ts\\":1258535653.087137,\\"uid\\":\\"CDsoEy4cHSHJRBvilg\\",\\"id.orig_h\\":\\"192.168.1.104\\",\\"id.orig_p\\":1191,\\"id.resp_h\\":\\"65.54.95.64\\",\\"id.resp_p\\":80,\\"trans_depth\\":1,\\"method\\":\\"HEAD\\",\\"host\\":\\"download.windowsupdate.com\\",\\"uri\\":\\"/v9/windowsupdate/redir/muv4wuredir.cab?0911180916\\",\\"version\\":\\"1.1\\",\\"user_agent\\":\\"Windows-Update-Agent\\",\\"request_body_len\\":0,\\"response_body_len\\":0,\\"status_code\\":200,\\"status_msg\\":\\"OK\\",\\"tags\\":[]}\\n{\\"ts\\":1258535655.525107,\\"uid\\":\\"C8muAY3KSDGScVUrO4\\",\\"id.orig_h\\":\\"192.168.1.104\\",\\"id.orig_p\\":1192,\\"id.resp_h\\":\\"65.55.184.16\\",\\"id.resp_p\\":80,\\"trans_depth\\":1,\\"method\\":\\"HEAD\\",\\"host\\":\\"www.update.microsoft.com\\",\\"uri\\":\\"/v9/windowsupdate/selfupdate/wuident.cab?0911180916\\",\\"version\\":\\"1.1\\",\\"user_agent\\":\\"Windows-Update-Agent\\",\\"request_body_len\\":0,\\"response_body_len\\":0,\\"status_code\\":200,\\"status_msg\\":\\"OK\\",\\"tags\\":[]}\\n```\\n\\nUse the regular [`json`](/next/formats/json) parser to get the data flowing:\\n\\n```bash\\ncat conn.log | tenzir \'read json --schema \\"zeek.conn\\" | head\'\\ncat http.log | tenzir \'read json --schema \\"zeek.http\\" | head\'\\n```\\n\\nThe option `--schema` of the `json` reader passes a name of a known schema that\\nbrings back the lost typing, e.g., the schema knows that the `duration` field in\\n`conn.log` is not a floating-point number, but a duration type, so that you can\\nfilter connections with `where duration < 4 mins`.\\n\\n### Streaming JSON\\n\\nThe above one-file-per-log format is not conducive to stream processing because\\na critical piece of information is missing: the type of the log (or *schema*),\\nwhich is only contained in the file name. So you can\'t just ship the data away\\nand infer the type later at ease. And passing the filename around through a side\\nchannel is cumbersome. Enter [JSON streaming\\nlogs](https://github.com/corelight/json-streaming-logs). This package adds two\\nnew fields: `_path` with the log type and `_write_ts` with the timestamp when\\nthe log was written. For example, `http.log` now gets an additional field\\n`{\\"_path\\": \\"http\\" , ...}`. This makes it a lot easier to consume, because you\\ncan now concatenate the entire output and multiplex it over a single stream.\\n\\nThis format doesn\'t come with stock Zeek. Use Zeek\'s package manager `zkg` to\\ninstall it:\\n\\n```bash\\nzkg install json-streaming-logs\\n```\\n\\nThen pass the package name to the list of scripts on the command line:\\n\\n```bash\\nzeek -r trace.pcap json-streaming-logs\\n```\\n\\nAnd now you get JSON logs in the current directory. Here\'s the same `conn.log`\\nand `http.log` example from above, this time with added `_path` and `_write_ts`\\nfields:\\n\\n```json title=\\"conn.log\\"\\n{\\"_path\\":\\"conn\\",\\"_write_ts\\":\\"2009-11-18T16:45:06.678526Z\\",\\"ts\\":\\"2009-11-18T16:43:56.223671Z\\",\\"uid\\":\\"CzFMRp2difzeGYponk\\",\\"id.orig_h\\":\\"192.168.1.104\\",\\"id.orig_p\\":1387,\\"id.resp_h\\":\\"74.125.164.85\\",\\"id.resp_p\\":80,\\"proto\\":\\"tcp\\",\\"service\\":\\"http\\",\\"duration\\":65.45066595077515,\\"orig_bytes\\":694,\\"resp_bytes\\":11708,\\"conn_state\\":\\"SF\\",\\"missed_bytes\\":0,\\"history\\":\\"ShADadfF\\",\\"orig_pkts\\":9,\\"orig_ip_bytes\\":1062,\\"resp_pkts\\":14,\\"resp_ip_bytes\\":12276}\\n```\\n\\n```json title=\\"http.log\\"\\n {\\"_path\\":\\"http\\",\\"_write_ts\\":\\"2009-11-18T17:00:51.888304Z\\",\\"ts\\":\\"2009-11-18T17:00:51.841527Z\\",\\"uid\\":\\"CgdQsm2eBBV8T8GjUk\\",\\"id.orig_h\\":\\"192.168.1.103\\",\\"id.orig_p\\":1399,\\"id.resp_h\\":\\"74.125.19.104\\",\\"id.resp_p\\":80,\\"trans_depth\\":1,\\"method\\":\\"GET\\",\\"host\\":\\"www.google.com\\",\\"uri\\":\\"/\\",\\"version\\":\\"1.1\\",\\"user_agent\\":\\"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)\\",\\"request_body_len\\":0,\\"response_body_len\\":10205,\\"status_code\\":200,\\"status_msg\\":\\"OK\\",\\"tags\\":[],\\"resp_fuids\\":[\\"FI1gWL1b9SuIA8HAv3\\"],\\"resp_mime_types\\":[\\"text/html\\"]}\\n```\\n\\nMany tools have have logic to disambiguate based on a field like `_path`. That\\nsaid, JSON is always \\"dumbed down\\" compared to TSV, which contains additional\\ntype information, such as timestamps, durations, IP addresses, etc. This type\\ninformation is lost in the JSON output and up to the downstream tooling to bring\\nback.\\n\\nWith JSON Streaming logs, you can simply concatenate all logs Zeek generated and\\npass them to a tool of your choice. Tenzir has native support for these logs via\\nthe [`zeek-json`](/next/formats/zeek-json) parser:\\n\\n```bash\\ncat *.log | tenzir \'read zeek-json | taste 1\'\\n```\\n\\nIn fact, the `zeek-json` parser just an alias for `json --selector=zeek:_path`,\\nwhich extracts the schema name from the `_path` field to demultiplex the JSON\\nstream and assign the corresponding schema.\\n\\n### Writer Plugin\\n\\nIf the stock options of Zeek\'s logging framework do not work for you, you can\\nstill write a C++ *writer plugin* to produce any output of your choice.\\n\\nFor example, the [zeek-kafka](https://github.com/SeisoLLC/zeek-kafka) plugin\\nwrites incoming Zeek data to Kafka topics. For this use case, you can also\\nleverage Tenzir\'s [`kafka`](/next/connectors/kafka) connector and write:\\n\\n```bash\\ncat *.log | tenzir \'\\n  read zeek-tsv\\n  | extend _path=#schema\\n  | to kafka -t zeek write json\\n  \'\\n```\\n\\nThis pipeline starts by reading Zeek TSV, appends the `_path` field to emulate\\nStreaming JSON, and then writes the events to the Kafka topic `zeek`. The\\nexample is not equivalent to the Zeek Kafka plugin, because concatenate existing\\nfields and apply a (one-shot) pipeline, as opposed to continuously streaming to\\na Kafka topic. We\'ll elaborate on this in the next blog post, stay tuned.\\n\\n## Conclusion\\n\\nIn this blog, we presented the most common Zeek logging formats. We also\\nprovided examples how you can mobilize any of them in a Tenzir pipeline. If\\nyou\'re unsure when to use what Zeek logging format, here are our\\nrecommendations:\\n\\n:::tip Recommendation\\n- **Use TSV when you can.** If your downstream tooling can parse TSV, it is the\\n  best choice because it retains Zeek\'s rich type annotations as\\n  metadata\u2014without the need for downstream schema wrangling.\\n- **Use Streaming JSON for the easy button**. The single stream of NDJSON\\n  logs is most versatile, since most downstream tooling supports it well. Use it\\n  when you need to get in business quickly.\\n- **Use stock JSON when you must**. There\'s marginal utility in the\\n  one-JSON-file-per-log format. It requires extra effort in keeping track of\\n  filenames and mapping fields to their corresponding types.\\n- **Use plugins for everything else**. If none of these fit the bill or you\\n  need a tighter integration, leverage Zeek\'s writer plugins to create a custom\\n  logger.\\n:::\\n\\nIf you\'re a Zeek power user and need power tools for data processing, take a\\ncloser look at what we do at [Tenzir](https://tenzir.com). There\'s a lot more\\nyou can do!"},{"id":"/migrating-from-vast-to-tenzir","metadata":{"permalink":"/blog/migrating-from-vast-to-tenzir","source":"@site/blog/migrating-from-vast-to-tenzir/index.md","title":"Migrating from VAST to Tenzir","description":"VAST is now Tenzir. This blog post describes what changed when [we renamed the","date":"2023-06-26T00:00:00.000Z","formattedDate":"June 26, 2023","tags":[{"label":"tenzir","permalink":"/blog/tags/tenzir"},{"label":"vast","permalink":"/blog/tags/vast"},{"label":"community","permalink":"/blog/tags/community"},{"label":"project","permalink":"/blog/tags/project"}],"readingTime":1.55,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Migrating from VAST to Tenzir","authors":"dominiklohmann","date":"2023-06-26T00:00:00.000Z","tags":["tenzir","vast","community","project"]},"prevItem":{"title":"Mobilizing Zeek Logs","permalink":"/blog/mobilizing-zeek-logs"},"nextItem":{"title":"Visibility Across Space and Time is now Tenzir","permalink":"/blog/vast-to-tenzir"}},"content":"VAST is now Tenzir. This blog post describes what changed when [we renamed the\\nproject](/blog/vast-to-tenzir).\\n\\n![VAST to Tenzir](vast-to-tenzir.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## TL;DR\\n\\n- Use `tenzir-node` instead of `vast start`.\\n- Use `tenzir` instead of `vast exec`.\\n- Use `tenzir-ctl` for all other commands.\\n- Move your configuration from `<prefix>/etc/vast/vast.yaml` to\\n  `<prefix>/etc/tenzir/tenzir.yaml`.\\n- Move your configuration from `$XDG_CONFIG_HOME/vast/vast.yaml` to\\n  `$XDG_CONFIG_HOME/tenzir/tenzir.yaml`.\\n- In your configuration, replace `vast:` with `tenzir:`.\\n- Prefix environment variables with `TENZIR_` instead of `VAST_`.\\n\\nIn addition to that, the following things have changed.\\n\\n## Project\\n\\n- The repository moved from `tenzir/vast` to `tenzir/tenzir`.\\n- Our Discord server is now the *Tenzir Community*. Join us at\\n  <https://discord.tenzir.com>!\\n- The documentation moved from [vast.io](https://vast.io) to\\n  [docs.tenzir.com](https://docs.tenzir.com).\\n\\n## Usage\\n\\n- We\'re making the split between starting a node and starting a pipeline more\\n  obvious:\\n  - `tenzir` executes pipelines (previously `vast exec`).\\n  - `tenzir-node` starts a node (previously `vast start`).\\n  - Some commands have not yet been ported over to pipelines, and are accessible\\n    under `tenzir-ctl`; this will be phased out over time without deprecation\\n    notices as commands are moving into pipeline operators.\\n  - The `vast` executable exists for drop-in backwards compatibility and is\\n    equivalent to running `tenzir-ctl`.\\n- Configuration moved to use `tenzir` over `vast` where possible.\\n- Packages are now called *Tenzir* instead of *VAST*.\\n- The default install prefix of packages moved from `/opt/vast` to `/opt/tenzir`.\\n- The Docker image now includes the proprietary plugins\\n- There exit separate Docker images `tenzir/tenzir` and `tenzir/tenzir-node` to\\n  match the new binaries `tenzir` and `tenzir-node`, respectively.\\n- The PyVAST package is deprecated and now called Tenzir. We will bring it back\\n  with the Tenzir v4.0 release.\\n- The interop with Apache Arrow uses `tenzir.` prefixes for the extension type\\n  names now. We support reading the old files transparently, but tools\\n  interfacing will need to adapt to the new names `tenzir.ip`, `tenzir.subnet`,\\n  and `tenzir.enumeration`."},{"id":"/vast-to-tenzir","metadata":{"permalink":"/blog/vast-to-tenzir","source":"@site/blog/vast-to-tenzir/index.md","title":"Visibility Across Space and Time is now Tenzir","description":"Renaming our project from VAST to Tenzir","date":"2023-06-20T00:00:00.000Z","formattedDate":"June 20, 2023","tags":[{"label":"tenzir","permalink":"/blog/tags/tenzir"},{"label":"vast","permalink":"/blog/tags/vast"},{"label":"community","permalink":"/blog/tags/community"},{"label":"project","permalink":"/blog/tags/project"}],"readingTime":1.715,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Visibility Across Space and Time is now Tenzir","description":"Renaming our project from VAST to Tenzir","authors":"mavam","date":"2023-06-20T00:00:00.000Z","tags":["tenzir","vast","community","project"]},"prevItem":{"title":"Migrating from VAST to Tenzir","permalink":"/blog/migrating-from-vast-to-tenzir"},"nextItem":{"title":"VAST v3.1","permalink":"/blog/vast-v3.1"}},"content":"After 5 years of developing two identities, the VAST project and Tenzir\\nthe company, we decided to streamline our efforts and **rename VAST to Tenzir**.\\n\\n\x3c!--truncate--\x3e\\n\\nVAST is the open-source software project Tenzir\'s Founder and CEO, Matthias\\nVallentin, created originally during his Master\'s at Technical University Munich\\nin 2006, and then continued to work on throughout his PhD from 2008 to 2016 at\\nthe University of California, Berkeley. The name is an acronym for Visibility\\nAcross Space and Time and originates from the HotSec\'08 paper about [Principles\\nfor Developing Comprehensive Network\\nVisibility](https://www.icir.org/mallman/papers/awareness-hotsec08.pdf) by Mark\\nAllman, Christian Kreibich, Vern Paxson, Robin Sommer, and Nicholas Weaver.\\n\\nThe following reasons ultimately drove our decision to rename VAST:\\n\\n- **Building two brands is double the effort**: we found that building two\\n  brands\u2014one for the VAST project and one for Tenzir as a company\u2014is a challenge\\n  for an early-stage startup like ourselves.\\n- **Taking a broader view when it comes to open source**: we also don\'t see our\\n  enterprise and open-source users as separate and isolated groupings. Instead\\n  we see a continuum. So we want to put all of our energy into a single, unified\\n  community.\\n- **Avoiding confusion and conflicts**: over the past years the term VAST has\\n  become very popular and widely used for a number of different projects and in\\n  adjacent industries (VAST Data, VAST.ai, anyone?) to avoid existing or future\\n  confusion we decided that it was better to retire the name but keep it part of\\n  our legacy and our story.\\n\\nOur next release will be Tenzir v4.0, continuing from the VAST v3.x series. In\\nother respects as well, the transition is an evolution, not a revolution. All\\nproject-related infrastructure, including the SemVer versioning, Git history,\\nGitHub repository, and so forth will remain the same. Stay tuned for a follow-up\\nblog post where we discuss the technical changes in depth and provide migration\\ninstructions.\\n\\nOh, and if you have any nostalgic anecdotes or stories to share, we\'d love to\\nhear them! Chime in on our [Discord chat](/discord) if you have any questions or\\nfeedback."},{"id":"/vast-v3.1","metadata":{"permalink":"/blog/vast-v3.1","source":"@site/blog/vast-v3.1/index.md","title":"VAST v3.1","description":"VAST v3.1 is out. This is","date":"2023-05-12T00:00:00.000Z","formattedDate":"May 12, 2023","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"pipelines","permalink":"/blog/tags/pipelines"},{"label":"operators","permalink":"/blog/tags/operators"}],"readingTime":1.84,"hasTruncateMarker":true,"authors":[{"name":"Tobias Mayer","title":"Software Architect","url":"https://github.com/tobim","email":"tobias@tenzir.com","imageURL":"https://github.com/tobim.png","key":"tobim"}],"frontMatter":{"title":"VAST v3.1","authors":["tobim"],"image":"/img/blog/vast-v3.1.excalidraw.svg","date":"2023-05-12T00:00:00.000Z","tags":["release","pipelines","operators"]},"prevItem":{"title":"Visibility Across Space and Time is now Tenzir","permalink":"/blog/vast-to-tenzir"},"nextItem":{"title":"VAST v3.0","permalink":"/blog/vast-v3.0"}},"content":"[VAST v3.1](https://github.com/tenzir/vast/releases/tag/v3.1.0) is out. This is\\na small checkpointing release that brings a few new changes and fixes.\\n\\n\x3c!--truncate--\x3e\\n\\n## Pipelines Reloaded\\n\\nThe old pipeline execution engine is now gone and we updated VAST to use\\nthe new engine everywhere. Most notably this applies to the `export` command,\\nthe compaction engine, and the `query` REST interface.\\n\\nFor this release, we removed support for configuration level export and import\\npipelines. This feature will make a return in the next major release.\\n\\nWe also removed the deprecated YAML-based pipeline syntax to fully concentrate\\non the VAST Language.\\n\\n## Operator Updates\\n\\nWe introduced several new operators:\\n\\n- [`tail`](/next/operators/transformations/tail): limits the\\n  input to the last N events.\\n- [`unique`](/next/operators/transformations/unique): removes\\n  adjacent duplicates\\n- [`measure`](/next/operators/transformations/measure): replaces\\n  the input with incremental metrics describing the input.\\n- [`version`](/next/operators/sources/version): returns a single\\n  event displaying version information of VAST.\\n- [`from`](/next/operators/sources/from): produces events by\\n  combining a connector and a format.\\n- [`read`](/VAST v3.1/understand/operators/sources/read): a short form of `from`\\n  that allows for omitting the connector.\\n- [`to`](/next/operators/sinks/to): consumes events by combining\\n  a connector and format.\\n- [`write`](/VAST v3.1/understand/operators/sinks/write): a short form of `to`\\n  that allows for omitting the connector.\\n\\nAdditionally, the `put`, `replace`, and `extend` operators have been updated to\\nwork with selectors and extractors. Check out the [growing list of\\noperators](/next/operators/).\\n\\n## Operator Aliases\\n\\nYou can now define aliases for operators in the configuration file. Use it to\\nassign a short and reusable name for operators that would otherwise require\\nseveral arguments. For example:\\n\\n```yaml\\nvast:\\n  operators:\\n    aggregate_flows: |\\n       summarize\\n         pkts_toserver=sum(flow.pkts_toserver),\\n         pkts_toclient=sum(flow.pkts_toclient),\\n         bytes_toserver=sum(flow.bytes_toserver),\\n         bytes_toclient=sum(flow.bytes_toclient),\\n         start=min(flow.start),\\n         end=max(flow.end)\\n       by\\n         timestamp,\\n         src_ip,\\n         dest_ip\\n       resolution\\n         10 mins\\n```\\n\\nNow use it like a regular operator in a pipeline:\\n\\n```\\nfrom file read suricata | aggregate_flows\\n```\\n\\n## Notable Fixes\\n\\n### Improved IPv6 Subnet Handling\\n\\nThe handling of subnets in the IPv6 space received multiple fixes:\\n\\n- The expression `:ip !in ::ffff:0:0/96` now finds all events that\\n  contain IPs that cannot be represented as IPv4 addresses.\\n- Subnets with a prefix above 32 are now correctly formatted with\\n  an IPv6 network part, even if the address is representable as IPv4.\\n\\n### A More Resilient Systemd Service\\n\\nThe systemd unit for VAST now automatically restarts the node in case the\\nprocess went down."},{"id":"/vast-v3.0","metadata":{"permalink":"/blog/vast-v3.0","source":"@site/blog/vast-v3.0/index.md","title":"VAST v3.0","description":"VAST Language Evolution \u2014 Dataflow Pipelines","date":"2023-03-14T00:00:00.000Z","formattedDate":"March 14, 2023","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"pipelines","permalink":"/blog/tags/pipelines"},{"label":"language","permalink":"/blog/tags/language"},{"label":"cef","permalink":"/blog/tags/cef"},{"label":"performance","permalink":"/blog/tags/performance"},{"label":"introspection","permalink":"/blog/tags/introspection"},{"label":"regex","permalink":"/blog/tags/regex"}],"readingTime":9.17,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"},{"name":"Daniel Kostuj","title":"Software Engineer","url":"https://github.com/dakostu","email":"daniel@tenzir.com","imageURL":"https://github.com/dakostu.png","key":"dakostu"}],"frontMatter":{"title":"VAST v3.0","description":"VAST Language Evolution \u2014 Dataflow Pipelines","authors":["dominiklohmann","dakostu"],"image":"/img/blog/building-blocks.excalidraw.svg","date":"2023-03-14T00:00:00.000Z","tags":["release","pipelines","language","cef","performance","introspection","regex"]},"prevItem":{"title":"VAST v3.1","permalink":"/blog/vast-v3.1"},"nextItem":{"title":"From Slack to Discord","permalink":"/blog/from-slack-to-discord"}},"content":"[VAST v3.0][github-vast-release] is out. This release brings some major updates\\nto the the VAST language, making it easy to write down dataflow pipelines that\\nfilter, reshape, aggregate, and enrich security event data. Think of VAST as\\nsecurity data pipelines plus open storage engine.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v3.0.4\\n\\n\x3c!--truncate--\x3e\\n\\n![Pipelines and Storage](/img/blog/building-blocks.excalidraw.svg)\\n\\n## The VAST Language: Dataflow Pipelines\\n\\nStarting with v3.0, VAST introduces a new way to write pipelines, with a syntax\\nsimilar to [splunk](https://splunk.com), [Kusto][kusto],\\n[PRQL](https://prql-lang.org/), and [Zed](https://zed.brimdata.io/). Previously,\\nVAST only supported a YAML-like definition of pipelines in configuration files\\nto deploy them statically during import, export, or use them during compaction.\\n\\n[kusto]: https://learn.microsoft.com/en-us/azure/data-explorer/kusto/query/\\n\\nThe new syntax resembles the well-known Unix paradigm of command chaining. The\\ndifference to Unix pipelines is that VAST exchanges structured data between\\noperators. The `vast export` and `vast import` commands now accept such a\\npipeline as a string argument. Refer to the [pipelines\\ndocumentation][pipeline-doc] for more details on how to use the new pipeline\\nsyntax.\\n\\n:::info Pipeline YAML Syntax Deprecation\\nThis release introduces a transitional period from YAML-style to textual\\npipelines. The old YAML syntax for pipelines will be deprecated and removed\\naltogether in a future version. The new pipeline operators [`head`][head-op] and\\n[`taste`][taste-op] have no YAML equivalent.\\n:::\\n\\n[pipeline-doc]: /VAST%20v3.0/understand/language/pipelines#define-a-pipeline\\n[head-op]: /VAST%20v3.0/understand/language/operators/head\\n[taste-op]: /VAST%20v3.0/understand/language/operators/taste\\n\\n## Language Upgrades\\n\\nWe\'ve made some breaking changes to the the VAST language that we\'ve wanted to\\ndo for a long time. Here\'s a summary:\\n\\n1. We removed the term VASTQL: The VAST Query Language is now simply\\n   the VAST language, and \\"VAST\\" will supersede the \\"VASTQL\\" abbreviation.\\n\\n2. Several built-in types have a new name:\\n\\n   - `int` \u2192 `int64`\\n   - `count` \u2192 `uint64`\\n   - `real` \u2192 `double`\\n   - `addr` \u2192 `ip`\\n\\n   The old names are still supported for the time being, but trigger a\\n   warning on startup. We will remove support for the old names in a future\\n   release.\\n\\n3. The match operator `~` and its negated form `!~` no longer exist. Use `==`\\n   and `!=` instead to perform searches with regular expressions, e.g., `url ==\\n   /^https?.*/`. Such queries now work for all string fields in addition to the\\n   previously supported `#type` meta extractor.\\n\\n4. We removed the `#field` meta extractor. That is, queries of the form `#field\\n   == \\"some.field.name\\"` no longer work. Use `some.field.name != null` or the\\n   new short form `some.field.name` to check for field existence moving forward.\\n\\n5. We renamed the boolean literal values `T` and `F` to `true` and `false`,\\n   respectively. For example the query `suricata.alert.alerted == T` is no\\n   longer valid; use `suricata.alert.alerted == true` instead.\\n\\n6. We renamed the non-value literal value `nil` to `null`. For example the\\n   query `x != nil` is no longer valid; use `x != null` instead.\\n\\n7. The `map` type no longer exists: Instead of `map<T, U>`, use the equivalent\\n   `list<record{ key: T, value: U }>`.\\n\\nOur goal is for these changes to make the query language feel more natural to\\nour users. We\'ve got [big plans][rfc-001] on how to extend it\u2014and this felt very\\nmuch necessary as a preparatory step to making the language more useful.\\n\\n[rfc-001]: https://github.com/tenzir/vast/blob/main/rfc/001-composable-pipelines/README.md\\n\\n## Regular Expression Evaluation\\n\\nVAST now supports searching with regular expressions. For example, let\'s say you\\nare looking for all events that contain a GUID surrounded by braces whose third\\nand fourth section are `5ec2-7015`:\\n\\n```json {0} title=\\"vast export -n 1 json \'/\\\\{[0-9a-f]{8}-[0-9a-f]\\\\{4}-5ec2-7015-[0-9a-f]\\\\{12}\\\\}/\'\\"\\n{\\n  \\"RuleName\\": \\"-\\",\\n  \\"UtcTime\\": \\"2020-05-18T09:42:40.443000\\",\\n  \\"ProcessGuid\\": \\"{8bcf3217-5890-5ec2-7015-00000000b000}\\",\\n  \\"ProcessId\\": 172,\\n  \\"Image\\": \\"C:\\\\\\\\Windows\\\\\\\\System32\\\\\\\\conhost.exe\\",\\n  \\"FileVersion\\": \\"10.0.17763.1075 (WinBuild.160101.0800)\\",\\n  \\"Description\\": \\"Console Window Host\\",\\n  \\"Product\\": \\"Microsoft\xae Windows\xae Operating System\\",\\n  \\"Company\\": \\"Microsoft Corporation\\",\\n  \\"OriginalFileName\\": \\"CONHOST.EXE\\",\\n  \\"CommandLine\\": \\"\\\\\\\\??\\\\\\\\C:\\\\\\\\Windows\\\\\\\\system32\\\\\\\\conhost.exe 0xffffffff -ForceV1\\",\\n  \\"CurrentDirectory\\": \\"C:\\\\\\\\Windows\\",\\n  \\"User\\": \\"NT AUTHORITY\\\\\\\\SYSTEM\\",\\n  \\"LogonGuid\\": \\"{8bcf3217-54f5-5ebe-e703-000000000000}\\",\\n  \\"LogonId\\": 999,\\n  \\"TerminalSessionId\\": 0,\\n  \\"IntegrityLevel\\": \\"System\\",\\n  \\"Hashes\\": \\"SHA1=74F28DD9B0DA310D85F1931DB2749A26A9A8AB02\\",\\n  \\"ParentProcessGuid\\": \\"{8bcf3217-5890-5ec2-6f15-00000000b000}\\",\\n  \\"ParentProcessId\\": 3440,\\n  \\"ParentImage\\": \\"C:\\\\\\\\Windows\\\\\\\\System32\\\\\\\\OpenSSH\\\\\\\\sshd.exe\\",\\n  \\"ParentCommandLine\\": \\"\\\\\\"C:\\\\\\\\Windows\\\\\\\\System32\\\\\\\\OpenSSH\\\\\\\\sshd.exe\\\\\\" \\\\\\"-R\\\\\\"\\"\\n}\\n```\\n\\n:::tip Case-Insensitive Patterns\\nIn addition to writing `/pattern/`, you can specify a regular expression that\\nignores the casing of characters via `/pattern/i`. The `/i` flag is currently\\nthe only support pattern modifier.\\n:::\\n\\n## Revamped Status for Event Distribution\\n\\nThe event distribution statistics moved within the output of `vast status`.\\n\\nThey were previously available under the `index.statistics` section when using\\nthe `--detailed` option:\\n\\n```json {0} title=\\"VAST v2.4.1 \u276f vast status --detailed | jq .index.statistics\\"\\n{\\n  \\"events\\": {\\n    \\"total\\": 42\\n  },\\n  \\"layouts\\": {\\n    \\"suricata.alert\\": {\\n      \\"count\\": 1,\\n      \\"percentage\\": 2.4\\n    },\\n    \\"suricata.flow\\": {\\n      \\"count\\": 41,\\n      \\"percentage\\": 97.6\\n    }\\n  }\\n}\\n```\\n\\nIt is now under the `catalog` section and shows some additional information:\\n\\n```json {0} title=\\"VAST v3.0 \u276f vast status | jq .catalog\\"\\n{\\n  \\"num-events\\": 42,\\n  \\"num-partitions\\": 3,\\n  \\"schemas\\": {\\n    \\"suricata.alert\\": {\\n      \\"import-time\\": {\\n        \\"max\\": \\"2023-01-13T22:51:23.730183\\",\\n        \\"min\\": \\"2023-01-13T22:51:23.730183\\"\\n      },\\n      \\"num-events\\": 1,\\n      \\"num-partitions\\": 1\\n    },\\n    \\"suricata.flow\\": {\\n      \\"import-time\\": {\\n        \\"max\\": \\"2023-01-13T22:51:24.127312\\",\\n        \\"min\\": \\"2023-01-13T23:13:01.991323\\"\\n      },\\n      \\"num-events\\": 41,\\n      \\"num-partitions\\": 2\\n    }\\n  }\\n}\\n```\\n\\n## Display Schema of Stored Events\\n\\nThe `vast show schemas` command makes it easy to see the structure of events in\\nthe database at a glance.\\n\\n```yaml {0} title=\\"vast show schemas --yaml suricata.flow\\"\\n- suricata.flow:\\n    record:\\n      - timestamp:\\n          timestamp: time\\n      - flow_id:\\n          type: uint64\\n          attributes:\\n            index: hash\\n      - pcap_cnt: uint64\\n      - vlan:\\n          list: uint64\\n      - in_iface: string\\n      - src_ip: ip\\n      - src_port:\\n          port: uint64\\n      - dest_ip: ip\\n      - dest_port:\\n          port: uint64\\n      - proto: string\\n      - event_type: string\\n      - community_id:\\n          type: string\\n          attributes:\\n            index: hash\\n      - flow:\\n          suricata.component.flow:\\n            record:\\n              - pkts_toserver: uint64\\n              - pkts_toclient: uint64\\n              - bytes_toserver: uint64\\n              - bytes_toclient: uint64\\n              - start: time\\n              - end: time\\n              - age: uint64\\n              - state: string\\n              - reason: string\\n              - alerted: bool\\n      - app_proto: string\\n```\\n\\n:::tip Filter Schemas\\nThe `vast show schemas` command supports filtering not just by the exact name of\\na schema, but also by the module name. E.g., `vast show schemas zeek` will print\\na list of all schemas in the Zeek module that the VAST server holds data for.\\n:::\\n\\n## Common Event Format (CEF) Parser\\n\\nThis release includes a new reader plugin for the [Common Event Format\\n(CEF)][cef], a text-based event format that originally stems from ArcSight. This\\nline-based format consists of up to 8 pipe-separated fields, with the last field\\nbeing an optional list of key-value pairs:\\n\\n[cef]: https://www.microfocus.com/documentation/arcsight/arcsight-smartconnectors/pdfdoc/common-event-format-v25/common-event-format-v25.pdf\\n\\n```\\nCEF:Version|Device Vendor|Device Product|Device Version|Device Event Class ID|Name|Severity|[Extension]\\n```\\n\\nHere\'s a real-world instance.\\n\\n```\\nCEF:0|Cynet|Cynet 360|4.5.4.22139|0|Memory Pattern - Cobalt Strike Beacon ReflectiveLoader|8| externalId=6 clientId=2251997 scanGroupId=3 scanGroupName=Manually Installed Agents sev=High duser=tikasrv01\\\\\\\\administrator cat=END-POINT Alert dhost=TikaSrv01 src=172.31.5.93 filePath=c:\\\\\\\\windows\\\\\\\\temp\\\\\\\\javac.exe fname=javac.exe rt=3/30/2022 10:55:34 AM fileHash=2BD1650A7AC9A92FD227B2AB8782696F744DD177D94E8983A19491BF6C1389FD rtUtc=Mar 30 2022 10:55:34.688 dtUtc=Mar 30 2022 10:55:32.458 hostLS=2022-03-30 10:55:34 GMT+00:00 osVer=Windows Server 2016 Datacenter x64 1607 epsVer=4.5.5.6845 confVer=637842168250000000 prUser=tikasrv01\\\\\\\\administrator pParams=\\"C:\\\\\\\\Windows\\\\\\\\Temp\\\\\\\\javac.exe\\" sign=Not signed pct=2022-03-30 10:55:27.140, 2022-03-30 10:52:40.222, 2022-03-30 10:52:39.609 pFileHash=1F955612E7DB9BB037751A89DAE78DFAF03D7C1BCC62DF2EF019F6CFE6D1BBA7 pprUser=tikasrv01\\\\\\\\administrator ppParams=C:\\\\\\\\Windows\\\\\\\\Explorer.EXE pssdeep=49152:2nxldYuopV6ZhcUYehydN7A0Fnvf2+ecNyO8w0w8A7/eFwIAD8j3:Gxj/7hUgsww8a0OD8j3 pSign=Signed and has certificate info gpFileHash=CFC6A18FC8FE7447ECD491345A32F0F10208F114B70A0E9D1CD72F6070D5B36F gpprUser=tikasrv01\\\\\\\\administrator gpParams=C:\\\\\\\\Windows\\\\\\\\system32\\\\\\\\userinit.exe gpssdeep=384:YtOYTIcNkWE9GHAoGLcVB5QGaRW5SmgydKz3fvnJYunOTBbsMoMH3nxENoWlymW:YLTVNkzGgoG+5BSmUfvJMdsq3xYu gpSign=Signed actRem=Kill, Rename\\n```\\n\\nVAST\'s CEF plugin supports parsing such lines using the `cef` format:\\n\\n```\\nvast import cef < cef.log\\n```\\n\\nVAST translates the `extension` field to a nested record, where the key-value\\npairs of the extensions map to record fields. Here is an example of the above\\nevent:\\n\\n```json {0} title=\\"vast export json \'172.31.5.93\' | jq\\"\\n{\\n  \\"cef_version\\": 0,\\n  \\"device_vendor\\": \\"Cynet\\",\\n  \\"device_product\\": \\"Cynet 360\\",\\n  \\"device_version\\": \\"4.5.4.22139\\",\\n  \\"signature_id\\": \\"0\\",\\n  \\"name\\": \\"Memory Pattern - Cobalt Strike Beacon ReflectiveLoader\\",\\n  \\"severity\\": \\"8\\",\\n  \\"extension\\": {\\n    \\"externalId\\": 6,\\n    \\"clientId\\": 2251997,\\n    \\"scanGroupId\\": 3,\\n    \\"scanGroupName\\": \\"Manually Installed Agents\\",\\n    \\"sev\\": \\"High\\",\\n    \\"duser\\": \\"tikasrv01\\\\\\\\administrator\\",\\n    \\"cat\\": \\"END-POINT Alert\\",\\n    \\"dhost\\": \\"TikaSrv01\\",\\n    \\"src\\": \\"172.31.5.93\\",\\n    \\"filePath\\": \\"c:\\\\\\\\windows\\\\\\\\temp\\\\\\\\javac.exe\\",\\n    \\"fname\\": \\"javac.exe\\",\\n    \\"rt\\": \\"3/30/2022 10:55:34 AM\\",\\n    \\"fileHash\\": \\"2BD1650A7AC9A92FD227B2AB8782696F744DD177D94E8983A19491BF6C1389FD\\",\\n    \\"rtUtc\\": \\"Mar 30 2022 10:55:34.688\\",\\n    \\"dtUtc\\": \\"Mar 30 2022 10:55:32.458\\",\\n    \\"hostLS\\": \\"2022-03-30 10:55:34 GMT+00:00\\",\\n    \\"osVer\\": \\"Windows Server 2016 Datacenter x64 1607\\",\\n    \\"epsVer\\": \\"4.5.5.6845\\",\\n    \\"confVer\\": 637842168250000000,\\n    \\"prUser\\": \\"tikasrv01\\\\\\\\administrator\\",\\n    \\"pParams\\": \\"C:\\\\\\\\Windows\\\\\\\\Temp\\\\\\\\javac.exe\\",\\n    \\"sign\\": \\"Not signed\\",\\n    \\"pct\\": \\"2022-03-30 10:55:27.140, 2022-03-30 10:52:40.222, 2022-03-30 10:52:39.609\\",\\n    \\"pFileHash\\": \\"1F955612E7DB9BB037751A89DAE78DFAF03D7C1BCC62DF2EF019F6CFE6D1BBA7\\",\\n    \\"pprUser\\": \\"tikasrv01\\\\\\\\administrator\\",\\n    \\"ppParams\\": \\"C:\\\\\\\\Windows\\\\\\\\Explorer.EXE\\",\\n    \\"pssdeep\\": \\"49152:2nxldYuopV6ZhcUYehydN7A0Fnvf2+ecNyO8w0w8A7/eFwIAD8j3:Gxj/7hUgsww8a0OD8j3\\",\\n    \\"pSign\\": \\"Signed and has certificate info\\",\\n    \\"gpFileHash\\": \\"CFC6A18FC8FE7447ECD491345A32F0F10208F114B70A0E9D1CD72F6070D5B36F\\",\\n    \\"gpprUser\\": \\"tikasrv01\\\\\\\\administrator\\",\\n    \\"gpParams\\": \\"C:\\\\\\\\Windows\\\\\\\\system32\\\\\\\\userinit.exe\\",\\n    \\"gpssdeep\\": \\"384:YtOYTIcNkWE9GHAoGLcVB5QGaRW5SmgydKz3fvnJYunOTBbsMoMH3nxENoWlymW:YLTVNkzGgoG+5BSmUfvJMdsq3xYu\\",\\n    \\"gpSign\\": \\"Signed\\",\\n    \\"actRem\\": \\"Kill, Rename\\"\\n  }\\n}\\n```\\n\\n:::note Syslog Header\\nSometimes CEF is prefixed with a syslog header. VAST currently only supports the\\n\\"raw\\" form without the syslog header. We are working on support for composable\\n*generic* formats, e.g., syslog, where the message can basically be any other\\nexisting format.\\n:::\\n\\n## Tidbits\\n\\nThis VAST release contains a fair amount of other changes and interesting\\nimprovements. As always, the [changelog][changelog] contains a complete list of\\nuser-facing changes since the last release.\\n\\nHere are some entries that we want to highlight:\\n\\n[changelog]: https://vast.io/changelog#v303\\n\\n### Removing Empty Fields from JSON Output\\n\\nThe `vast export json` command gained new options in addition to the already\\nexisting `--omit-nulls`: Pass `--omit-empty-records`, `--omit-empty-lists`,\\nor `--omit-empty-maps` to cause VAST not to display empty records, lists, or\\nmaps respectively.\\n\\nThe flag `--omit-empty` empty combines the three new options and `--omit-nulls`,\\nessentially causing VAST not to render empty values at all. To set these options\\nglobally, add the following to your vast.yaml configuration file:\\n\\n```yaml\\nvast:\\n  export:\\n    json:\\n      # Always omit empty records and lists when using the JSON export format,\\n      # but keep empty lists and maps.\\n      omit-nulls: true\\n      omit-empty-records: true\\n      omit-empty-maps: false\\n      omit-empty-lists: false\\n```\\n\\n### Faster Shutdown\\n\\nVAST processes now shut down faster, which especially improves the performance\\nof the `vast import` and `vast export` commands for small amounts of data\\ningested or quickly finishing queries.\\n\\nTo quantify this, we\'ve created a database with nearly 300M Zeek events, and ran\\nan export of a single event with both VAST v2.4.1 and VAST v3.0 repeatedly.\\n\\n```text {0} title=\\"\u276f vast -qq count --estimate | numfmt --grouping\\"\\n299,759,532\\n```\\n\\n```text {0} title=\\"VAST v2.4.1 \u276f hyperfine --warmup=5 --min-runs=20 \'vast -qq --bare-mode export -n1 null\'\\"\\nBenchmark 1: vast -qq --bare-mode export -n1 null\\n  Time (mean \xb1 \u03c3):     975.5 ms \xb1   4.8 ms    [User: 111.2 ms, System: 51.9 ms]\\n  Range (min \u2026 max):   966.3 ms \u2026 985.3 ms    20 runs\\n```\\n\\n```text {0} title=\\"VAST v3.0 \u276f hyperfine --warmup=5 --min-runs=20 \'vast -qq export -n1 null\'\\"\\nBenchmark 1: vast -qq --bare-mode export -n1 null\\n  Time (mean \xb1 \u03c3):     210.8 ms \xb1   3.5 ms    [User: 99.8 ms, System: 42.5 ms]\\n  Range (min \u2026 max):   204.1 ms \u2026 217.1 ms    20 runs\\n```\\n\\n### Connection Stability\\n\\nVAST clients may now be started before the VAST server: Client processes now\\nattempt to connect to server processes repeatedly until the configured\\nconnection timeout expires.\\n\\nWe found this to generally improve reliability of services with multiple VAST\\nclients, for which we often encountered problems with VAST clients being unable\\nto connect to a VAST server when started before or immediately after the VAST\\nserver.\\n\\nAdditionally, we\'ve fixed a bug that caused VAST to crash when thousands of\\nclients attempted to connect at around the same time.\\n\\n### Slim Docker Image\\n\\nThe new `tenzir/vast-slim` Docker image is an alternative to the existing\\n`tenzir/vast` Docker image that comes in at just under 40 MB in size\u2014less than a\\nthird than the regular image, making it even quicker to get started with VAST.\\n\\n### Bundled Python Bindings\\n\\nVAST installations now include Python bindings to VAST as a site package. The\\npackage is called `vast` and also available [separately on PyPI][vast-pypi].\\n\\n[vast-pypi]: https://pypi.org/project/pyvast\\n\\n### Expression Short Forms\\n\\nExtractors can now be used where predicates are expected to test for the\\nexistance of a field or type. For example, `x` and `:T` expand to `x != null`\\nand `:T != null`, respectively. This pairs nicely with the already existing\\nshort forms for values, e.g., `\\"foo\\"` expands to `:string == \\"foo`."},{"id":"/from-slack-to-discord","metadata":{"permalink":"/blog/from-slack-to-discord","source":"@site/blog/from-slack-to-discord/index.md","title":"From Slack to Discord","description":"Moving our community chat from Slack to Discord","date":"2023-02-09T00:00:00.000Z","formattedDate":"February 9, 2023","tags":[{"label":"community","permalink":"/blog/tags/community"},{"label":"chat","permalink":"/blog/tags/chat"},{"label":"discord","permalink":"/blog/tags/discord"}],"readingTime":0.785,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"From Slack to Discord","description":"Moving our community chat from Slack to Discord","authors":"mavam","image":"/img/blog/slack-to-discord.excalidraw.svg","date":"2023-02-09T00:00:00.000Z","tags":["community","chat","discord"]},"prevItem":{"title":"VAST v3.0","permalink":"/blog/vast-v3.0"},"nextItem":{"title":"The New REST API","permalink":"/blog/the-new-rest-api"}},"content":"We are moving our community chat from Slack to Discord. Why? TL;DR: because\\nDiscord has better support for community building. VAST is not the first project\\nthat abandons Slack. [Numerous][meilisearch] [open-source][appwrite]\\n[projects][deepset] [have][sst] [done][qovery] [the][neo4j] [same][discord-oss].\\n\\n[meilisearch]: https://blog.meilisearch.com/from-slack-to-discord-our-migration/\\n[appwrite]: https://appwrite.io/\\n[deepset]: https://www.deepset.ai/blog/migration-to-discord\\n[sst]: https://sst.dev/blog/moving-to-discord.html\\n[qovery]: https://www.qovery.com/blog/feedback-from-slack-to-discord-13-months-later\\n[neo4j]: https://neo4j.com/blog/neo4j-community-is-migrating-from-slack-to-discord/\\n[discord-oss]: https://discord.com/open-source\\n\\n\x3c!--truncate--\x3e\\n\\n![Slack-to-Discord](/img/blog/slack-to-discord.excalidraw.svg)\\n\\n:::info Discord Invite Link\\nYou can join our Discord community chat via <https://discord.tenzir.com>.\\n:::\\n\\nHere are the top four reasons why we are switching:\\n\\n- **Retention**: Slack\'s free plan has only 90 days message retention. We prefer\\n  permanence of our community discussion.\\n\\n- **Moderation**: Discord has solid moderation tools that rely on role-based\\n  access, and makes it possible adhere to our [Code of\\n  Conduct](/next/contribute/code-of-conduct) upon joining.\\n\\n- **Invitation**: Unlimited invite links that do not expire.\\n\\n- **Inclusion**: Users can self-assign their preferred pronouns.\\n\\nWe hope that the majority of our Slack users understand these concerns and\\nwill join us over at Discord. See you there!"},{"id":"/the-new-rest-api","metadata":{"permalink":"/blog/the-new-rest-api","source":"@site/blog/the-new-rest-api/index.md","title":"The New REST API","description":"As of v2.4 VAST ships with a new web plugin that","date":"2023-01-26T00:00:00.000Z","formattedDate":"January 26, 2023","tags":[{"label":"frontend","permalink":"/blog/tags/frontend"},{"label":"rest","permalink":"/blog/tags/rest"},{"label":"api","permalink":"/blog/tags/api"},{"label":"architecture","permalink":"/blog/tags/architecture"}],"readingTime":6.92,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"},{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"The New REST API","authors":["lava","mavam"],"date":"2023-01-26T00:00:00.000Z","last_updated":"2023-02-08T00:00:00.000Z","image":"/img/blog/rest-api-deployment-single.excalidraw.svg","tags":["frontend","rest","api","architecture"]},"prevItem":{"title":"From Slack to Discord","permalink":"/blog/from-slack-to-discord"},"nextItem":{"title":"Parquet & Feather: Data Engineering Woes","permalink":"/blog/parquet-and-feather-data-engineering-woes"}},"content":"As of [v2.4](/blog/vast-v2.4) VAST ships with a new `web` [plugin][plugins] that\\nprovides a [REST API][rest-api]. The [API documentation](/api) describes the\\navailable endpoints also provides an\\n[OpenAPI](https://spec.openapis.org/oas/latest.html) spec for download. This\\nblog post shows how we built the API and what you can do with it.\\n\\n[rest-api]: /VAST%20v3.0/use/integrate/rest-api\\n[plugins]: /VAST%20v3.0/understand/architecture/plugins\\n[actors]: /VAST%20v3.0/understand/architecture/actor-model\\n\\n\x3c!--truncate--\x3e\\n\\nWhy does VAST need a REST API? Two reasons:\\n\\n1. **Make it easy to integrate with VAST**. To date, the only interface to VAST\\n   is the command line. This is great for testing and ad-hoc use cases, but to\\n   make it easy for other tools to integrate with VAST, a REST API is the common\\n   expectation.\\n\\n2. **Develop our own web frontend**. We are in the middle of building a\\n   [Svelte](https://svelte.dev/) frontend that delivers a web-based experience\\n   of interacting with VAST through the browser. This frontend interacts with\\n   VAST through the REST API.\\n\\nTwo architectural features of VAST made it really smooth to design the REST API:\\n[Plugins][plugins] and [Actors][actors].\\n\\nFirst, VAST\'s plugin system offers a flexible extension mechanism to add\\nadditional functionality without bloating the core. Specifically, we chose\\n[RESTinio](https://github.com/Stiffstream/restinio) as C++ library that\\nimplements an asynchronous HTTP and WebSocket server. Along with it comes a\\ndependency on Boost ASIO. We deem it acceptable to have this dependency of the\\n`web` plugin, but would feel less comfortable with adding dependencies to the\\nVAST core, which we try to keep as lean as possible.\\n\\nSecond, the [actor model architecture][actors] of VAST makes it easy to\\nintegrate new \\"microservices\\" into the system. The `web` plugin is a *component\\nplugin* that provides a new actor with a typed messaging interface. It neatly\\nfits into the existing architecture and thereby inherits the flexible\\ndistribution and scaling properties. Concretely, there exist two ways to run the\\nREST API actor: either as a separate process or embedded inside a VAST server\\nnode:\\n\\n![REST API - Single Deployment](rest-api-deployment-single.excalidraw.svg)\\n\\nRunning the REST API as dedicated process gives you more flexibility with\\nrespect to deployment, fault isolation, and scaling. An embedded setup offers\\nhigher throughput and lower latency between the REST API and the other VAST\\ncomponents.\\n\\nThe REST API is also a *command plugin* and exposes the\u2014you guessed it\u2014`web`\\ncommand. To run the REST API as dedicated process, spin up a VAST node as\\nfollows:\\n\\n```bash\\nvast web server --certfile=/path/to/server.certificate --keyfile=/path/to/private.key\\n```\\n\\nTo run the server within the main VAST process, use a `start` command:\\n\\n```bash\\nvast start --commands=\\"web server [...]\\"\\n```\\n\\nThe server will only accept TLS requests by default. To allow clients to connect\\nsuccessfully, you need to pass a valid certificate and corresponding private key\\nwith the `--certfile` and `--keyfile` arguments.\\n\\n## Authentication\\n\\nClients must authenticate all requests with a valid token. The token is a short\\nstring that clients put in the `X-VAST-Token` request header.\\n\\nYou can generate a valid token on the command line as follows:\\n\\n```bash\\nvast web generate-token\\n```\\n\\nFor local testing and development, generating suitable certificates and tokens\\ncan be a hassle. For this scenario, you can start the server in [developer\\nmode](#developer-mode) where it accepts plain HTTP connections and does not\\nperform token authentication.\\n\\n## TLS Modes\\n\\nThere exist four modes to start the REST API, each of which suits a slightly\\ndifferent use case.\\n\\n### Developer Mode\\n\\nThe developer mode bypasses encryption and authentication token verification.\\n\\n![REST API - Developer Mode](rest-api-mode-developer.excalidraw.svg)\\n\\nPass `--mode=dev` to start the REST API in developer mode:\\n\\n```bash\\nvast web server --mode=dev\\n```\\n\\n### Server Mode\\n\\nThe server mode reflects the \\"traditional\\" mode of operation where VAST binds to\\na network interface. This mode only accepts HTTPS connections and requires a\\nvalid authentication token for every request. This is the default mode of\\noperation.\\n\\n![REST API - Server Mode](rest-api-mode-server.excalidraw.svg)\\n\\nPass `--mode=server` to start the REST API in server mode:\\n\\n```bash\\nvast web server --mode=server\\n```\\n\\n### Upstream TLS Mode\\n\\nThe upstream TLS mode is suitable when VAST sits upstream of a separate\\nTLS terminator that is running on the same machine. This kind of setup\\nis commonly encountered when running nginx as a reverse proxy.\\n\\n![REST API - Developer Mode](rest-api-mode-developer.excalidraw.svg)\\n\\nVAST only listens on localhost addresses, accepts plain HTTP but still\\nchecks authentication tokens.\\n\\nPass `--mode=upstream` to start the REST API in server mode:\\n\\n```bash\\nvast web server --mode=upstream\\n```\\n\\n### Mutual TLS Mode\\n\\nThe mutual TLS mode is suitable when VAST sits upstream of a separate TLS\\nterminator that may be running on a different machine. In this scenario,\\nthe connection between the terminator and VAST must again be encrypted\\nto avoid leaking the authentication token to the network.\\n\\nRegular TLS requires only the server to present a certificate to prove his\\nidentity. In mutual TLS mode, the client additionally needs to provide a\\nvalid *client certificate* to the server. This ensures that the TLS terminator\\ncannot be impersonated or bypassed.\\n\\nTypically self-signed certificates are used for that purpose, since both ends of\\nthe connection are configured together and not exposed to the public internet.\\n\\n![REST API - mTLS Mode](rest-api-mode-mtls.excalidraw.svg)\\n\\nPass `--mode=mtls` to start the REST API in mutual TLS mode:\\n\\n```bash\\nvast web server --mode=mtls\\n```\\n\\n## Usage Examples\\n\\nNow that you know how we put the REST API together, let\'s look at some\\nend-to-end examples.\\n\\n### See what\'s inside VAST\\n\\nOne straightforward example is checking the number of records in VAST:\\n\\n```bash\\ncurl \\"https://vast.example.org:42001/api/v0/status?verbosity=detailed\\" \\\\\\n  | jq .index.statistics\\n```\\n\\n```json\\n{\\n  \\"events\\": {\\n    \\"total\\": 8462\\n  },\\n  \\"layouts\\": {\\n    \\"zeek.conn\\": {\\n      \\"count\\": 8462,\\n      \\"percentage\\": 100\\n    }\\n  }\\n}\\n```\\n\\n:::caution Status changes in v3.0\\nIn the upcoming v3.0 release, the statistics under the key `.index.statistics`\\nwill move to `.catalog`. This change is already merged into the master branch.\\nConsult the [status key reference](/VAST%20v3.0/setup/monitor#reference) for details.\\n:::\\n\\n### Perform a HTTP health check\\n\\nThe `/status` endpoint can also be used as a HTTP health check in\\n`docker-compose`:\\n\\n```yaml\\nversion: \'3.4\'\\nservices:\\n  web:\\n    image: tenzir/vast\\n    environment:\\n      - \\"VAST_START__COMMANDS=web server --mode=dev\\"\\n    ports:\\n      - \\"42001:42001\\"\\n    healthcheck:\\n      test: curl --fail http://localhost:42001/status || exit 1\\n      interval: 60s\\n      retries: 5\\n      start_period: 20s\\n      timeout: 10s\\n```\\n\\n### Run a query\\n\\nThe other initial endpoints can be used to get data out of VAST. For example, to\\nget up to two `zeek.conn` events which connect to the subnet `192.168.0.0/16`, using\\nthe VAST query expression `net.src.ip in 192.168.0.0/16`:\\n\\n```bash\\ncurl \\"http://127.0.0.1:42001/api/v0/export?limit=2&expression=net.src.ip%20in%20192.168.0.0%2f16\\"\\n```\\n\\n```json\\n{\\n  \\"version\\": \\"v2.4.0-457-gb35c25d88a\\",\\n  \\"num_events\\": 2,\\n  \\"events\\": [\\n    {\\n      \\"ts\\": \\"2009-11-18T08:00:21.486539\\",\\n      \\"uid\\": \\"Pii6cUUq1v4\\",\\n      \\"id.orig_h\\": \\"192.168.1.102\\",\\n      \\"id.orig_p\\": 68,\\n      \\"id.resp_h\\": \\"192.168.1.1\\",\\n      \\"id.resp_p\\": 67,\\n      \\"proto\\": \\"udp\\",\\n      \\"service\\": null,\\n      \\"duration\\": \\"163.82ms\\",\\n      \\"orig_bytes\\": 301,\\n      \\"resp_bytes\\": 300,\\n      \\"conn_state\\": \\"SF\\",\\n      \\"local_orig\\": null,\\n      \\"missed_bytes\\": 0,\\n      \\"history\\": \\"Dd\\",\\n      \\"orig_pkts\\": 1,\\n      \\"orig_ip_bytes\\": 329,\\n      \\"resp_pkts\\": 1,\\n      \\"resp_ip_bytes\\": 328,\\n      \\"tunnel_parents\\": []\\n    },\\n    {\\n      \\"ts\\": \\"2009-11-18T08:08:00.237253\\",\\n      \\"uid\\": \\"nkCxlvNN8pi\\",\\n      \\"id.orig_h\\": \\"192.168.1.103\\",\\n      \\"id.orig_p\\": 137,\\n      \\"id.resp_h\\": \\"192.168.1.255\\",\\n      \\"id.resp_p\\": 137,\\n      \\"proto\\": \\"udp\\",\\n      \\"service\\": \\"dns\\",\\n      \\"duration\\": \\"3.78s\\",\\n      \\"orig_bytes\\": 350,\\n      \\"resp_bytes\\": 0,\\n      \\"conn_state\\": \\"S0\\",\\n      \\"local_orig\\": null,\\n      \\"missed_bytes\\": 0,\\n      \\"history\\": \\"D\\",\\n      \\"orig_pkts\\": 7,\\n      \\"orig_ip_bytes\\": 546,\\n      \\"resp_pkts\\": 0,\\n      \\"resp_ip_bytes\\": 0,\\n      \\"tunnel_parents\\": []\\n    }\\n  ]\\n}\\n```\\n\\nNote that when using `curl`, all request parameters need to be properly\\nurlencoded. This can be cumbersome for the `expression` and `pipeline`\\nparameters, so we also provide an `/export` POST endpoint that accepts\\nparameters in the JSON body. The next example shows how to use POST requests\\nfrom curl. It also uses the `/query` endpoint instead of `/export` to get\\nresults iteratively instead of a one-shot result. The cost for this is having to\\nmake two API calls instead of one:\\n\\n```bash\\ncurl -XPOST -H\\"Content-Type: application/json\\" -d\'{\\"expression\\": \\"udp\\"}\' http://127.0.0.1:42001/api/v0/query/new\\n```\\n\\n```json\\n{\\"id\\": \\"31cd0f6c-915f-448e-b64a-b5ab7aae2474\\"}\\n```\\n\\n```bash\\ncurl http://127.0.0.1:42001/api/v0/query/31cd0f6c-915f-448e-b64a-b5ab7aae2474/next?n=2 | jq\\n```\\n\\n```json\\n{\\n  \\"position\\": 0,\\n  \\"events\\": [\\n    {\\n      \\"ts\\": \\"2009-11-18T08:00:21.486539\\",\\n      \\"uid\\": \\"Pii6cUUq1v4\\",\\n      \\"id.orig_h\\": \\"192.168.1.102\\",\\n      \\"id.orig_p\\": 68,\\n      \\"id.resp_h\\": \\"192.168.1.1\\",\\n      \\"id.resp_p\\": 67,\\n      \\"proto\\": \\"udp\\",\\n      \\"service\\": null,\\n      \\"duration\\": \\"163.82ms\\",\\n      \\"orig_bytes\\": 301,\\n      \\"resp_bytes\\": 300,\\n      \\"conn_state\\": \\"SF\\",\\n      \\"local_orig\\": null,\\n      \\"missed_bytes\\": 0,\\n      \\"history\\": \\"Dd\\",\\n      \\"orig_pkts\\": 1,\\n      \\"orig_ip_bytes\\": 329,\\n      \\"resp_pkts\\": 1,\\n      \\"resp_ip_bytes\\": 328,\\n      \\"tunnel_parents\\": []\\n    },\\n    {\\n      \\"ts\\": \\"2009-11-18T08:08:00.237253\\",\\n      \\"uid\\": \\"nkCxlvNN8pi\\",\\n      \\"id.orig_h\\": \\"192.168.1.103\\",\\n      \\"id.orig_p\\": 137,\\n      \\"id.resp_h\\": \\"192.168.1.255\\",\\n      \\"id.resp_p\\": 137,\\n      \\"proto\\": \\"udp\\",\\n      \\"service\\": \\"dns\\",\\n      \\"duration\\": \\"3.78s\\",\\n      \\"orig_bytes\\": 350,\\n      \\"resp_bytes\\": 0,\\n      \\"conn_state\\": \\"S0\\",\\n      \\"local_orig\\": null,\\n      \\"missed_bytes\\": 0,\\n      \\"history\\": \\"D\\",\\n      \\"orig_pkts\\": 7,\\n      \\"orig_ip_bytes\\": 546,\\n      \\"resp_pkts\\": 0,\\n      \\"resp_ip_bytes\\": 0,\\n      \\"tunnel_parents\\": []\\n    }\\n  ]\\n}\\n```\\n\\n:::note Still Experimental\\nPlease note that we consider the API version `v0` experimental, and we make no\\nstability guarantees at the moment.\\n:::\\n\\nAs always, if you have any question on usage, swing by our [community\\nchat](/discord). Missing routes? Let us know so that we know\\nwhat to prioritize. Now happy curling! :curling_stone:"},{"id":"/parquet-and-feather-data-engineering-woes","metadata":{"permalink":"/blog/parquet-and-feather-data-engineering-woes","source":"@site/blog/parquet-and-feather-data-engineering-woes/index.md","title":"Parquet & Feather: Data Engineering Woes","description":"Apache Arrow and [Apache","date":"2023-01-10T00:00:00.000Z","formattedDate":"January 10, 2023","tags":[{"label":"arrow","permalink":"/blog/tags/arrow"},{"label":"parquet","permalink":"/blog/tags/parquet"},{"label":"feather","permalink":"/blog/tags/feather"}],"readingTime":7.09,"hasTruncateMarker":true,"authors":[{"name":"Thomas Peiselt","title":"Data Engineer","url":"https://github.com/dispanser","email":"thomas@tenzir.com","imageURL":"https://github.com/dispanser.png","key":"dispanser"}],"frontMatter":{"title":"Parquet & Feather: Data Engineering Woes","authors":"dispanser","date":"2023-01-10T00:00:00.000Z","tags":["arrow","parquet","feather"]},"prevItem":{"title":"The New REST API","permalink":"/blog/the-new-rest-api"},"nextItem":{"title":"VAST v2.4.1","permalink":"/blog/vast-v2.4.1"}},"content":"[Apache Arrow](https://arrow.apache.org/) and [Apache\\nParquet](https://parquet.apache.org) have become the de-facto columnar formats\\nfor in-memory and on-disk representations when it comes to structured data.\\nBoth are strong together, as they provide data interoperability and foster a\\ndiverse ecosystem of data tools. But how well do they actually work together\\nfrom an engineering perspective?\\n\\n\x3c!--truncate--\x3e\\n\\nIn our previous posts, we introduced the formats and did a quantitative\\ncomparison of Parquet and Feather-on the write path. In this post, we look at\\nthe developer experience.\\n\\n:::info Parquet & Feather: 3/3\\nThis blog post is the last part of a 3-piece series on Parquet and Feather.\\n\\n1. [Enabling Open Investigations][parquet-and-feather-1]\\n2. [Writing Security Telemetry][parquet-and-feather-2]\\n3. This blog post\\n\\n[parquet-and-feather-1]: /blog/parquet-and-feather-enabling-open-investigations/\\n[parquet-and-feather-2]: /blog/parquet-and-feather-writing-security-telemetry/\\n:::\\n\\nWhile our Feather implementation proved to be straight-forward, the Parquet\\nstore implementation turned out to be more difficult. Recall that VAST has its\\nown [type system](/VAST%20v3.0/understand/data-model/type-system) relying on\\n[Arrow extension\\ntypes](https://arrow.apache.org/docs/format/Columnar.html#extension-types) to\\nexpress domain-specific concepts like IP addresses, subnets, or enumerations. We\\nhit a few places where the Arrow C++ implementation does not support all VAST\\ntypes directly. It\'s trickier than we thought, as we see next.\\n\\n## Row Groups\\n\\nIn Apache Parquet, a [row group](https://parquet.apache.org/docs/concepts/) is a\\nsubset of a Parquet file that\'s itself written in a columnar fashion. Smaller\\nrow groups allow for higher granularity in reading parts of an individual file,\\nat the expense of a potentially increased file size due to less optimal\\nencoding. In VAST, we send around [batches of data](/VAST%20v3.0/setup/tune)\\nthat are considerably smaller than what a recommended Parquet file size would\\nlook like. A typical Parquet file size recommendation is 1GB, which translates\\nto 5\u201310GB of data in memory when reading the entire file. To produce files sized\\nin this order of magnitude, we planned to use individual row groups, each of\\nwhich aligned with the size of our Arrow record batches that comprise\\n2<sup>16</sup> events occupying a few MBs.\\n\\nHowever, attempting to read a Parquet file that was split into multiple row\\ngroups doesn\'t work for some of our schemas, yielding:\\n\\n```\\nNotImplemented: Nested data conversions not implemented for chunked array outputs\\n```\\n\\nThis appears to be related to\\n[ARROW-5030](https://issues.apache.org/jira/browse/ARROW-5030). Our current\\nworkaround is to write a single row group, and split up the resulting Arrow\\nrecord batches into the desired size after reading. However, this increases\\nlatency to first result, an important metric for some interactive use cases we\\nenvision for VAST.\\n\\n## Arrow \u2192 Parquet \u2192 Arrow Roundtrip Schema Mismatch\\n\\nParquet is a separate project which precedes Arrow, and has its own data types,\\nwhich don\'t exactly align with what Arrow provides. While it\'s possible to\\ninstruct Arrow to also serialize [its own\\nschema](https://arrow.apache.org/docs/cpp/api/formats.html#_CPPv4N7parquet21ArrowWriterProperties7BuilderE)\\ninto the Parquet file metadata, this doesn\'t seem to play well in concert with\\nextension types. As a result, a record batch written to and then read from a\\nParquet file no longer adheres to the same schema!\\n\\nThis bit us in the following scenarios.\\n\\n### VAST Enumerations\\n\\nVAST comes with an enumeration type that represents a fixed mapping of strings\\nto numeric values, where the mapping is part of the type metadata. We represent\\nenums as extension types wrapping an Arrow dictionary of strings backed by\\nunsigned 8-bit integers. On read, Arrow turns these 8-bit index values into\\n32-bit values, which is not compatible with our extension type definition, so\\nthe extension type wrapping is lost. The diagram below illustrates this issue.\\n\\n![Arrow Schema Conversion](arrow-schema-conversion.excalidraw.svg)\\n\\n### Extension Types inside Maps\\n\\nBoth our address type and subnet type extensions are lost if they occur in\\nnested records. For example, a map from a VAST address to a VAST enumeration of\\nthe following Arrow type is not preserved:\\n\\n```\\nmap<extension<vast.address>, extension<vast.enumeration>>\\n```\\n\\nAfter reading it from a Parquet file, the resulting Arrow type is:\\n\\n```\\nmap<fixed_size_binary[16], string>.\\n```\\n\\nThe key, an address type, has been replaced by its physical representation,\\nwhich is 16 bytes (allowing room for an IPv6 address). Interestingly, the\\nenumeration is replaced by a string instead of a dictionary as observed in the\\nprevious paragraph. So the same type behaves differently depending on where in\\nthe schema it occurs.\\n\\nWe created an issue in the Apache JIRA to track this:\\n[ARROW-17839](https://issues.apache.org/jira/browse/ARROW-17839).\\n\\nTo fix these 3 issues, we\'re post-processing the data after reading it from\\nParquet. The workaround is a multi-step process:\\n\\n1. Side-load the Arrow schema from the Parquet metadata. This yields the actual\\n   schema, because it\'s in no way related to Parquet other than using its\\n   metadata capabilities to store it.\\n\\n1. Load the actual Arrow table. This table has its own schema, which is not the\\n   same schema as the one derived from the Parquet metadata directly.\\n\\n1. Finally, recursively walk the two schema trees with the associated data\\n   columns, and whenever there\'s a mismatch between the two, fix the data arrays\\n   by casting or transforming it, yielding a table that is aligned with the\\n   expected schema.\\n\\n   - In the first case (`dictionary` vs `vast.enumeration`) we cast the `int32`\\n     Arrow array of values into a `uint8` Arrow array, and manually create the\\n     wrapping extension type and extension array. This is relatively cheap, as\\n     casting is cheap and the wrapping is done at the array level, not the value\\n     level.\\n\\n   - In the second case (physical `binary[16]` instead of `vast.address`) we\\n     just wrap it in the appropriate extension type. Again, this is a cheap\\n     operation.\\n\\n   - The most expensive fix-up we perform is when the underlying type has been\\n     changed from an enumeration to a string: we have to create the entire array\\n     from scratch after building a lookup table that translates the string values\\n     into their corresponding numerical representation.\\n\\n## Apache Spark Support\\n\\nSo now VAST writes its data into a standardized, open format\u2014we integrate\\nseamlessly with the entire big data ecosystem, for free, right? I can read my\\nVAST database with Apache Spark and analyze security telemetry data on a\\n200-node cluster?\\n\\nNope. It\u2019s not *that* standardized. Yet. Not every tool or library supports\\nevery data type. In fact, as discussed above, writing a Parquet file and reading\\nit back *even with the same tool* doesn\'t always produce the data you started\\nwith.\\n\\nWe attempting to load a Parquet file with a single row, and a single field of\\ntype VAST\'s `count` (a 64-bit unsigned integer) into Apache Spark v3.2, we are\\ngreeted with:\\n\\n```\\norg.apache.spark.sql.AnalysisException: Illegal Parquet type: INT64 (TIMESTAMP(NANOS,false))\\n  at org.apache.spark.sql.errors.QueryCompilationErrors$.illegalParquetTypeError(QueryCompilationErrors.scala:1284)\\n```\\n\\nApache Spark v3.2 refuses to read the `import_time` field (a metadata column\\nadded by VAST itself). It turns out that Spark v3.2 has a\\n[regression](https://issues.apache.org/jira/browse/SPARK-40819). Let\'s try with\\nversion v3.1 instead, which shouldn\u2019t have this problem:\\n\\n```\\norg.apache.spark.sql.AnalysisException: Parquet type not supported: INT64 (UINT_64)\\n```\\n\\nWe got past the timestamp issue, but it still doesn\'t work: Spark only supports\\nsigned integer types, and refuses to load our Parquet file with an unsigned 64\\nbit integer value. The [related Spark JIRA\\nissue](https://issues.apache.org/jira/browse/SPARK-10113) is marked as resolved,\\nbut unfortunately the resolution is \\"a better error message.\\" However, [this\\nstack overflow post](https://stackoverflow.com/q/64383029) has the solution: if\\nwe define an explicit schema, Spark happily converts our column into a signed\\ntype.\\n\\n```scala\\nval schema = StructType(\\n  Array(\\n    StructField(\\"event\\",\\n      StructType(\\n        Array(\\n          StructField(\\"c\\", LongType))))))\\n```\\n\\nFinally, it works!\\n\\n```\\nscala> spark.read.schema(schema).parquet(<file>).show()\\n+-----+\\n|event|\\n+-----+\\n| {13}|\\n+-----+\\n```\\n\\nWe were able to read VAST data in Spark, but it\'s not an easy and out-of-the-box\\nexperience we were hoping for. It turns out that different tools don\'t always\\nsupport all the data types, and additional effort is required to integrate with\\nthe big players in the Parquet ecosystem.\\n\\n## Conclusion\\n\\nWe love Apache Arrow\u2014it\'s a cornerstone of our system, and we\'d be in much\\nworse shape without it. We use it everywhere from the storage layer (using\\nFeather and Parquet) to the data plane (where we are passing around Arrow record\\nbatches).\\n\\nHowever, as VAST uses a few less common Arrow features we sometimes stumble over\\nsome of the rougher edges. We\'re looking forward to fixing some of these things\\nupstream, but sometimes you just need a quick solution to help our users.\\n\\nThe real reason why we wrote this blog post is to show how quickly the data\\nengineering can escalate. This is the long tail that nobody wants to talk about\\nwhen telling you to build your own security data lake. And it quickly adds up!\\nIt\'s also heavy-duty data wrangling, and not ideally something you want your\\nsecurity team working on when they would be more useful hunting threats. Even\\nmore reasons to use a purpose-built security data technology like VAST."},{"id":"/vast-v2.4.1","metadata":{"permalink":"/blog/vast-v2.4.1","source":"@site/blog/vast-v2.4.1/index.md","title":"VAST v2.4.1","description":"Faster Query Taste","date":"2022-12-19T00:00:00.000Z","formattedDate":"December 19, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"feather","permalink":"/blog/tags/feather"},{"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":1.695,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.4.1","description":"Faster Query Taste","authors":"dominiklohmann","date":"2022-12-19T00:00:00.000Z","tags":["release","feather","performance"]},"prevItem":{"title":"Parquet & Feather: Data Engineering Woes","permalink":"/blog/parquet-and-feather-data-engineering-woes"},"nextItem":{"title":"VAST v2.4","permalink":"/blog/vast-v2.4"}},"content":"[VAST v2.4.1][github-vast-release] improves the performance of queries when VAST\\nis under high load, and significantly reduces the time to first result for\\nqueries with a low selectivity.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.4.1\\n\\n\x3c!--truncate--\x3e\\n\\n## Reading Feather Files Incrementally\\n\\nVAST\'s Feather store na\xefvely used the [Feather reader][feather-reader] from the\\nApache Arrow C++ library in its initial implementation. However, its API is\\nrather limited: It does not support reading record batches incrementally. We\'ve\\nswapped this out with a more efficient implementation that does.\\n\\n[feather-reader]: https://github.com/apache/arrow/blob/apache-arrow-10.0.1/cpp/src/arrow/ipc/feather.h#L57-L108\\n\\nThis is best explained visually:\\n\\n![Incremental Reads](incremental-reads.excalidraw.svg)\\n\\nWithin the scope of a single Feather store file, a single query takes the same\\namount of time overall, but there exist two distinct advantages of this\\napproach:\\n\\n1. The first result arrives much faster at the client.\\n2. Stores do less work for cancelled queries.\\n\\nOne additional benefit that is not immediately obvious comes into play when\\nqueries arrives at multiple stores in parallel: disk reads are more evenly\\nspread out now, making them less likely to overlap between stores. For\\ndeployments with slower I/O paths this can lead to a significant query\\nperformance improvement.\\n\\nTo verify and test this, we\'ve created a VAST database with 300M Zeek events\\n(33GB on disk) from a Corelight sensor. All tests were performed on a cold start\\nof VAST, i.e., we stopped and started VAST after every repetition of each test.\\n\\nWe performed three tests:\\n\\n1. Export a single event (20 times)\\n2. Export all events (20 times)\\n3. [Rebuild][rebuild-docs] the entire database (3 times)\\n\\n[rebuild-docs]: /VAST%20v3.0/setup/tune#rebuild-partitions\\n\\nThe results are astonishingly good:\\n\\n|Test|Benchmark|v2.4.0|v2.4.1|Improvement|\\n|:-:|:-:|:-:|:-:|:-:|\\n|**(1)**|Avg. store load time|55.1ms|4.2ms|13.1x|\\n||Time to first result/Total time|19.8ms|14.5ms|1.4x|\\n|**(2)**|Avg. store load time|386.5ms|7.3ms|52.9x|\\n||Time to first result|69.2ms|25.4ms|2.7x|\\n||Total time|39.38s|33.30s|1.2x|\\n|**(3)**|Avg. store load time|480.3ms|9.1ms|52.7x|\\n||Total time|210.5s|198.0s|1.1x|\\n\\nIf you\'re using the Feather store backend (the default as of v2.4.0), you will\\nsee an immediate improvement with VAST v2.4.1. There are no other changes\\nbetween the two releases.\\n\\n:::info Parquet Stores\\nVAST also offers an experimental Parquet store backend, for which we plan to\\nmake a similar improvement in a coming release.\\n:::"},{"id":"/vast-v2.4","metadata":{"permalink":"/blog/vast-v2.4","source":"@site/blog/vast-v2.4/index.md","title":"VAST v2.4","description":"Open Storage","date":"2022-12-09T00:00:00.000Z","formattedDate":"December 9, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"frontend","permalink":"/blog/tags/frontend"},{"label":"feather","permalink":"/blog/tags/feather"},{"label":"parquet","permalink":"/blog/tags/parquet"},{"label":"docker","permalink":"/blog/tags/docker"},{"label":"python","permalink":"/blog/tags/python"},{"label":"arrow","permalink":"/blog/tags/arrow"}],"readingTime":4.215,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.4","description":"Open Storage","authors":"dominiklohmann","date":"2022-12-09T00:00:00.000Z","last_updated":"2023-01-10T00:00:00.000Z","tags":["release","frontend","feather","parquet","docker","python","arrow"]},"prevItem":{"title":"VAST v2.4.1","permalink":"/blog/vast-v2.4.1"},"nextItem":{"title":"Parquet & Feather: Writing Security Telemetry","permalink":"/blog/parquet-and-feather-writing-security-telemetry"}},"content":"[VAST v2.4][github-vast-release] completes the switch to open storage formats,\\nand includes an early peek at three upcoming features for VAST: A web plugin\\nwith a REST API and an integrated frontend user interface, Docker Compose\\nconfiguration files for getting started with VAST faster and showing how to\\nintegrate VAST into your SOC, and new Python bindings that will make writing\\nintegrations easier and allow for using VAST with your data science libraries,\\nlike Pandas.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.4.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Preventing Vendor Lock-in with Open Storage\\n\\nVAST\'s Apache Feather (V2) and Apache Parquet storage backends are now\\nconsidered stable, and the default storage format is now Feather. This marks the\\nbeginning of a new era for VAST for all users: There is no more vendor lock-in\\nof your data!\\n\\nBoth as engineers and users of software we disdain vendor lock-in. Your data is\\nyours and no tool should hold it hostage. We want you to choose VAST because\\nit\'s the best engine when building a sustainable security data architecture. In\\nother words, *VAST decouples data acquisition from downstream security\\nanalytics*. To this end, we are not only committed to open source, but also to\\nopen standards\u2014for storage and processing.\\n\\nAs of this release, VAST no longer supports *writing* to its old proprietary\\nstorage format, but will still support *reading* from it until the next major\\nrelease. In the background, VAST transparently rebuilds old partitions to take\\nadvantage of the new format without any downtime. This may cause some additional\\nload when starting VAST first up after the update, but ensures that queries run\\nas fast as possible once all old partitions have been converted.\\n\\nIf you want to know more about Feather and Parquet, check out our in-depth blog\\npost series on them:\\n\\n1. [Enabling Open Investigations][parquet-and-feather-1]\\n2. [Writing Security Telemetry][parquet-and-feather-2]\\n3. [Data Engineering Woes][parquet-and-feather-3]\\n\\n[parquet-and-feather-1]: /blog/parquet-and-feather-enabling-open-investigations/\\n[parquet-and-feather-2]: /blog/parquet-and-feather-writing-security-telemetry/\\n[parquet-and-feather-3]: /blog/parquet-and-feather-data-engineering-woes/\\n\\n## What\'s Next?\\n\\nVAST v2.4 contains a few new and experimental toys to play with. Here\'s an\\noverview of what they are, and how they all make it easier to integrate VAST\\nwith other security tools.\\n\\n### Docker Compose\\n\\nA new set of [Docker Compose files][docker-compose] makes it easier than ever to\\nget started with VAST. This is not designed for high-performance deployments of\\nVAST, but rather to make it easier to try VAST out\u2014all-batteries included,\\nbecause we want to use this to showcase and test the myriad of integrations\\nin a modern SOC.\\n\\nOur vision for this is to show how VAST as a modular platform can power modern\\nand sustainable approaches to composable security.\\n\\n[docker-compose]: /VAST%20v3.0/setup/deploy/docker-compose\\n\\n### REST API and Frontend User Interface\\n\\nThe experimental `web` plugin adds a [REST API][rest-api] to VAST, and also a\\nfrontend user interface we [built in Svelte][frontend-code].\\n\\nBoth the API and the frontend are still considered unstable and subject to\\nchange without notice. We plan to stabilize and version the API in the future.\\nFundamentally, the API serves two purposes:\\n\\n1. Make it easier to write integrations with VAST\\n2. Serve as a backend for VAST\'s bundled frontend\\n\\nThe frontend UI currently displays a status page for the installed VAST node.\\n\\n\x3c!--- this weird markup is to render a border around the image ---\x3e\\n![UI showing a status page](vast-ui-experimental.jpg)\\n\\nWe have some exciting features planned for both of these. Stay tuned!\\n\\n[rest-api]: /VAST%20v3.0/use/integrate/rest-api\\n[frontend-code]: https://github.com/tenzir/vast/tree/v2.4.0/plugins/web/ui\\n\\n### Python Bindings\\n\\nWe want to make it as easy as possible to integrate VAST with other tools, so\\nwe\'re working on making that as easy as possible using VAST\'s Python bindings.\\nThe new bindings support analyzing data from VAST using industry-standard Python\\nlibraries, like Pandas.\\n\\nThis is all enabled by our commitment to open standards: VAST leverages Apache\\nArrow as its in-memory data representation. The Python bindings make it easy to\\nuse VAST\'s security-specific data types. For example, when running a query, IP\\naddresses, subnets, and patterns automatically convert to the Python-native\\ntypes, as opposed to remaining binary blobs or sheer strings.\\n\\n:::note Not yet on PyPI\\nVAST\'s new Python bindings are not yet on PyPI, as they are still heavily under\\ndevelopment. If you\'re too eager and cannot wait, go [check out the source\\ncode][python-code].\\n:::\\n\\n[python-code]: https://github.com/tenzir/vast/tree/v2.4.0/python\\n\\n## Other Noteworthy Changes\\n\\nA full list of changes to VAST since the last release is available in the\\n[changelog][changelog-2.4]. Here\'s a selection of changes that are particularly\\nnoteworthy:\\n\\n- VAST now loads all plugins by default. When asking new users for pitfalls they\\n  encountered, this ranked pretty high on the list of things we needed to\\n  change. To revert to the old behavior, set `vast.plugins: []` in your\\n  configuration file, or set `VAST_PLUGINS=` in your environment.\\n- The default endpoint changed from `localhost` to `127.0.0.1` to ensure a\\n  deterministic listening address.\\n- Exporting VAST\'s performance metrics via UDS no longer deadlocks VAST\'s\\n  metrics exporter when a listener is suspended.\\n- VAST\'s build process now natively supports building Debian packages. This\\n  makes upgrades for bare-metal deployments a breeze. As of this release, our\\n  CI/CD pipeline automatically attaches a Debian package in addition to the\\n  build archive to our releases.\\n\\n[changelog-2.4]: /changelog#v240"},{"id":"/parquet-and-feather-writing-security-telemetry","metadata":{"permalink":"/blog/parquet-and-feather-writing-security-telemetry","source":"@site/blog/parquet-and-feather-writing-security-telemetry/index.md","title":"Parquet & Feather: Writing Security Telemetry","description":"How does Apache Parquet compare to Feather for storing","date":"2022-10-24T00:00:00.000Z","formattedDate":"October 24, 2022","tags":[{"label":"benchmark","permalink":"/blog/tags/benchmark"},{"label":"arrow","permalink":"/blog/tags/arrow"},{"label":"parquet","permalink":"/blog/tags/parquet"},{"label":"feather","permalink":"/blog/tags/feather"},{"label":"quarto","permalink":"/blog/tags/quarto"},{"label":"r","permalink":"/blog/tags/r"}],"readingTime":26.56,"hasTruncateMarker":true,"authors":[{"name":"Thomas Peiselt","title":"Data Engineer","url":"https://github.com/dispanser","email":"thomas@tenzir.com","imageURL":"https://github.com/dispanser.png","key":"dispanser"},{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Parquet & Feather: Writing Security Telemetry","authors":["dispanser","mavam"],"date":"2022-10-24T00:00:00.000Z","last_updated":"2023-02-08T00:00:00.000Z","tags":["benchmark","arrow","parquet","feather","quarto","r"]},"prevItem":{"title":"VAST v2.4","permalink":"/blog/vast-v2.4"},"nextItem":{"title":"VAST v2.3.1","permalink":"/blog/vast-v2.3.1"}},"content":"How does Apache [Parquet](https://parquet.apache.org/) compare to [Feather](https://arrow.apache.org/docs/python/feather.html) for storing\\nstructured security data? In this blog post, we answer this question.\\n\\n\x3c!--truncate--\x3e\\n\\n:::info Parquet & Feather: 2/3\\nThis is blog post is part of a 3-piece series on Parquet and Feather.\\n\\n1.  [Enabling Open Investigations](/blog/parquet-and-feather-enabling-open-investigations/)\\n2.  This blog post\\n3.  [Data Engineering Woes](/blog/parquet-and-feather-data-engineering-woes/)\\n\\n:::\\n\\nIn the [previous blog](/blog/parquet-and-feather-enabling-open-investigations/), we explained why Parquet and\\nFeather are great building blocks for modern investigations. In this blog, we take\\na look at how they actually perform on the write path in two dimensions:\\n\\n- **Size**: how much space does typical security telemetry occupy?\\n- **Speed**: how fast can we write out to a store?\\n\\nParquet and Feather have different goals. While Parquet is an on-disk format\\nthat optimizes for size, Feather is a thin layer around the native Arrow\\nin-memory representation. This puts them at different points in the spectrum of\\nthroughput and latency.\\n\\nTo better understand this spectrum, we instrumented the write path of VAST,\\nwhich consists roughly of the following steps:\\n\\n1.  Parse the input\\n2.  Convert it into Arrow record batches\\n3.  Ship Arrow record batches to a VAST server\\n4.  Write Arrow record batches out into a Parquet or Feather store\\n5.  Create an index from Arrow record batches\\n\\nSince steps (1\u20133) and (5) are the same for both stores, we ignore them in the\\nfollowing analysis and solely zoom in on (4).\\n\\n## Dataset\\n\\nFor our evaluation, we use a dataset that models a \u201cnormal day in a corporate\\nnetwork\u201d fused with data from for real-world attacks. While this approach might\\nnot be ideal for detection engineering, it provides enough diversity to analyze\\nstorage and processing behavior.\\n\\nSpecifically, we rely on a 3.77 GB PCAP trace of the [M57 case study](https://www.sciencedirect.com/science/article/pii/S1742287612000370). We\\nalso injected real-world attacks from\\n[malware-traffic-analysis.net](https://www.malware-traffic-analysis.net/index.html) into the PCAP trace. To\\nmake the timestamps look somewhat realistic, we shifted the timestamps of the\\nPCAPs to pretend that the corresponding activity happens on the same day. For\\nthis we used [`editcap`](https://www.wireshark.org/docs/wsug_html_chunked/AppToolseditcap.html) and then merged the resulting PCAPs into one\\nbig file using [`mergecap`](https://www.wireshark.org/docs/wsug_html_chunked/AppToolsmergecap.html).\\n\\nWe then ran [Zeek](https://zeek.org) and [Suricata](https://suricata.io) over\\nthe trace to produce structured logs. For full reproducibility, we host this\\ncustom data set in a [Google Drive folder](https://drive.google.com/drive/folders/1mPJYVGKTk86P2JU3KD-WFz8tUkTLK095?usp=sharing).\\n\\nVAST can ingest PCAP, Zeek, and Suricata natively. All three data sources are\\nhighly valuable for detection and investigation, which is why we use them in\\nthis analysis. They represent a good mix of nested and structured data (Zeek &\\nSuricata) vs.\xa0simple-but-bulky data (PCAP). To give you a flavor, here\u2019s an\\nexample Zeek log:\\n\\n    #separator \\\\x09\\n    #set_separator  ,\\n    #empty_field    (empty)\\n    #unset_field    -\\n    #path   http\\n    #open   2022-04-20-09-56-45\\n    #fields ts  uid id.orig_h   id.orig_p   id.resp_h   id.resp_p   trans_depth method  host    uri referrer    version user_agent  origin  request_body_len    response_body_len   status_code status_msg  info_code   info_msg    tags    username    password    proxied orig_fuids  orig_filenames  orig_mime_types resp_fuids  resp_filenames  resp_mime_types\\n    #types  time    string  addr    port    addr    port    count   string  string  string  string  string  string  string  count   count   count   string  count   string  set[enum]   string  string  set[string] vector[string]  vector[string]  vector[string]  vector[string]  vector[string]  vector[string]\\n    1637155963.249475   CrkwBA3xeEV9dzj1n   128.14.134.170  57468   198.71.247.91   80  1   GET 198.71.247.91   /   -   1.1 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36     -   0   51  200 OK  -   -   (empty) -   -   -   -   -   -   FhEFqzHx1hVpkhWci   -   text/html\\n    1637157241.722674   Csf8Re1mi6gYI3JC6f  87.251.64.137   64078   198.71.247.91   80  1   -   -   -   -   1.1 -   -   0   18  400 Bad Request -   -   (empty) -   -   -   -   -   -   FpKcQG2BmJjEU9FXwh  -   text/html\\n    1637157318.182504   C1q1Lz1gxAAyf4Wrzk  139.162.242.152 57268   198.71.247.91   80  1   GET 198.71.247.91   /   -   1.1 Mozilla/5.0 (Windows NT 6.1; WOW64; rv:8.0) Gecko/20100101 Firefox/8.0  -   0   51  200 OK  -   -   (empty) -   -   -   -   -   -   FyTOLL1rVGzjXoNAb   -   text/html\\n    1637157331.507633   C9FzNf12ebDETzvDLk  172.70.135.112  37220   198.71.247.91   80  1   GET lifeisnetwork.com   /   -   1.1 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36 -   0   51  200 OK  -   -   (empty) -   -   X-FORWARDED-FOR -> 137.135.117.126  -   -   -   Fnmp6k1xVFoqqIO5Ub  -   text/html\\n    1637157331.750342   C9FzNf12ebDETzvDLk  172.70.135.112  37220   198.71.247.91   80  2   GET lifeisnetwork.com   /   -   1.1 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36 -   0   51  200 OK  -   -   (empty) -   -   X-FORWARDED-FOR -> 137.135.117.126  -   -   -   F1uLr1giTpXx81dP4   -   text/html\\n    1637157331.915255   C9FzNf12ebDETzvDLk  172.70.135.112  37220   198.71.247.91   80  3   GET lifeisnetwork.com   /wp-includes/wlwmanifest.xml    -   1.1 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36 -   0   279 404 Not Found   -   -   (empty) -   -   X-FORWARDED-FOR -> 137.135.117.126  -   -   -   F9dg5w2y748yNX9ZCc  -   text/html\\n    1637157331.987527   C9FzNf12ebDETzvDLk  172.70.135.112  37220   198.71.247.91   80  4   GET lifeisnetwork.com   /xmlrpc.php?rsd -   1.1 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36 -   0   279 404 Not Found   -   -   (empty) -   -   X-FORWARDED-FOR -> 137.135.117.126  -   -   -   FxzLxklm7xyuzTF8h   -   text/html\\n\\nHere\u2019s a snippet of a Suricata log:\\n\\n``` json\\n{\\"timestamp\\":\\"2021-11-17T14:32:43.262184+0100\\",\\"flow_id\\":1129058930499898,\\"pcap_cnt\\":7,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"128.14.134.170\\",\\"src_port\\":57468,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":0,\\"community_id\\":\\"1:YXWfTYEyYLKVv5Ge4WqijUnKTrM=\\",\\"http\\":{\\"hostname\\":\\"198.71.247.91\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51}}\\n{\\"timestamp\\":\\"2021-11-17T14:32:43.237882+0100\\",\\"flow_id\\":675134617085815,\\"event_type\\":\\"flow\\",\\"src_ip\\":\\"54.176.143.72\\",\\"dest_ip\\":\\"198.71.247.91\\",\\"proto\\":\\"ICMP\\",\\"icmp_type\\":8,\\"icmp_code\\":0,\\"response_icmp_type\\":0,\\"response_icmp_code\\":0,\\"flow\\":{\\"pkts_toserver\\":1,\\"pkts_toclient\\":1,\\"bytes_toserver\\":50,\\"bytes_toclient\\":50,\\"start\\":\\"2021-11-17T14:43:34.649079+0100\\",\\"end\\":\\"2021-11-17T14:43:34.649210+0100\\",\\"age\\":0,\\"state\\":\\"established\\",\\"reason\\":\\"timeout\\",\\"alerted\\":false},\\"community_id\\":\\"1:WHH+8OuOygRPi50vrH45p9WwgA4=\\"}\\n{\\"timestamp\\":\\"2021-11-17T14:32:48.254950+0100\\",\\"flow_id\\":1129058930499898,\\"pcap_cnt\\":10,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"128.14.134.170\\",\\"dest_port\\":57468,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"198.71.247.91\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":51,\\"tx_id\\":0}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:18.327585+0100\\",\\"flow_id\\":652708491465446,\\"pcap_cnt\\":206,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"139.162.242.152\\",\\"src_port\\":57268,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":0,\\"community_id\\":\\"1:gEyyy4v7MJSsjLvl+3D17G/rOIY=\\",\\"http\\":{\\"hostname\\":\\"198.71.247.91\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:8.0) Gecko/20100101 Firefox/8.0\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:18.329669+0100\\",\\"flow_id\\":652708491465446,\\"pcap_cnt\\":208,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"139.162.242.152\\",\\"dest_port\\":57268,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"198.71.247.91\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:8.0) Gecko/20100101 Firefox/8.0\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":51,\\"tx_id\\":0}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.569634+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":224,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"172.70.135.112\\",\\"src_port\\":37220,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":0,\\"community_id\\":\\"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.750383+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":226,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"172.70.135.112\\",\\"dest_port\\":37220,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":51,\\"tx_id\\":0}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.812254+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":228,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"172.70.135.112\\",\\"src_port\\":37220,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":1,\\"community_id\\":\\"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.915298+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":230,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"172.70.135.112\\",\\"dest_port\\":37220,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":51,\\"tx_id\\":1}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.977269+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":232,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"172.70.135.112\\",\\"src_port\\":37220,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":2,\\"community_id\\":\\"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/wp-includes/wlwmanifest.xml\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":404,\\"length\\":279}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.987556+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":234,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"172.70.135.112\\",\\"dest_port\\":37220,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/wp-includes/wlwmanifest.xml\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":404,\\"length\\":279},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/wp-includes/wlwmanifest.xml\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":279,\\"tx_id\\":2}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:32.049539+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":236,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"172.70.135.112\\",\\"src_port\\":37220,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":3,\\"community_id\\":\\"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/xmlrpc.php?rsd\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":404,\\"length\\":279}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:32.057985+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":238,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"172.70.135.112\\",\\"dest_port\\":37220,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/xmlrpc.php?rsd\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":404,\\"length\\":279},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/xmlrpc.php\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":279,\\"tx_id\\":3}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:32.119589+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":239,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"172.70.135.112\\",\\"src_port\\":37220,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":4,\\"community_id\\":\\"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:32.127935+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":241,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"172.70.135.112\\",\\"dest_port\\":37220,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":51,\\"tx_id\\":4}}\\n```\\n\\nNote that Zeek\u2019s tab-separated value (TSV) format is already a structured table,\\nwhereas Suricata data needs to be demultiplexed first through the `event_type`\\nfield.\\n\\nThe PCAP packet type is currently hard-coded in VAST\u2019s PCAP plugin and looks\\nlike this:\\n\\n``` go\\ntype pcap.packet = record {\\n  time: timestamp,\\n  src: addr,\\n  dst: addr,\\n  sport: port,\\n  dport: port,\\n  vlan: record {\\n    outer: count,\\n    inner: count,\\n  },\\n  community_id: string #index=hash,\\n  payload: string #skip,\\n}\\n```\\n\\nNow that we\u2019ve looked at the structure of the dataset, let\u2019s take a look at our\\nmeasurement methodology.\\n\\n### Measurement\\n\\nOur objective is understanding the storage and runtime characteristics of\\nParquet and Feather on the provided input data. To this end, we instrumented\\nVAST to produce us with a measurement trace file that we then analyze with R for\\ngaining insights. The [corresponding patch](feather-parquet-zstd-experiments.diff) is not meant for further\\nproduction, so we kept it separate. But we did find an opportunity to improve\\nVAST and [made the Zstd compression level configurable](https://github.com/tenzir/vast/pull/2623). Our [benchmark\\nscript](benchmark.fish) is available for full reproducibility.\\n\\nOur instrumentation produced a [CSV file](data.csv) with the following features:\\n\\n- **Store**: the type of store plugin used in the measurement, i.e., `parquet`\\n  or `feather`.\\n- **Construction time**: the time it takes to convert Arrow record batches into\\n  Parquet or Feather. We fenced the corresponding code blocks and computed the\\n  difference in nanoseconds.\\n- **Input size**: the number of bytes that the to-be-converted record batches\\n  consume.\\n- **Output size**: the number of bytes that the store file takes up.\\n- **Number of events**: the total number of events in all input record batches\\n- **Number of record batches**: the number Arrow record batches per store\\n- **Schema**: the name of the schema; there exists one store file per schema\\n- **Zstd compression level**: the applied Zstd compression level\\n\\nEvery row corresponds to a single store file where we varied some of these\\nparameters. We used [hyperfine](https://github.com/sharkdp/hyperfine) as\\nbenchmark driver tool, configured with 8 runs. Let\u2019s take a look at the data.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(dplyr)\\nlibrary(ggplot2)\\nlibrary(lubridate)\\nlibrary(scales)\\nlibrary(stringr)\\nlibrary(tidyr)\\n\\n# For faceting, to show clearer boundaries.\\ntheme_bw_trans <- function(...) {\\n  theme_bw(...) +\\n  theme(panel.background = element_rect(fill = \\"transparent\\"),\\n        plot.background = element_rect(fill = \\"transparent\\"),\\n        legend.key = element_rect(fill = \\"transparent\\"),\\n        legend.background = element_rect(fill = \\"transparent\\"))\\n}\\n\\ntheme_set(theme_minimal())\\n\\ndata <- read.csv(\\"data.csv\\") |>\\n  rename(store = store_type) |>\\n  mutate(duration = dnanoseconds(duration))\\n\\noriginal <- read.csv(\\"sizes.csv\\") |>\\n  mutate(store = \\"original\\", store_class = \\"original\\") |>\\n  select(store, store_class, schema, bytes)\\n\\n# Global view on number of events per schema.\\nschemas <- data |>\\n  # Pick one element from the run matrix.\\n  filter(store == \\"feather\\" & zstd.level == 1) |>\\n  group_by(schema) |>\\n  summarize(n = sum(num_events),\\n            bytes_memory = sum(bytes_memory))\\n\\n# Normalize store sizes by number of events/store.\\nnormalized <- data |>\\n  mutate(duration_normalized = duration / num_events,\\n         bytes_memory_normalized = bytes_memory / num_events,\\n         bytes_storage_normalized = bytes_in_storage / num_events,\\n         bytes_ratio = bytes_in_storage / bytes_memory)\\n\\n# Compute average over measurements.\\naggregated <- normalized |>\\n  group_by(store, schema, zstd.level) |>\\n  summarize(duration = mean(duration_normalized),\\n            memory = mean(bytes_memory_normalized),\\n            storage = mean(bytes_storage_normalized))\\n\\n# Treat in-memory measurements as just another storage type.\\nmemory <- aggregated |>\\n  filter(store == \\"feather\\" & zstd.level == 1) |>\\n  mutate(store = \\"memory\\", store_class = \\"memory\\") |>\\n  select(store, store_class, schema, bytes = memory)\\n\\n# Unite with rest of data.\\nunified <-\\n  aggregated |>\\n  select(-memory) |>\\n  mutate(zstd.level = factor(str_replace_na(zstd.level),\\n                             levels = c(\\"NA\\", \\"-5\\", \\"1\\", \\"9\\", \\"19\\"))) |>\\n  rename(bytes = storage, store_class = store) |>\\n  unite(\\"store\\", store_class, zstd.level, sep = \\"+\\", remove = FALSE)\\n\\nschemas_gt10k <- schemas |> filter(n > 10e3) |> pull(schema)\\nschemas_gt100k <- schemas |> filter(n > 100e3) |> pull(schema)\\n\\n# Only schemas with > 100k events.\\ncleaned <- unified |>\\n  filter(schema %in% schemas_gt100k)\\n\\n# Helper function to format numbers with SI unit suffixes.\\nfmt_si <- function(x) scales::label_number_si(accuracy = 0.1)(x)\\n```\\n\\n</details>\\n\\n### Schemas\\n\\nWe have a total of 42 unique schemas:\\n\\n     [1] \\"zeek.dce_rpc\\"       \\"zeek.dhcp\\"          \\"zeek.x509\\"         \\n     [4] \\"zeek.dpd\\"           \\"zeek.ftp\\"           \\"zeek.files\\"        \\n     [7] \\"zeek.ntlm\\"          \\"zeek.kerberos\\"      \\"zeek.ocsp\\"         \\n    [10] \\"zeek.ntp\\"           \\"zeek.dns\\"           \\"zeek.packet_filter\\"\\n    [13] \\"zeek.pe\\"            \\"zeek.radius\\"        \\"zeek.http\\"         \\n    [16] \\"zeek.reporter\\"      \\"zeek.weird\\"         \\"zeek.smb_files\\"    \\n    [19] \\"zeek.sip\\"           \\"zeek.smb_mapping\\"   \\"zeek.smtp\\"         \\n    [22] \\"zeek.conn\\"          \\"zeek.snmp\\"          \\"zeek.tunnel\\"       \\n    [25] \\"zeek.ssl\\"           \\"suricata.krb5\\"      \\"suricata.ikev2\\"    \\n    [28] \\"suricata.http\\"      \\"suricata.smb\\"       \\"suricata.ftp\\"      \\n    [31] \\"suricata.dns\\"       \\"suricata.fileinfo\\"  \\"suricata.tftp\\"     \\n    [34] \\"suricata.snmp\\"      \\"suricata.sip\\"       \\"suricata.anomaly\\"  \\n    [37] \\"suricata.smtp\\"      \\"suricata.dhcp\\"      \\"suricata.tls\\"      \\n    [40] \\"suricata.dcerpc\\"    \\"suricata.flow\\"      \\"pcap.packet\\"       \\n\\nThe schemas belong to three data *modules*: Zeek, Suricata, and PCAP. A module\\nis the prefix of a concrete type, e.g., for the schema `zeek.conn` the module is\\n`zeek` and the type is `conn`. This is only a distinction in terminology,\\ninternally VAST stores the full-qualified type as schema name.\\n\\nHow many events do we have per schema?\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nschemas <- normalized |>\\n  # Pick one element from the run matrix.\\n  filter(store == \\"feather\\" & zstd.level == 1) |>\\n  group_by(schema) |>\\n  summarize(n = sum(num_events),\\n            bytes_memory = sum(bytes_memory))\\n\\nschemas |>\\n  separate(schema, c(\\"module\\", \\"type\\"), remove = FALSE) |>\\n  ggplot(aes(x = reorder(schema, -n), y = n, fill = module)) +\\n    geom_bar(stat = \\"identity\\") +\\n    scale_y_log10(labels = scales::label_comma()) +\\n    labs(x = \\"Schema\\", y = \\"Number of Events\\", fill = \\"Module\\") +\\n    theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))\\n```\\n\\n</details>\\n\\nimport Svg1 from \'./index_files/figure-gfm/number-of-events-by-schema-1.svg\';\\n\\n<Svg1 />\\n\\nThe above plot (log-scaled y-axis) shows how many events we have per type.\\nBetween 1 and 100M events, we almost see everything.\\n\\nWhat\u2019s the typical event size?\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nschemas |>\\n  separate(schema, c(\\"module\\", \\"type\\"), remove = FALSE) |>\\n  ggplot(aes(x = reorder(schema, -n), y = bytes_memory / n, fill = module)) +\\n    geom_bar(stat = \\"identity\\") +\\n    guides(fill = \\"none\\") +\\n    scale_y_continuous(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    labs(x = \\"Schema\\", y = \\"Bytes (in-memory)\\", color = \\"Module\\") +\\n    theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))\\n```\\n\\n</details>\\n\\nimport Svg2 from \'./index_files/figure-gfm/event-size-by-schema-1.svg\';\\n\\n<Svg2 />\\n\\nThe above plot keeps the x-axis from the previous plot, but exchanges the y-axis\\nto show normalized event size, in memory after parsing. Most events\\ntake up a few 100 bytes, with packet data consuming a bit more, and one 5x\\noutlier: `suricata.ftp`.\\n\\nSuch distributions are normal, even with these outliers. Some telemetry events\\nsimply have more string data that\u2019s a function of user input. For `suricata.ftp`\\nspecifically, it can grow linearly with the data transmitted. Here\u2019s a stripped\\ndown example of an event that is greater than 5 kB in its raw JSON:\\n\\n``` json\\n{\\n  \\"timestamp\\": \\"2021-11-19T05:08:50.885981+0100\\",\\n  \\"flow_id\\": 1339403323589433,\\n  \\"pcap_cnt\\": 5428538,\\n  \\"event_type\\": \\"ftp\\",\\n  \\"src_ip\\": \\"10.5.5.101\\",\\n  \\"src_port\\": 50479,\\n  \\"dest_ip\\": \\"62.24.128.228\\",\\n  \\"dest_port\\": 110,\\n  \\"proto\\": \\"TCP\\",\\n  \\"tx_id\\": 12,\\n  \\"community_id\\": \\"1:kUFeGEpYT1JO1VCwF8wZWUWn0J0=\\",\\n  \\"ftp\\": {\\n    \\"completion_code\\": [\\n      \\"155\\",\\n      ...\\n      <stripped 330 lines>\\n      ...\\n      \\"188\\",\\n      \\"188\\",\\n      \\"188\\"\\n    ],\\n    \\"reply\\": [\\n      \\" 41609\\",\\n      ...\\n      <stripped 330 lines>\\n      ...\\n      \\" 125448\\",\\n      \\" 126158\\",\\n      \\" 29639\\"\\n    ],\\n    \\"reply_received\\": \\"yes\\"\\n  }\\n}\\n```\\n\\nThis matches our mental model. A few hundred bytes per event with some outliers.\\n\\n### Batching\\n\\nOn the inside, a store is a concatenation of homogeneous Arrow record batches,\\nall having the same schema.\\n\\nThe Feather format is essentially the IPC wire format of record batches. Schemas\\nand dictionaries are only included when they change. For our stores, this means\\njust once in the beginning. In order to access a given row in a Feather file,\\nyou need to start at the beginning, iterate batch by batch until you arrive at\\nthe desired batch, and then materialize it before you can access the desired\\nrow via random access.\\n\\nParquet has *row groups* that are much like a record batch, except that they are\\ncreated at write time, so Parquet determines their size rather than the incoming\\ndata. Parquet offers random access over both the row groups and within an\\nindividual batch that is materialized from a row group. The on-disk layout of\\nParquet is still row-group by row-group, and in that column by column, so\\nthere\u2019s no big difference between Parquet and Feather in that regard. Parquet\\nencodes columns using different encoding techniques than Arrow\u2019s IPC format.\\n\\nMost stores only consist of a few record batches. PCAP is the only difference.\\nSmall stores are suboptimal because the catalog keeps in-memory state that is a\\nlinear function of the number of stores. (We are aware of this concern and are\\nexploring improvements, but this topic is out of scope for this post.) The issue\\nhere is catalog fragmentation.\\n\\nAs of [v2.3](/blog/vast-v2.3), VAST has automatic rebuilding in place, which\\nmerges underfull partitions to reduce pressure on the catalog. This doesn\u2019t fix\\nthe problem of linear state, but gives us much sufficient reach for real-world\\ndeployments.\\n\\n## Size\\n\\nTo better understand the difference between Parquet and Feather, we now take a\\nlook at them right next to each other. In addition to Feather and Parquet, we\\nuse two other types of \u201cstores\u201d for the analysis to facilitate comparison:\\n\\n1.  **Original**: the size of the input prior it entered VAST, e.g., the raw JSON or\\n    a PCAP file.\\n\\n2.  **Memory**: the size of the data in memory, measured as the sum of Arrow\\n    buffers that make up the table slice.\\n\\nLet\u2019s kick of the analysis by getting a better understanding at the size\\ndistribution.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nunified |>\\n  bind_rows(original, memory) |>\\n  ggplot(aes(x = reorder(store, -bytes, FUN = \\"median\\"),\\n             y = bytes, color = store_class)) +\\n  geom_boxplot() +\\n  scale_y_log10(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n  labs(x = \\"Store\\", y = \\"Bytes/Event\\", color = \\"Store\\") +\\n  theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))\\n```\\n\\n</details>\\n\\nimport Svg3 from \'./index_files/figure-gfm/plot-schema-distribution-boxplot-1.svg\';\\n\\n<Svg3 />\\n\\nEvery boxplot corresponds to one store, with `original` and `memory` being also\\ntreated like stores. The suffix `-Z` indicates Zstd level `Z`, with `NA` meaning\\n\u201ccompression turned off\u201d entirely. Parquet stores on the right (in purple) have\\nthe smallest size, followed by Feather (red), and then their corresponding\\nin-memory (green) and original (turquoise) representation. The negative Zstd\\nlevel -5 makes Parquet actually worse than Feather.\\n\\n:::tip Analysis\\nWhat stands out is that disabling compression for Feather inflates the data\\nlarger than the original. This is not the case for Parquet. Why? Because Parquet\\nhas an orthogonal layer of compression using dictionaries. This absorbs\\ninefficiencies in heavy-tailed distributions, which are pretty standard in\\nmachine-generated data.\\n:::\\n\\nThe y-axis of above plot is log-scaled, which makes it hard for relative\\ncomparison. Let\u2019s focus on the medians (the bars in the box) only and bring the\\ny-axis to a linear scale:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nmedians <- unified |>\\n  bind_rows(original, memory) |>\\n  group_by(store, store_class) |>\\n  summarize(bytes = median(bytes))\\n\\nmedians |>\\n  ggplot(aes(x = reorder(store, -bytes), y = bytes, fill = store_class)) +\\n  geom_bar(stat = \\"identity\\") +\\n  scale_y_continuous(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n  labs(x = \\"Store\\", y = \\"Bytes/Event\\", fill = \\"Store\\") +\\n  theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))\\n```\\n\\n</details>\\n\\nimport Svg4 from \'./index_files/figure-gfm/plot-schema-distribution-medians-1.svg\';\\n\\n<Svg4 />\\n\\nTo better understand the compression in numbers, we\u2019ll anchor the original size\\nat 100% and now show the *relative* gains of Parquet and Feather:\\n\\n| Store      | Class   | Bytes/Event | Size (%) | Compression Ratio |\\n|:-----------|:--------|------------:|---------:|------------------:|\\n| parquet+19 | parquet |        53.5 |     22.7 |               4.4 |\\n| parquet+9  | parquet |        54.4 |     23.1 |               4.3 |\\n| parquet+1  | parquet |        55.8 |     23.7 |               4.2 |\\n| feather+19 | feather |        57.8 |     24.6 |               4.1 |\\n| feather+9  | feather |        66.9 |     28.4 |               3.5 |\\n| feather+1  | feather |        68.9 |     29.3 |               3.4 |\\n| parquet+-5 | parquet |        72.9 |     31.0 |               3.2 |\\n| parquet+NA | parquet |        90.8 |     38.6 |               2.6 |\\n| feather+-5 | feather |        95.8 |     40.7 |               2.5 |\\n| feather+NA | feather |       255.1 |    108.3 |               0.9 |\\n\\n:::tip Analysis\\nParquet dominates Feather with respect to space savings, but not by much for\\nhigh Zstd levels. Zstd levels \\\\> 1 do not provide substantial space savings on\\naverage, where observe a compression ratio of **\\\\~4x** over the base data. Parquet\\nstill provides a **2.6** compression ratio in the absence of compression because\\nit applies dictionary encoding.\\n\\nFeather offers competitive compression with **\\\\~3x** ratio for equal Zstd levels.\\nHowever, without compression Feather expands beyond the original dataset size at\\na compression ratio of **\\\\~0.9**.\\n:::\\n\\nThe above analysis covered averages across schemas. If we juxtapose Parquet and\\nFeather per schema, we see the difference between the two formats more clearly:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(ggrepel)\\n\\nparquet_vs_feather_size <- unified |>\\n  select(-store, -duration) |>\\n  pivot_wider(names_from = store_class,\\n              values_from = bytes,\\n              id_cols = c(schema, zstd.level))\\n\\nplot_parquet_vs_feather <- function(data) {\\n  data |>\\n    mutate(zstd.level = str_replace_na(zstd.level)) |>\\n    separate(schema, c(\\"module\\", \\"type\\"), remove = FALSE) |>\\n    ggplot(aes(x = parquet, y = feather,\\n               shape = zstd.level, color = zstd.level)) +\\n      geom_abline(intercept = 0, slope = 1, color = \\"grey\\") +\\n      geom_point(alpha = 0.6, size = 3) +\\n      geom_text_repel(aes(label = schema),\\n                color = \\"grey\\",\\n                size = 1, # font size\\n                box.padding = 0.2,\\n                min.segment.length = 0, # draw all line segments\\n                max.overlaps = Inf,\\n                segment.size = 0.2,\\n                segment.color = \\"grey\\",\\n                segment.alpha = 0.3) +\\n      scale_size(range = c(0, 10)) +\\n      labs(x = \\"Bytes (Parquet)\\", y = \\"Bytes (Feather)\\",\\n           shape = \\"Zstd Level\\", color = \\"Zstd Level\\")\\n}\\n\\nparquet_vs_feather_size |>\\n  filter(schema %in% schemas_gt100k) |>\\n  plot_parquet_vs_feather() +\\n    scale_x_log10(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_log10(labels = scales::label_bytes(units = \\"auto_si\\"))\\n```\\n\\n</details>\\n\\nimport Svg5 from \'./index_files/figure-gfm/plot-parquet-vs-feather-1.svg\';\\n\\n<Svg5 />\\n\\nIn the above log-log scatterplot, the straight line is the identity function.\\nEach point represents the median store size for a given schema. If a point is on\\nthe line, it means there is no difference between Feather and Parquet. We only\\nlook at schemas with more than 100k events to ensure that the constant factor\\ndoes not perturb the analysis. (Otherwise we end up with points *below* the\\nidentity line, which are completely dwarfed by the bulk in practice.) The color\\nand shape shows the different Zstd levels, with `NA` meaning no compression.\\nPoints clouds closer to the origin mean that the corresponding store class takes\\nup less space.\\n\\n:::tip Analysis\\nWe observe that **disabling compression hits Feather the hardest**.\\nUnexpectedly, a negative Zstd level of -5 does not compress well. The remaining\\nZstd levels are difficult to take apart visually, but it appears that the point\\nclouds form a parallel line, indicating stable compression gains. Notably,\\n**compressing PCAP packets is nearly identical with Feather and Parquet**,\\npresumably because of the low entropy and packet meta data where general-purpose\\ncompressors like Zstd shine.\\n:::\\n\\nZooming in to the bottom left area with average event size of less than 100B,\\nand removing the log scaling, we see the following:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nparquet_vs_feather_size |>\\n  filter(feather <= 100 & schema %in% schemas_gt100k) |>\\n  plot_parquet_vs_feather() +\\n    scale_x_continuous(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_continuous(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    coord_fixed()\\n```\\n\\n</details>\\n\\nimport Svg6 from \'./index_files/figure-gfm/plot-parquet-vs-feather-100-1.svg\';\\n\\n<Svg6 />\\n\\nThe respective point clouds form a parallel to the identity function, i.e., the\\ncompression ratio in this region pretty constant across schemas. There\u2019s also no\\nnoticeable difference between Zstd level 1, 9, and 19.\\n\\nIf we take pick a single point, e.g., `zeek.conn` with\\n4.7M events, we\\ncan confirm that the relative performance matches the results of our analysis\\nabove:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nunified |>\\n  filter(schema == \\"zeek.conn\\") |>\\n  ggplot(aes(x = reorder(store, -bytes), y = bytes, fill = store_class)) +\\n    geom_bar(stat = \\"identity\\") +\\n    guides(fill = \\"none\\") +\\n    labs(x = \\"Store\\", y = \\"Bytes/Event\\") +\\n    scale_y_continuous(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0)) +\\n    facet_wrap(~ schema, scales = \\"free\\")\\n```\\n\\n</details>\\n\\nimport Svg7 from \'./index_files/figure-gfm/plot-zeek-suricata-1.svg\';\\n\\n<Svg7 />\\n\\nFinally, we look at the fraction of space Parquet takes compared to Feather on a\\nper schema basis, restricted to schemas with more than 10k events:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(tibble)\\n\\nparquet_vs_feather_size |>\\n  filter(feather <= 100 & schema %in% schemas_gt10k) |>\\n  mutate(zstd.level = str_replace_na(zstd.level)) |>\\n  ggplot(aes(x = reorder(schema, -parquet / feather),\\n             y = parquet / feather,\\n             fill = zstd.level)) +\\n    geom_hline(yintercept = 1) +\\n    geom_bar(stat = \\"identity\\", position = \\"dodge\\") +\\n    labs(x = \\"Schema\\", y = \\"Parquet / Feather (%)\\", fill = \\"Zstd Level\\") +\\n    scale_y_continuous(breaks = 6:1 * 20 / 100, labels = scales::label_percent()) +\\n    theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))\\n```\\n\\n</details>\\n\\nimport Svg8 from \'./index_files/figure-gfm/plot-parquet-divided-by-feather-1.svg\';\\n\\n<Svg8 />\\n\\nThe horizontal line is similar to the identity line in the scatterplot,\\nindicating that Feather and Parquet compress equally well. The bars represent\\nthat ratio of Parquet divided by Feather. The shorter the bars, the smaller the\\nsize, so the higher the gain over Feather.\\n\\n:::tip Analysis\\nWe see that Zstd level 19 brings Parquet and Feather close together. Even at\\nZstd level 1, the median ratio of Parquet stores is **78%**, and the 3rd\\nquartile **82%**. This shows that **Feather is remarkably competitive for typical\\nsecurity analytics workloads**.\\n:::\\n\\n## Speed\\n\\nNow that we have looked at the spatial properties of Parquet and Feather, we\\ntake a look at the runtime. With *speed*, we mean the time it takes to transform\\nArrow Record Batches into Parquet and Feather format. This analysis only\\nconsiders only CPU time; VAST writes the respective store in memory first and\\nthen flushes it one sequential write. Our mental model is that Feather is faster\\nthan Parquet. Is that the case when enabling compression for both?\\n\\nTo avoid distortion of small events, we also restrict the analysis to schemas\\nwith more than 100k events.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nunified |>\\n  filter(schema %in% schemas_gt100k) |>\\n  ggplot(aes(x = reorder(store, -duration, FUN = \\"median\\"),\\n             y = duration, color = store_class)) +\\n  geom_boxplot() +\\n  scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n  theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0)) +\\n  labs(x = \\"Store\\", y = \\"Speed (us)\\", color = \\"Store\\")\\n```\\n\\n</details>\\n\\nimport Svg9 from \'./index_files/figure-gfm/duration-distribution-1.svg\';\\n\\n<Svg9 />\\n\\nThe above boxplots show the time it takes to write a store for a given store and\\ncompression level combination. The log-scaled y-axis shows the normalized to number\\nof microseconds per event, across the distribution of all schemas. The sort order\\nis the median processing time, similar to the size discussion above.\\n\\n:::tip Analysis\\nAs expected, we roughly observe an ordering according to Zstd level: more\\ncompression means a longer runtime.\\n\\nUnexpectedly, for the same Zstd level, **Parquet store creation was always\\nfaster**. Our unconfirmed hunch is that Feather compression operates on more and\\nsmaller column buffers, whereas Parquet compression only runs over the\\nconcatenated Arrow buffers, yielding bigger strides.\\n:::\\n\\nWe don\u2019t have an explanation for why disabling compression for Parquet is\\n*slower* compared Zstd levels -5 and 1. In theory, strictly less cycles are\\nspent by disabling the compression code path. Perhaps compression results in\\ndifferent memory layout that is more cache-efficient. Unfortunately, we did not\\nhave the time to dig deeper into the analysis to figure out why disabling\\nParquet compression is slower. Please don\u2019t hesitate to reach out, e.g., via our\\n[community chat](/discord).\\n\\nLet\u2019s compare Parquet and Feather by compression level, per schema:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nparquet_vs_feather_duration <- unified |>\\n  filter(schema %in% schemas_gt100k) |>\\n  select(-store, -bytes) |>\\n  pivot_wider(names_from = store_class,\\n              values_from = duration,\\n              id_cols = c(schema, zstd.level))\\n\\nparquet_vs_feather_duration |>\\n  mutate(zstd.level = str_replace_na(zstd.level)) |>\\n  separate(schema, c(\\"module\\", \\"type\\"), remove = FALSE) |>\\n  ggplot(aes(x = parquet, y = feather,\\n             shape = zstd.level, color = zstd.level)) +\\n    geom_abline(intercept = 0, slope = 1, color = \\"grey\\") +\\n    geom_point(alpha = 0.7, size = 3) +\\n    geom_text_repel(aes(label = schema),\\n              color = \\"grey\\",\\n              size = 1, # font size\\n              box.padding = 0.2,\\n              min.segment.length = 0, # draw all line segments\\n              max.overlaps = Inf,\\n              segment.size = 0.2,\\n              segment.color = \\"grey\\",\\n              segment.alpha = 0.3) +\\n    scale_size(range = c(0, 10)) +\\n    scale_x_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    labs(x = \\"Speed (Parquet)\\", y = \\"Speed (Feather)\\",\\n         shape = \\"Zstd Level\\", color = \\"Zstd Level\\")\\n```\\n\\n</details>\\n\\nimport Svg10 from \'./index_files/figure-gfm/pairwise-runtime-comparion-1.svg\';\\n\\n<Svg10 />\\n\\nThe above scatterplot has an identity line. Points on this line indicates that\\nthere is no speed difference between Parquet and Feather. Feather is faster for\\npoints below the line, and Parquet is faster for points above the line.\\n\\n:::tip Analysis\\nIn addition to the above boxplot, this scatterplot makes it clearer to see the\\nimpact of the schemas.\\n\\nInterestingly, **there is no significant difference in Zstd levels -5 and 1,\\nwhile levels 9 and 19 stand apart further**. Disabling compression for Feather\\nhas a stronger effect on speed than for Parquet.\\n\\nOverall, we were surprised that **Feather and Parquet are not far apart in terms\\nof write performance once compression is enabled**. Only when compression is\\ndisabled, Parquet is substantially slower in our measurements.\\n:::\\n\\n## Space-Time Trade-off\\n\\nFinally, we combine the size and speed analysis into a single benchmark. Our\\ngoal is to find an optimal parameterization, i.e., one that strictly dominates\\nothers. To this end, we now plot size against speed:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\ncleaned <- unified |>\\n  filter(schema %in% schemas_gt100k) |>\\n  mutate(zstd.level = factor(str_replace_na(zstd.level),\\n                             levels = c(\\"NA\\", \\"-5\\", \\"1\\", \\"9\\", \\"19\\"))) |>\\n  group_by(schema, store_class, zstd.level) |>\\n  summarize(bytes = median(bytes), duration = median(duration))\\n\\ncleaned |>\\n  ggplot(aes(x = bytes, y = duration,\\n             shape = store_class, color = zstd.level)) +\\n    geom_point(size = 3, alpha = 0.7) +\\n    geom_text_repel(aes(label = schema),\\n              color = \\"grey\\",\\n              size = 1, # font size\\n              box.padding = 0.2,\\n              min.segment.length = 0, # draw all line segments\\n              max.overlaps = Inf,\\n              segment.size = 0.2,\\n              segment.color = \\"grey\\",\\n              segment.alpha = 0.3) +\\n    scale_x_log10(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    labs(x = \\"Size\\", y = \\"Speed\\", shape = \\"Store\\", color = \\"Zstd Level\\")\\n```\\n\\n</details>\\n\\nimport Svg11 from \'./index_files/figure-gfm/points-by-level-1.svg\';\\n\\n<Svg11 />\\n\\nEvery point in the above log-log scatterplot represents a store with a fixed\\nschema. Since we have multiple stores for a given schema, we took the median\\nsize and median speed. We then varied the run matrix by Zstd level (color) and\\nstore type (triangle/point shape). Points closer to the origin are \u201cbetter\u201d in\\nboth dimensions. So we\u2019re looking for the left-most and bottom-most ones.\\nDisabling compression puts points into the bottom-right area, and maximum\\ncompression into the top-left area.\\n\\nThe point closest to the origin has the schema `zeek.dce_rpc` for Zstd level 1,\\nboth for Feather and Parquet. Is there anything special about this log file?\\nHere\u2019s a sample:\\n\\n    #separator \\\\x09\\n    #set_separator  ,\\n    #empty_field    (empty)\\n    #unset_field    -\\n    #path   dce_rpc\\n    #open   2022-04-20-09-56-46\\n    #fields ts  uid id.orig_h   id.orig_p   id.resp_h   id.resp_p   rtt named_pipe  endpoint    operation\\n    #types  time    string  addr    port    addr    port    interval    string  string  string\\n    1637222709.134638   Cypdo7cTBbiS4Asad   10.2.9.133  49768   10.2.9.9    135 0.000254    135 epmapper    ept_map\\n    1637222709.140898   CTDU3j3iAXfRITNiah  10.2.9.133  49769   10.2.9.9    49671   0.000239    49671   drsuapi DRSBind\\n    1637222709.141520   CTDU3j3iAXfRITNiah  10.2.9.133  49769   10.2.9.9    49671   0.000311    49671   drsuapi DRSCrackNames\\n    1637222709.142068   CTDU3j3iAXfRITNiah  10.2.9.133  49769   10.2.9.9    49671   0.000137    49671   drsuapi DRSUnbind\\n    1637222709.143104   Cypdo7cTBbiS4Asad   10.2.9.133  49768   10.2.9.9    135 0.000228    135 epmapper    ept_map\\n    1637222709.143642   CTDU3j3iAXfRITNiah  10.2.9.133  49769   10.2.9.9    49671   0.000147    49671   drsuapi DRSBind\\n    1637222709.144040   CTDU3j3iAXfRITNiah  10.2.9.133  49769   10.2.9.9    49671   0.000296    49671   drsuapi DRSCrackNames\\n\\nIt appears to be rather normal: 10 columns, several different data types, unique\\nIDs, and some short strings. By looking at the data alone, there is no obvious\\nhint that explains the performance.\\n\\nWith dozens to hundreds of different schemas per data source (sometimes even\\nmore), there it will be difficult to single out individual schemas. But a point\\ncloud is unwieldy for relative comparison. To better represent the variance of\\nschemas for a given configuration, we can strip the \u201cinner\u201d points and only look\\nat their convex hull:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\n# Native convex hull does the job, no need to leverage ggforce.\\nconvex_hull <- cleaned |>\\n  group_by(store_class, zstd.level) |>\\n  slice(chull(x = bytes, y = duration))\\n\\nconvex_hull |>\\n  ggplot(aes(x = bytes, y = duration,\\n             shape = store_class, color = zstd.level)) +\\n    geom_point(size = 3) +\\n    geom_polygon(aes(fill = zstd.level, color = zstd.level),\\n                 alpha = 0.1,\\n                 show.legend = FALSE) +\\n    scale_x_log10(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    labs(x = \\"Size\\", y = \\"Speed\\", shape = \\"Store\\", color = \\"Zstd Level\\")\\n```\\n\\n</details>\\n\\nimport Svg12 from \'./index_files/figure-gfm/convex-hull-1.svg\';\\n\\n<Svg12 />\\n\\nIntuitively, the area of a given polygon captures its variance. A smaller area\\nis \u201cgood\u201d in that it offers more predictable behavior. The high amount of\\noverlap makes it still difficult to perform clearer comparisons. If we facet by\\nstore type, it becomes easier to compare the areas:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\ncleaned |>\\n  group_by(store_class, zstd.level) |>\\n  # Native convex hull does the job, no need to leverage ggforce.\\n  slice(chull(x = bytes, y = duration)) |>\\n  ggplot(aes(x = bytes, y = duration,\\n             shape = store_class, color = store_class)) +\\n    geom_point(size = 3) +\\n    geom_polygon(aes(color = store_class, fill = store_class),\\n                 alpha = 0.3,\\n                 show.legend = FALSE) +\\n    scale_x_log10(n.breaks = 4, labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    labs(x = \\"Size\\", y = \\"Speed\\", shape = \\"Store\\", color = \\"Store\\") +\\n    facet_grid(cols = vars(zstd.level)) +\\n    theme_bw_trans()\\n```\\n\\n</details>\\n\\nimport Svg13 from \'./index_files/figure-gfm/convex-hull-facet-by-level-1.svg\';\\n\\n<Svg13 />\\n\\nArranging the facets above row-wise makes it easier to compare the y-axis, i.e.,\\nspeed, where lower polygons are better. Arranging them column-wise makes it easier\\nto compare the x-axis, i.e., size, where the left-most polygons are better:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\ncleaned |>\\n  group_by(store_class, zstd.level) |>\\n  slice(chull(x = bytes, y = duration)) |>\\n  ggplot(aes(x = bytes, y = duration,\\n             shape = zstd.level, color = zstd.level)) +\\n    geom_point(size = 3) +\\n    geom_polygon(aes(color = zstd.level, fill = zstd.level),\\n                 alpha = 0.3,\\n                 show.legend = FALSE) +\\n    scale_x_log10(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    labs(x = \\"Size\\", y = \\"Speed\\", shape = \\"Zstd Level\\", color = \\"Zstd Level\\") +\\n    facet_grid(rows = vars(store_class)) +\\n    theme_bw_trans()\\n```\\n\\n</details>\\n\\nimport Svg14 from \'./index_files/figure-gfm/convex-hull-facet-by-store-1.svg\';\\n\\n<Svg14 />\\n\\n:::tip Analysis\\nAcross both dimensions, **Zstd level 1 shows the best average space-time\\ntrade-off for both Parquet and Feather**. In the above plots, we also observe our\\nfindings from the speed analysis: Parquet still dominates when compression is\\nenabled.\\n:::\\n\\n## Conclusion\\n\\nIn summary, we set out to better understand how Parquet and Feather behave on\\nthe write path of VAST, when acquiring security telemetry from high-volume data\\nsources. Our findings show that columnar Zstd compression offers great space\\nsavings for both Parquet and Feather. For certain schemas, Feather and Parquet\\nexhibit only a marginal differences. To our surprise, writing Parquet files is\\nstill faster than Feather for our workloads.\\n\\nThe pressing next question is obviously: what about the read path, i.e., query\\nlatency? This is a topic for future, stay tuned."},{"id":"/vast-v2.3.1","metadata":{"permalink":"/blog/vast-v2.3.1","source":"@site/blog/vast-v2.3.1/index.md","title":"VAST v2.3.1","description":"VAST v2.3.1 is now available. This small bugfix release","date":"2022-10-17T00:00:00.000Z","formattedDate":"October 17, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"rebuild","permalink":"/blog/tags/rebuild"},{"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":0.225,"hasTruncateMarker":false,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"}],"frontMatter":{"title":"VAST v2.3.1","authors":"lava","date":"2022-10-17T00:00:00.000Z","tags":["release","rebuild","performance"]},"prevItem":{"title":"Parquet & Feather: Writing Security Telemetry","permalink":"/blog/parquet-and-feather-writing-security-telemetry"},"nextItem":{"title":"Parquet & Feather: Enabling Open Investigations","permalink":"/blog/parquet-and-feather-enabling-open-investigations"}},"content":"[VAST v2.3.1][github-vast-release] is now available. This small bugfix release\\naddresses an issue where [compaction][compaction] would hang if encountering\\ninvalid partitions that were produced by older versions of VAST when a large\\n`max-partition-size` was set in combination with badly compressible input data.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.3.1\\n[compaction]: https://vast.io/VAST%20v3.0/use/transform#transform-old-data-when-reaching-storage-quota"},{"id":"/parquet-and-feather-enabling-open-investigations","metadata":{"permalink":"/blog/parquet-and-feather-enabling-open-investigations","source":"@site/blog/parquet-and-feather-enabling-open-investigations/index.md","title":"Parquet & Feather: Enabling Open Investigations","description":"Apache Parquet is the common denominator for structured data at rest.","date":"2022-10-07T00:00:00.000Z","formattedDate":"October 7, 2022","tags":[{"label":"arrow","permalink":"/blog/tags/arrow"},{"label":"parquet","permalink":"/blog/tags/parquet"},{"label":"feather","permalink":"/blog/tags/feather"}],"readingTime":5.17,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"},{"name":"Thomas Peiselt","title":"Data Engineer","url":"https://github.com/dispanser","email":"thomas@tenzir.com","imageURL":"https://github.com/dispanser.png","key":"dispanser"}],"frontMatter":{"title":"Parquet & Feather: Enabling Open Investigations","authors":["mavam","dispanser"],"date":"2022-10-07T00:00:00.000Z","last_updated":"2023-01-10T00:00:00.000Z","tags":["arrow","parquet","feather"]},"prevItem":{"title":"VAST v2.3.1","permalink":"/blog/vast-v2.3.1"},"nextItem":{"title":"A Git Retrospective","permalink":"/blog/a-git-retrospective"}},"content":"[Apache Parquet][parquet] is the common denominator for structured data at rest.\\nThe data science ecosystem has long appreciated this. But infosec? Why should\\nyou care about Parquet when building a threat detection and investigation\\nplatform? In this blog post series we share our opinionated view on this\\nquestion. In the next three blog posts, we\\n\\n1. describe how VAST uses Parquet and its little brother [Feather][feather]\\n2. benchmark the two formats against each other for typical workloads\\n3. share our experience with all the engineering gotchas we encountered along\\n   the way\\n\\n[parquet]: https://parquet.apache.org/\\n[feather]: https://arrow.apache.org/docs/python/feather.html\\n\\n\x3c!--truncate--\x3e\\n\\n:::info Parquet & Feather: 1/3\\nThis is blog post is part of a 3-piece series on Parquet and Feather.\\n\\n1. This blog post\\n2. [Writing Security Telemetry][parquet-and-feather-2]\\n3. [Data Engineering Woes][parquet-and-feather-3]\\n\\n[parquet-and-feather-2]: /blog/parquet-and-feather-writing-security-telemetry/\\n[parquet-and-feather-3]: /blog/parquet-and-feather-data-engineering-woes/\\n:::\\n\\n## Why Parquet and Feather?\\n\\nParquet is the de-facto standard for storing structured data in a format\\nconducive for analytics. Nearly all analytics engines support reading Parquet\\nfiles to load a dataset in memory for subsequent analysis.\\n\\nThe data science community has long built on this foundation, but the majority\\nof infosec tooling [does not build on an open\\nfoundation](/VAST%20v3.0/about/vision#the-soc-architecture-maze). Too many\\nproducts hide their data behind silos, either wrapped behind a SaaS with a thin\\nAPI, or in a custom format that requires cumbersome ETL pipelines. Nearly all\\nadvanced use cases require full access to the data. Especially when\\nthe goal is developing realtime threat detection and response systems.\\n\\nSecurity is a data problem. But how should we represent that data? This is where\\nParquet enters the picture. As a vendor-agnostic storage format for structured\\nand nested data, it decouples storage from analytics. This is where SIEM\\nmonoliths fail: they offer a single black box that tightly couples data\\nacquisition and processing capabilities. Providing a thin \\"open\\" API is not really\\nopen, as it prevents high-bandwidth data access that is needed for advanced\\nanalytics workloads.\\n\\nOpen storage prevents vendor-lock-in. When any tool can work with the data, you\\nbuild a sustainable foundation for implementing future use cases. For example,\\nwith Parquet\'s column encryption, you can offload fine-grained compliance use\\ncases to a dedicated application. Want to try out a new analytics engine? Just\\npoint it to the Parquet files.\\n\\n## Parquet\'s Little Brother\\n\\n[Feather][feather] is Parquet\'s little brother. It emerged while building a\\nproof of concept for \\"fast, language-agnostic data frame storage for Python\\n(pandas) and R.\\" The format is a thin layer on top of [Arrow\\nIPC](https://arrow.apache.org/docs/python/ipc.html#ipc), making it conducive for\\nmemory mapping and zero-copy usage. On the spectrum of speed and\\nspace-efficiency, think of it this way:\\n\\n![Parquet vs. Feather](parquet-vs-feather.excalidraw.svg)\\n\\nBefore Feather existed, VAST had its own storage format that was 95% like\\nFeather, minus a thin framing. (We called it the *segment store*.)\\n\\nWait, but Feather is an in-memory format and Parquet an on-disk format. You\\ncannot compare them! Fair point, but don\'t forget transparent Zstd compression.\\nFor some schemas, we barely notice a difference (e.g., PCAP), whereas for\\nothers, Parquet stores boil down to a fraction of their Feather equivalent.\\n\\nThe [next blog post][parquet-and-feather-2] goes into these details. For now, we\\nwant to stress that Feather is in fact a reasonable format for data at rest,\\neven when looking at space utilization alone.\\n\\n## Parquet and Feather in VAST\\n\\nVAST can store event data as Parquet or Feather. The unit of storage scaling is\\na *partition*. In Arrow terms, a partition is a persisted form of an [Arrow\\nTable][arrow-table], i.e., a concatenation of [Record\\nBatches][arrow-record-batch]. A partition has thus a fixed schema. VAST\'s [store\\nplugin][store-plugin] determines how a partition writes its buffered record\\nbatches to disk. The diagram below illustrates the architecture:\\n\\n![Parquet Analytics](parquet-analytics.excalidraw.svg)\\n\\n[arrow-table]: https://arrow.apache.org/docs/python/data.html#tables\\n[arrow-record-batch]: https://arrow.apache.org/docs/python/data.html#record-batches\\n[store-plugin]: /VAST%20v3.0/understand/architecture/plugins#store\\n\\nThis architecture makes it easy to point an analytics application directly to\\nthe store files, without the need for ETLing it into a dedicated warehouse, such\\nas Spark or Hadoop.\\n\\nThe event data thrown at VAST has quite some variety of schemas. During\\ningestion, VAST first demultiplexes the heterogeneous stream of events into\\nmultiple homogeneous streams, each of which has a unique schema. VAST buffers\\nevents until the partition hits a pre-configured event limit (e.g., 1M) or until\\na timeout occurs (e.g., 60m). Thereafter, VAST writes the partition in one shot\\nand persists it.\\n\\nThe buffering provides optimal freshness of the data, as it enables queries run\\non not-yet-persisted data. But it also sets an upper bound on the partition\\nsize, given that it must fit in memory in its entirety. In the future, we plan\\nto make this freshness trade-off explicit, making it possible to write out\\nlarger-than-memory stores incrementally.\\n\\n## Imbuing Domain Semantics\\n\\nIn a [past blog][blog-arrow] we described how VAST uses Arrow\'s extensible\\ntype system to add richer semantics to the data. This is how the value of VAST\\ntranscends through the analytics stack. For example, VAST has native IP address\\ntypes that you can show up in Python as [ipaddress][ipaddress] instance. This\\navoids friction in the data exchange process. Nobody wants to spend time\\nconverting bytes or strings into the semantic objects that are ultimately need\\nfor the analysis.\\n\\n[blog-arrow]: /blog/apache-arrow-as-platform-for-security-data-engineering\\n[ipaddress]: https://docs.python.org/3/library/ipaddress.html\\n\\nHere\'s how [VAST\'s type system](/VAST%20v3.0/understand/data-model/type-system)\\nlooks like:\\n\\n![Type System - VAST](type-system-vast.excalidraw.svg)\\n\\nThere exist two major classes of types: *basic*, stateless types with a static\\nstructure and a-priori known representation, and *complex*, stateful types that\\ncarry additional runtime information. We map this type system without\\ninformation loss to Arrow:\\n\\n![Type System - Arrow](type-system-arrow.excalidraw.svg)\\n\\nVAST converts enum, address, and subnet types to\\n[extension-types][arrow-extension-types]. All types are self-describing and part\\nof the record batch meta data. Conversion is bi-directional. Both Parquet and\\nFeather support fully nested structures in this type system. In theory. Our\\nthird blog post in this series describes the hurdles we had to overcome to make\\nit work in practice.\\n\\n[arrow-extension-types]: https://arrow.apache.org/docs/format/Columnar.html#extension-types\\n\\nIn the next blog post, we perform a quantitative analysis of the two formats: how\\nwell do they compress the original data? How much space do they take up in\\nmemory? How much CPU time do I pay for how much space savings? In the meantime,\\nif you want to learn more about Parquet, take a look at the [blog post\\nseries][arrow-parquet-blog] from the Arrow team.\\n\\n[arrow-parquet-blog]: https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/"},{"id":"/a-git-retrospective","metadata":{"permalink":"/blog/a-git-retrospective","source":"@site/blog/a-git-retrospective/index.md","title":"A Git Retrospective","description":"The VAST project is roughly a decade old. But what happened over the last 10","date":"2022-09-15T00:00:00.000Z","formattedDate":"September 15, 2022","tags":[{"label":"git","permalink":"/blog/tags/git"},{"label":"r","permalink":"/blog/tags/r"},{"label":"quarto","permalink":"/blog/tags/quarto"},{"label":"notebooks","permalink":"/blog/tags/notebooks"},{"label":"engineering","permalink":"/blog/tags/engineering"},{"label":"open-source","permalink":"/blog/tags/open-source"}],"readingTime":4.54,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"A Git Retrospective","authors":"mavam","date":"2022-09-15T00:00:00.000Z","tags":["git","r","quarto","notebooks","engineering","open-source"]},"prevItem":{"title":"Parquet & Feather: Enabling Open Investigations","permalink":"/blog/parquet-and-feather-enabling-open-investigations"},"nextItem":{"title":"Public Roadmap and Open RFCs","permalink":"/blog/public-roadmap-and-open-rfcs"}},"content":"The VAST project is roughly a decade old. But what happened over the last 10\\nyears? This blog post looks back over time through the lens of the git *merge*\\ncommits.\\n\\nWhy merge commits? Because they represent a unit of completed contribution.\\nFeature work takes place in dedicated branches, with the merge to the main\\nbranch sealing the deal. Some feature branches have just one commit, whereas\\nothers dozens. The distribution is not uniform. As of `6f9c84198` on Sep 2,\\n2022, there are a total of 13,066 commits, with 2,334 being merges (17.9%).\\nWe\u2019ll take a deeper look at the merge commits.\\n\\n\x3c!--truncate--\x3e\\n\\n``` bash\\ncd /tmp\\ngit clone https://github.com/tenzir/vast.git\\ncd vast\\ngit checkout 6f9c841980b2333028b1ac19e2a21e99d96cbd36\\ngit log --merges --pretty=format:\\"%ad|%d\\" --date=iso-strict |\\n  sed -E \'s/(.+)\\\\|.*tag: ([^,)]+).*/\\\\1 \\\\2/\' |\\n  sed -E \'s/(.*)\\\\|.*/\\\\1 NA/\' \\\\\\n  > /tmp/vast-merge-commits.txt\\n```\\n\\nFor the statistics, we\u2019ll switch to R. In all subsequent figures, a single point\\ncorresponds to a merge commit. The reduced opacity alleviates the effects of\\noverplotting.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(dplyr)\\nlibrary(ggplot2)\\nlibrary(lubridate)\\nlibrary(readr)\\n\\ntheme_set(theme_minimal())\\n\\ndata <- read_table(\\"/tmp/vast-merge-commits.txt\\",\\n  col_names = c(\\"time\\", \\"tag\\"),\\n  col_types = \\"Tc\\"\\n) |>\\n  arrange(time) |>\\n  mutate(count = row_number())\\n\\nfirst_contribution <- \\\\(x) data |>\\n  filter(time >= x) |>\\n  pull(count) |>\\n  first()\\n\\nevents <- tribble(\\n  ~time, ~event,\\n  ymd(\\"2016-03-17\\"), \\"NSDI \'16\\\\npublication\\",\\n  ymd(\\"2017-08-31\\"), \\"Tenzir\\\\nincorporated\\",\\n  ymd(\\"2018-07-01\\"), \\"Tobias\\",\\n  ymd(\\"2019-09-15\\"), \\"Dominik\\",\\n  ymd(\\"2020-01-01\\"), \\"Benno\\",\\n  ymd(\\"2021-12-01\\"), \\"Thomas\\",\\n  ymd(\\"2022-07-01\\"), \\"Patryk\\",\\n) |>\\n  mutate(time = as.POSIXct(time), count = Vectorize(first_contribution)(time))\\n\\ndata |>\\n  ggplot(aes(x = time, y = count)) +\\n  geom_point(size = 1, alpha = 0.2) +\\n  geom_segment(\\n    data = events,\\n    aes(xend = time, yend = count + 200),\\n    color = \\"red\\"\\n  ) +\\n  geom_label(\\n    data = events,\\n    aes(y = count + 200, label = event),\\n    color = \\"red\\",\\n    size = 2\\n  ) +\\n  scale_x_datetime(date_breaks = \\"1 year\\", date_labels = \\"%Y\\") +\\n  labs(x = \\"Time\\", y = \\"Merge Commits\\")\\n```\\n\\n</details>\\n\\nimport Svg1 from \'./index_files/figure-gfm/full-time-spectrum-1.svg\';\\n\\n<Svg1 />\\n\\nPrior to Tenzir taking ownership of the project and developing VAST, it was a\\ndissertation project evolving along during PhD work at the University of\\nCalifornia, Berkeley. We can see that the first pre-submission crunch started a\\nfew months before the [NSDI \u201916\\npaper](https://matthias.vallentin.net/papers/nsdi16.pdf).\\n\\nTenzir was born in fall 2017. Real-world contributions arrived as of 2018 when\\nthe small team set sails. Throughput increased as core contributors joined the\\nteam. Fast-forward to 2020 when we started doing public releases. The figure\\nbelow shows how this process matured.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(ggrepel)\\n\\ndata |>\\n  ggplot(aes(x = time, y = count, label = tag)) +\\n  geom_point(size = 1, alpha = 0.1) +\\n  geom_text_repel(\\n    size = 2,\\n    min.segment.length = 0,\\n    max.overlaps = Inf,\\n    segment.color = \\"red\\",\\n    segment.alpha = 0.2,\\n    box.padding = 0.2\\n  ) +\\n  scale_x_datetime(\\n    date_breaks = \\"1 year\\",\\n    limits = c(as.POSIXct(ymd(\\"2020-01-01\\")), max(data$time)),\\n    date_labels = \\"%Y\\"\\n  ) +\\n  labs(x = \\"Time\\", y = \\"Merge Commits\\")\\n```\\n\\n</details>\\n\\nimport Svg2 from \'./index_files/figure-gfm/since-2020-1.svg\';\\n\\n<Svg2 />\\n\\nAs visible from the tag labels, we were at [CalVer](https://calver.org) for a\\nwhile, but ultimately switched to [SemVer](https://semver.org). Because we had\\nalready commercial users at the time, this helped us better communicate breaking\\nvs.\xa0non-breaking changes.\\n\\nLet\u2019s zoom in on all releases since v1.0. At this time, we had a solid\\nengineering and release process in place.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(tidyr)\\nv1_0_0_rc1_time <- data |>\\n  filter(tag == \\"v1.0.0-rc1\\") |>\\n  pull(time)\\n\\nsince_v1_0_0_rc1 <- data |> filter(time >= v1_0_0_rc1_time)\\n\\nrc <- since_v1_0_0_rc1 |>\\n  drop_na() |>\\n  filter(grepl(\\"rc\\", tag))\\n\\nnon_rc <- since_v1_0_0_rc1 |>\\n  drop_na() |>\\n  filter(!grepl(\\"rc\\", tag))\\n\\nsince_v1_0_0_rc1 |>\\n  ggplot(aes(x = time, y = count, label = tag)) +\\n  geom_point(size = 1, alpha = 0.2) +\\n  geom_segment(\\n    data = non_rc,\\n    aes(xend = time, yend = min(count)), color = \\"red\\"\\n  ) +\\n  geom_text_repel(\\n    size = 2,\\n    min.segment.length = 0,\\n    max.overlaps = Inf,\\n    segment.color = \\"grey\\",\\n    box.padding = 0.7\\n  ) +\\n  geom_point(\\n    data = rc, aes(x = time, y = count),\\n    color = \\"blue\\",\\n    size = 2\\n  ) +\\n  geom_point(\\n    data = non_rc, aes(x = time, y = count),\\n    color = \\"red\\",\\n    size = 2\\n  ) +\\n  geom_label(data = non_rc, aes(y = min(count)), size = 2, color = \\"red\\") +\\n  scale_x_datetime(date_breaks = \\"1 month\\", date_labels = \\"%b %y\\") +\\n  labs(x = \\"Time\\", y = \\"Merge Commits\\")\\n```\\n\\n</details>\\n\\nimport Svg3 from \'./index_files/figure-gfm/since-v1.0-1.svg\';\\n\\n<Svg3 />\\n\\nThe v2.0 release was a hard one for us, given the long distance to v1.1. We\\nmerged too much and testing took forever. Burnt by the time sunk in testing and\\nfixups, we decided to switch to an LPU model (\u201cleast publishable unit\u201d) to\\nreduce release cadence. We didn\u2019t manage to implement this model until after\\nv2.1 though, where the release cadence finally gets smaller. A monthly release\\nfeels about the right for our team size.\\n\\nThe key challenge is minimizing the feature freeze phase. The first release\\ncandidate (RC) kicks this phase off, and the final release lifts the\\nrestriction. In this period, features are not allowed to be merged.[^1] This is\\na delicate time window: too long and the fixups in the RC phase cause the\\npostponed pull requests to diverge, too short and we compromise on testing\\nrigor, causing a release that doesn\u2019t meet our Q&A requirements.\\n\\nThis is where we stand as of today. We\u2019re happy how far along we came, but\\nmany challenges still lay ahead of us. Increased automation and deeper testing\\nis the overarching theme, e.g., code coverage, fuzzing, GitOps. We\u2019re constantly\\nstriving to improve or processes. With a small team of passionate, senior\\nengineers, this is a lot of fun!\\n\\n[^1]: We enforced this with a `blocked` label. CI [doesn\u2019t allow\\n    merging](https://github.com/tenzir/vast/blob/6f9c841980b2333028b1ac19e2a21e99d96cbd36/.github/workflows/blocked.yaml) when this label is on a pull request."},{"id":"/public-roadmap-and-open-rfcs","metadata":{"permalink":"/blog/public-roadmap-and-open-rfcs","source":"@site/blog/public-roadmap-and-open-rfcs/index.md","title":"Public Roadmap and Open RFCs","description":"Open Source needs Open Governance","date":"2022-09-07T00:00:00.000Z","formattedDate":"September 7, 2022","tags":[{"label":"roadmap","permalink":"/blog/tags/roadmap"},{"label":"github","permalink":"/blog/tags/github"},{"label":"rfc","permalink":"/blog/tags/rfc"},{"label":"open-source","permalink":"/blog/tags/open-source"}],"readingTime":3.745,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Public Roadmap and Open RFCs","description":"Open Source needs Open Governance","authors":"mavam","date":"2022-09-07T00:00:00.000Z","last_updated":"2023-02-08T00:00:00.000Z","tags":["roadmap","github","rfc","open-source"]},"prevItem":{"title":"A Git Retrospective","permalink":"/blog/a-git-retrospective"},"nextItem":{"title":"VAST v2.3","permalink":"/blog/vast-v2.3"}},"content":"We are happy to announce that we have published [our engineering\\nroadmap][roadmap] along with an [RFC process][rfc] to actively participate in\\nshaping upcoming topics. This blog post explains why and how we did it.\\n\\n[roadmap]: https://vast.io/roadmap\\n[rfc]: https://github.com/tenzir/vast/tree/master/rfc\\n\\n\x3c!--truncate--\x3e\\n\\nAs a community-first open-source project, we constantly strive for increasing\\ntransparency. Our long-term goal is establishing a fully open governance model.\\nThis will allow for a clear delineation of the open-source project and unbiased\\ncommercial offerings that we, the engineering team behind VAST at\\n[Tenzir](https://tenzir.com), provide on top. Until we have bootstrapped\\nourselves and an active community, we aim for the right balance between open\\nand closed.\\n\\nOne step in the direction of open is publishing our [roadmap][roadmap] and\\nenabling the community to participate in the planning through an [Request For\\nComments (RFC)][rfc] process.\\n\\n## Public Roadmap\\n\\nIn the process of opening the roadmap, we had to answer several questions:\\n\\n1. **Audience**: should the content be for users only? What about developers?\\n   Should we only mention features or also refactorings?\\n\\n2. **Interaction**: should this just be a read-only page or something the\\n   community can directly interact with?\\n\\n3. **Tooling**: what is the right tool to encode the roadmap?\\n\\nLet\'s go through them one by one.\\n\\nRegarding audience, we want to avoid an overly narrow target group, as we are in\\nphase of growth where breadth instead of depth is more important. Moreover, we\\ngain more transparency if we can unveil all ongoing thrusts. Therefore, we want\\nto cover the full spectrum of personas, but make it possible for each individual\\ntype of persona to get a relevant view.\\n\\nRegarding interaction, we are actively looking for engagement. Throwing a\\nread-only version over the fence to the community is certainly informational,\\nbut we are looking for creating dialogue. Therefore, we want to allow everyone\\nto discuss the various roadmap items in the open.\\n\\nRegarding tooling, we are in need for something that integrates well with the\\nexisting environment. Our GitHub presence includes code, documentation, website\\ncontent, and third-party integrations. We also promote use of GitHub Discussions\\nto engage with us. This makes GitHub the focal point to engage with the content.\\nTherefore, we decided to encode the roadmap as GitHub issues; for clarity in a\\ndedicated repository at <https://github.com/tenzir/public-roadmap>.\\n\\nWe decided against dual-purposing the issue tracker of our main repository\\n<https://github.com/tenzir/vast> because it would add roadmap items as many\\nopen, long-running issues that scatter the attention and potentially confuse the\\ncommunity. That said, the primary value of the issue tracker is the layer on top\\nof issues: [GitHub Projects][github-projects], which allows for organizing\\nissues across multiple dimensions in a visually appealing way.\\n\\n[github-projects]: https://docs.github.com/en/issues/planning-and-tracking-with-projects\\n\\nThe quarterly board view make it easy to understand ongoing thrusts:\\n\\n[![Github Roadmap - Board](roadmap-board.jpg)][roadmap]\\n\\nThe milestones view provides a different perspective that focuses more on the\\nbigger-picture theme:\\n\\n[![Github Roadmap - Milestones](roadmap-milestones.jpg)][roadmap]\\n\\n## Open RFCs\\n\\nThe roadmap provides a lens into the short-term future. We don\'t want it to be\\njust read-only. Fundamentally, we want to build something that our users love.\\nWe also want to tap into the full potential of our enthusiasts by making it\\npossible to engage in technical depth with upcoming changes. Therefore, we are\\nestablishing a formal [Request for Comments (RFC) process][rfc].\\n\\nTo get an idea, how an RFC looks like, here\'s the [RFC template][rfc-template]:\\n\\n[rfc-template]: https://github.com/tenzir/vast/blob/main/rfc/000-template/README.md\\n\\nimport CodeBlock from \'@theme/CodeBlock\';\\nimport Template from \'!!raw-loader!@site/../rfc/000-template/README.md\';\\n\\n<CodeBlock language=\\"markdown\\">{Template}</CodeBlock>\\n\\n[RFC-001: Composable Pipelines](https://github.com/tenzir/vast/pull/2511) is an\\nexample instantiation of this template.\\n\\nThe RFC reviews take place 100% in the open. As of today, reviewers constitute\\nmembers from Tenzir\'s engineering team. Given our current resource constraints\\nand project state, we can only support a corporate-backed governance model. That\\nsaid, opening ourselves up is laying the foundation of trust and committment\\nthat we want to go beyond a walled garden. We understand that this is a long\\njourney and are excited about what\'s ahead of us.\\n\\nWhen an RFC gets accepted, it means that we put it on the roadmap, adjacent to\\nexisting items that compete for prioritization. In other words, even though we\\naccepted an RFC, there will be an indeterminate period of time until we can\\ndevote resources. We will always encourage community-led efforts and are\\nenthusiastic about supporting external projects that we can support within our\\ncapacities.\\n\\nThese are our \\"growing pains\\" that we can hopefully overcome together while\\nbuilding a thriving community. We still have our [community chat](/discord)\\nwhere we are looking forward to interact with everyone with questions or\\nfeedback. See you there!"},{"id":"/vast-v2.3","metadata":{"permalink":"/blog/vast-v2.3","source":"@site/blog/vast-v2.3/index.md","title":"VAST v2.3","description":"Automatic Rebuilds","date":"2022-09-01T00:00:00.000Z","formattedDate":"September 1, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"rebuild","permalink":"/blog/tags/rebuild"},{"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":3.9,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.3","description":"Automatic Rebuilds","authors":"dominiklohmann","date":"2022-09-01T00:00:00.000Z","tags":["release","rebuild","performance"]},"prevItem":{"title":"Public Roadmap and Open RFCs","permalink":"/blog/public-roadmap-and-open-rfcs"},"nextItem":{"title":"Richer Typing in Sigma","permalink":"/blog/richer-typing-in-sigma"}},"content":"[VAST v2.3][github-vast-release] is now available, which introduces an automatic\\ndata defragmentation capability.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.3.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Automatic Rebuilds\\n\\nVAST server processes now continuously rebuild partitions in the background. The\\nfollowing diagram visualizes what happens under the hood:\\n\\n![Rebuild](rebuild.excalidraw.svg)\\n\\nRebuilding kicks in when a partition has the following properties:\\n\\n1. **Outdated**: if a partitions does not have the latest partition version, it\\n   may not enjoy the latest features and optimizations. It makes it also faster\\n   to adopt VAST versions that include breaking changes in the storage layout.\\n   Therefore, VAST rebuilds outdated partitions to bring them into the most\\n   recent state.\\n\\n2. **Undersized**: numerous small partitions can cause fragmentation in the\\n   catalog, causing higher memory consumption, larger database footprint, and\\n   slower queries. Rebuilding merges undersized partitions, thereby\\n   defragmenting the system. This reduces the resource footprint and makes\\n   queries faster.\\n\\nTo enable automatic rebuilding, set the new `vast.automatic-rebuild` option.\\n\\n```yaml\\nvast:\\n  # Control automatic rebuilding of partitions in the background for\\n  # optimization purposes. The given number controls how many rebuilds to run\\n  # concurrently, and thus directly controls the performance vs. memory and CPU\\n  # usage trade-off. Set to 0 to disable. Defaults to 1.\\n  automatic-rebuild: 1\\n```\\n\\nNow that we have an LSM-style merge operation of partitions, we reduced\\nthe partition cutoff timeout to 5 minutes from 1 hour by default (controlled\\nthrough the option `vast.active-partition-timeout`). This reduces the risk of\\ndata loss in case of a crash. This comes in handy in particular for low-volume\\ndata sources that never exhaust their capacity.\\n\\n## Optional Partition Indexes\\n\\nHistorically, VAST evolved from a special-purpose bitmap indexing system into a\\ngeneral-purpose telemetry engine for security data. Today, VAST has a two-tiered\\nindexing architecture with sparse sketch structures at the top, followed by a\\nsecond layer of dense indexes. As of this release, it is possible to disable\\nthis second layer.\\n\\nThe space savings can be substantial based on the size of your index. For\\nexample, if the first layer of indexing always yields highly selective results,\\nthen it the dense indexes do not provide a lot of value. One scenario would be\\nretro-matching: if you only do IoC-style point queries, they will be most likely\\ncovered well by the sketches. If you do not have selective queries, the dense\\nindex is not helping much anyway, since you need access the base data anyway. A\\nreally good use case for the indexes when your have a scatterd data access\\npatterns, i.e., highly selective results *within* a partition, but a result that\\nspans many disparate partitions.\\n\\nIn a simplified model, VAST performs three steps when executing a query:\\n\\n1. Send the query to the catalog, which maintains VAST\'s partitions, and ask it\\n   for a list of candidate partitions. The catalog maintains the first tier of\\n   sparse indexes, currently one per partition.\\n\\n2. Send the query to all candidate partitions in parallel, each of which\\n   contains dense indexes for all fields in the partition\'s schema. The index\\n   lookup yields a set of candidate records IDs within the partition.\\n\\n3. Send the query to all candidate partition\'s stores, provided the index lookup\\n   yielded record IDs. Then evaluating the query against the candidate events\\n   and return the result.\\n\\nHere\'s how you can configure a partition index to be disabled:\\n\\n```yaml\\nvast:\\n  index:\\n    rules:\\n        # Don\'t create partition indexes the suricata.http.http.url field.\\n      - targets:\\n          - suricata.http.http.url\\n        partition-index: false\\n        # Don\'t create partition indexes for fields of type addr.\\n      - targets:\\n          - :ip\\n        partition-index: false\\n```\\n\\n## Improved Responsiveness Under High Load\\n\\nTwo small changes improve VAST\'s behavior under exceptionally high load.\\n\\nFirst, the new `vast.connection-timeout` option allows for modifying the default\\nclient-to-server connection timeout of 10 seconds. Previously, if a VAST server\\nwas too busy to respond to a new client within 10 seconds, the client simply\\nexited with an unintelligable `request_timeout` error message. Here\'s how you\\ncan set a custom timeout:\\n\\n```yaml\\nvast:\\n  # The timeout for connecting to a VAST server. Set to 0 seconds to wait\\n  # indefinitely.\\n  connection-timeout: 10s\\n```\\n\\nThe option is additionally available under the environment variable\\n`VAST_CONNECTION_TIMEOUT` and the `--connection-timeout` command-line option.\\n\\nSecond, we improved the operability of VAST servers under high load from\\nautomated low-priority queries. We noticed that when spawning thousands of\\nautomated retro-match queries that compaction would stall and make little\\nvisible progress, risking the disk running full or no longer being compliant\\nwith GDPR-related policies enforced by compaction.\\n\\nTo ensure that compaction\'s internal and regular user-issued queries work as\\nexpected even in this scenario, VAST now considers queries issued with\\n`--low-priority`, with even less priority compared to regular queries (down from\\n33.3% to 4%) and internal high-priority queries used for rebuilding and\\ncompaction (down from 12.5% to 1%)."},{"id":"/richer-typing-in-sigma","metadata":{"permalink":"/blog/richer-typing-in-sigma","source":"@site/blog/richer-typing-in-sigma/index.md","title":"Richer Typing in Sigma","description":"Towards Native Sigma Rule Execution","date":"2022-08-12T00:00:00.000Z","formattedDate":"August 12, 2022","tags":[{"label":"sigma","permalink":"/blog/tags/sigma"},{"label":"regex","permalink":"/blog/tags/regex"},{"label":"query-frontend","permalink":"/blog/tags/query-frontend"}],"readingTime":4.705,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Richer Typing in Sigma","description":"Towards Native Sigma Rule Execution","authors":"mavam","date":"2022-08-12T00:00:00.000Z","last_updated":"2023-02-12T00:00:00.000Z","tags":["sigma","regex","query-frontend"]},"prevItem":{"title":"VAST v2.3","permalink":"/blog/vast-v2.3"},"nextItem":{"title":"VAST v2.2","permalink":"/blog/vast-v2.2"}},"content":"VAST\'s [Sigma frontend](/VAST%20v3.0/understand/language/frontends/sigma)\\nnow supports more modifiers. In the Sigma language, modifiers transform\\npredicates in various ways, e.g., to apply a function over a value or to change\\nthe operator of a predicate. Modifiers are the customization point to enhance\\nexpressiveness of query operations.\\n\\nThe new [pySigma][pysigma] effort, which will eventually replace the\\nnow-considered-legacy [sigma][sigma] project, comes with new modifiers as well.\\nMost notably, `lt`, `lte`, `gt`, `gte` provide comparisons over value domains\\nwith a total ordering, e.g., numbers: `x >= 42`. In addition, the `cidr`\\nmodifier interprets a value as subnet, e.g., `10.0.0.0/8`. Richer typing!\\n\\n[sigma]: https://github.com/SigmaHQ/sigma\\n[pysigma]: https://github.com/SigmaHQ/pySigma\\n\\n\x3c!--truncate--\x3e\\n\\nHow does the frontend work? Think of it as a parser that processes the YAML and\\ntranslates it into an expression tree, where the leaves are predicates with\\ntyped operands according to VAST\'s data model. Here\'s how it works:\\n\\n![Sigma Query Frontend](sigma-query-frontend.excalidraw.svg)\\n\\nLet\'s take a closer look at some Sigma rule modifiers:\\n\\n```yaml\\nselection:\\n  x|re: \'f(o+|u)\'\\n  x|lt: 42\\n  x|cidr: 192.168.0.0/23\\n  x|base64offset|contains: \'http://\'\\n```\\n\\nThe `|` symbol applies a modifier to a field. Let\'s walk through the above\\nexample:\\n\\n1. The `re` modifier changes the predicate operand from `x == \\"f(o+|u)\\"` to\\n   `x == /f(o+|u)/`, i.e., the type of the right-hand side changes from `string`\\n   to `pattern`.\\n\\n2. The `lt` modifier changes the predicate operator from `==` to `<`, i.e.,\\n   `x == 42` becomes `x < 42`.\\n\\n3. The `cidr` modifier changes the predicate operand to type subnet. In VAST,\\n   parsing the operand type into a subnet happens automatically, so the Sigma\\n   frontend only changes the operator to `in`. That is, `x == \\"192.168.0.0/23\\"`\\n   becomes `x in 192.168.0.0/23`. Since VAST supports top-k prefix search on\\n   subnets natively, nothing else needs to be changed.\\n\\n   Other backends expand this to:\\n\\n   ```c\\n   x == \\"192.168.0.*\\" || x == \\"192.168.1.*\\"\\n   ```\\n\\n   This expansion logic on strings doesn\'t scale very well: for a `/22`, you\\n   would have to double the number of predicates, and for a `/21` quadruple\\n   them. This is where rich and deep typing in the language pays off.\\n\\n4. `x`: there are two modifiers that operate in a chained fashion,\\n   transforming the predicate in two steps:\\n\\n   1. Initial: `x == \\"http://\\"`\\n   2. `base64offset`: `x == \\"aHR0cDovL\\" || x == \\"h0dHA6Ly\\" || x == \\"odHRwOi8v\\"`\\n   3. `contains`: `x in \\"aHR0cDovL\\" || x in \\"h0dHA6Ly\\" || x in \\"odHRwOi8v\\"`\\n\\n   First, `base64offset` always expands a value into a disjunction of 3\\n   predicates, each of which performs an equality comparison to a\\n   Base64-transformed value.[^1]\\n\\n   Thereafter, the `contains` modifier translates the respective predicate\\n   operator from `==` to `in`. Other Sigma backends that don\'t support substring\\n   search natively transform the value instead by wrapping it into `*`\\n   wildcards, e.g., translate `\\"foo\\"` into `\\"*foo*\\"`.\\n\\n[^1]: What happens under the hood is a padding a string with spaces. [Anton\\nKutepov\'s article][sigma-article] illustrates how this works.\\n\\n[sigma-article]: https://tech-en.netlify.app/articles/en513032/index.html\\n\\nOur ultimate goal is to support a fully function executional platform for Sigma\\nrules. The table below shows the current implementation status of modifiers,\\nwhere \u2705 means implemented, \ud83d\udea7 not yet implemented but possible, and \u274c not yet\\nsupported by VAST\'s execution engine:\\n\\n|Modifier|Use|sigmac|VAST|\\n|--------|---|:----:|:--:|\\n|`contains`|perform a substring search with the value|\u2705|\u2705|\\n|`startswith`|match the value as a prefix|\u2705|\u2705|\\n|`endswith`|match the value as a suffix|\u2705|\u2705|\\n|`base64`|encode the value with Base64|\u2705|\u2705\\n|`base64offset`|encode value as all three possible Base64 variants|\u2705|\u2705\\n|`utf16le`/`wide`|transform the value to UTF16 little endian|\u2705|\ud83d\udea7\\n|`utf16be`|transform the value to UTF16 big endian|\u2705|\ud83d\udea7\\n|`utf16`|transform the value to UTF16|\u2705|\ud83d\udea7\\n|`re`|interpret the value as regular expression|\u2705|\ud83d\udea7\\n|`cidr`|interpret the value as a IP CIDR|\u274c|\u2705\\n|`all`|changes the expression logic from OR to AND|\u2705|\u2705\\n|`lt`|compare less than (`<`) the value|\u274c|\u2705\\n|`lte`|compare less than or equal to (`<=`) the value|\u274c|\u2705\\n|`gt`|compare greater than (`>`) the value|\u274c|\u2705\\n|`gte`|compare greater than or equal to (`>=`) the value|\u274c|\u2705\\n|`expand`|expand value to placeholder strings, e.g., `%something%`|\u274c|\u274c\\n\\nAside from completing the implementation of the missing modifiers, there are\\nthree missing pieces for Sigma rule execution to become viable in VAST:\\n\\n1. **Regular expressions**: VAST currently has no efficient mechanism to execute\\n   regular expressions. A regex lookup requires a full scan of the data.\\n   Moreover, the regular expression execution speed is abysimal. But we are\\n   aware of it and are working on this soon. The good thing is that the\\n   complexity of regular expression execution over batches of data is\\n   manageable, given that we would call into the corresponding [Arrow Compute\\n   function][arrow-containment-tests] for the heavy lifting. The number one\\n   challenge will be reduing the data to scan, because the Bloom-filter-like\\n   sketch data structures in the catalog cannot handle pattern types. If the\\n   sketches cannot identify a candidate set, all data needs to be scanned,\\n\\n   To alleviate the effects of full scans, it\'s possible to winnow down the\\n   candidate set of partitions by executing rules periodically. When making the\\n   windows asymptotically small, this yields effectively streaming execution,\\n   which VAST already supports in the form of \\"live queries\\".\\n\\n2. **Case-insensitive strings**: All strings in Sigma rules are case-insensitive\\n   by default, but VAST\'s string search is case-sensitive. As a workaround, we\\n   could translate Sigma strings into regular expressions, e.g., `\\"Foo\\"` into\\n   `/Foo/i`. Unfortunately there is a big performance gap between string\\n   equality search and regular expression search. We will need to find a better\\n   solution for production-grade rule execution.\\n\\n3. **Field mappings**: while Sigma rules execute already syntactically, VAST\\n   currently doesn\'t touch the field names in the rules and interprets them as\\n   [field extractors][field-extractors]. In other words, VAST doesn\'t support\\n   the Sigma taxonomy yet. Until we provide the mappings, you can already write\\n   generic Sigma rules using [concepts][concepts].\\n\\n[arrow-containment-tests]: https://arrow.apache.org/docs/cpp/compute.html#containment-tests\\n[field-extractors]: /VAST%20v3.0/understand/language/expressions#field-extractor\\n[concepts]: /VAST%20v3.0/understand/data-model/taxonomies#concepts\\n\\nPlease don\'t hesitate to swing by our [community chat](/discord)\\nand talk with us if you are passionate about Sigma and other topics around open\\ndetection and response."},{"id":"/vast-v2.2","metadata":{"permalink":"/blog/vast-v2.2","source":"@site/blog/vast-v2.2/index.md","title":"VAST v2.2","description":"Pipelines","date":"2022-08-05T00:00:00.000Z","formattedDate":"August 5, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"summarize","permalink":"/blog/tags/summarize"},{"label":"pipelines","permalink":"/blog/tags/pipelines"}],"readingTime":2.145,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"}],"frontMatter":{"title":"VAST v2.2","description":"Pipelines","authors":"lava","date":"2022-08-05T00:00:00.000Z","tags":["release","summarize","pipelines"]},"prevItem":{"title":"Richer Typing in Sigma","permalink":"/blog/richer-typing-in-sigma"},"nextItem":{"title":"VAST v2.1","permalink":"/blog/vast-v2.1"}},"content":"We released [VAST v2.2][github-vast-release] \ud83d\ude4c! Transforms now have a new name:\\n[pipelines](/blog/vast-v2.2#transforms-are-now-pipelines). The [summarize\\noperator](/blog/vast-v2.2#summarization-improvements) also underwent a facelift,\\nmaking aggregation functions pluggable and allowing for assigning names to\\noutput fields.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.2.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Transforms are now Pipelines\\n\\nAfter carefully reconsidering our naming decisions related to query execution\\nand data transformation, we came up with a naming convention that does a better\\njob in capturing the underlying concepts.\\n\\nMost notably, we renamed *transforms* to *pipelines*. A transform *step* is now a\\npipeline *operator*. This nomenclature is much more familiar to users coming\\nfrom dataflow and collection-based query engines. The implementation underneath\\nhasn\'t changed. As in the [Volcano model][volcano], data still flows through\\noperators, each of which consumes input from upstream operators and produces\\noutput for downstream operators. What we term a pipeline is the sequence of such\\nchained operators.\\n\\n[volcano]: https://paperhub.s3.amazonaws.com/dace52a42c07f7f8348b08dc2b186061.pdf\\n\\nWhile pipelines are not yet available at the query layer, they soon will be.\\nUntil then, you can deploy pipelines at load-time to [transform data in motion\\nor data at rest](/VAST%20v3.0/use/transform).\\n\\nFrom a user perspective, the configuration keys associated with transforms have\\nchanged. Here\'s the updated example from our previous [VAST v1.0 release\\nblog](/blog/vast-v1.0).\\n\\n```yaml\\nvast:\\n  # Specify and name our pipelines, each of which are a list of configured\\n  # pipeline operators. Pipeline operators are plugins, enabling users to \\n  # write complex transformations in native code using C++ and Apache Arrow.\\n  pipelines:\\n     # Prevent events with certain strings to be exported, e.g., \\n     # \\"tenzir\\" or \\"secret-username\\".\\n     remove-events-with-secrets:\\n       - select:\\n           expression: \':string !in [\\"tenzir\\", \\"secret-username\\"]\'\\n\\n  # Specify whether to trigger each pipeline at server- or client-side, on\\n  # `import` or `export`, and restrict them to a list of event types.\\n  pipeline-triggers:\\n    export:\\n      # Apply the remove-events-with-secrets transformation server-side on\\n      # export to the suricata.dns and suricata.http event types.\\n      - pipeline: remove-events-with-secrets\\n        location: server\\n        events:\\n          - suricata.dns\\n          - suricata.http\\n```\\n\\n## Summarization Improvements\\n\\nIn line with the above nomenclature changes, we\'ve improved the behavior of the\\n[`summarize`][summarize] operator. It is now possible to specify an explicit\\nname for the output fields. This is helpful when the downstream processing needs\\na predictable schema. Previously, VAST took simply the name of the input field.\\nThe syntax was as follows:\\n\\n```yaml\\nsummarize:\\n  group-by:\\n    - ...\\n  aggregate:\\n    min:\\n      - ts # implied name for aggregate field\\n```\\n\\nWe now switched the syntax such that the new field name is at the beginning:\\n\\n```yaml\\nsummarize:\\n  group-by:\\n    - ...\\n  aggregate:\\n    ts_min: # explicit name for aggregate field\\n      min: ts\\n```\\n\\nIn SQL, this would be the `AS` token: `SELECT min(ts) AS min_ts`.\\n\\n[summarize]: /VAST%20v3.0/understand/language/operators/summarize"},{"id":"/vast-v2.1","metadata":{"permalink":"/blog/vast-v2.1","source":"@site/blog/vast-v2.1/index.md","title":"VAST v2.1","description":"VAST v2.1 - Tune VAST Databases","date":"2022-07-07T00:00:00.000Z","formattedDate":"July 7, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":3.935,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.1","description":"VAST v2.1 - Tune VAST Databases","authors":"dominiklohmann","date":"2022-07-07T00:00:00.000Z","tags":["release","performance"]},"prevItem":{"title":"VAST v2.2","permalink":"/blog/vast-v2.2"},"nextItem":{"title":"Apache Arrow as Platform for Security Data Engineering","permalink":"/blog/apache-arrow-as-platform-for-security-data-engineering"}},"content":"[VAST v2.1][github-vast-release] is out! This release comes with a particular\\nfocus on performance and reducing the size of VAST databases. It brings a new\\nutility for optimizing databases in production, allowing existing deployments to\\ntake full advantage of the improvements after upgrading.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.1.0\\n\\n\x3c!--truncate--\x3e\\n\\n## New Project Site\\n\\nVAST has new project site: [vast.io](https://vast.io). We ported all\\ndocumentation from `https://docs.tenzir.com`, added a lot of new content, and\\nrestructured the reading experience along the user journey.\\n\\nYou can find the Threat Bus documentation in [Use VAST \u2192 Integrate \u2192 Threat\\nBus](/VAST%20v3.0/use/integrate/threatbus). Threat Bus is now officially in\\nmaintainance mode: we are only supporting existing features with bugfixes. That\\nsaid, Threat Bus will resurface in a new shape with its existing functionality\\nintegrated into VAST itself. Stay tuned.\\n\\n## Performance Improvements\\n\\nVAST now compresses data with [Zstd](http://www.zstd.net). The default\\nconfiguration achieves over 2x space savings. When transferring data between\\nclient and server processes, compression reduces the amount of transferred data\\nby up to 5x.\\n\\nAdditionally, VAST now compresses on-disk indexes with Zstd, resulting in a\\n50-80% size reduction depending on the type of indexes used.\\n\\nThis allowed us to increase the default partition size from 1,048,576 to\\n4,194,304 events[^1], and the default number of events in a single batch from 1,024\\nto 65,536, resulting in a massive performance increase at the cost of a ~20%\\nlarger memory footprint at peak loads. Use the option `vast.max-partition-size`\\nto tune this space-time tradeoff.\\n\\nTo benchmark this, we used [`speeve`][speeve] to generate 20 EVE JSON files\\ncontaining 8,388,608 events each[^2]. We spawned a VAST server process and ran\\n20 VAST client processes in parallel, with one process per file.\\n\\nWe observed a reduction of **up to 73%** of disk space utilization:\\n\\n![Database Size](storage-light.png#gh-light-mode-only)\\n![Database Size](storage-dark.png#gh-dark-mode-only)\\n\\nIn addition, we were able to scale the ingest rate by almost **6x** due to the\\nhigher batch size and the reduced memory usage per batch:\\n\\n![Ingest Rate](rate-light.png#gh-light-mode-only)\\n![Ingest Rate](rate-dark.png#gh-dark-mode-only)\\n\\nThe table below summaries the benchmarks:\\n\\n||VAST v2.0|VAST v2.1|Change|\\n|-:|:-|:-|:-|\\n|Ingest Duration|1,650 s|242 s|-85.3%|\\n|Ingest Rate|101,680 events/s|693,273 events/s|+581.8%|\\n|Index Size|14,791 MiB|5,721 MiB|-61.3%|\\n|Store Size|37,656 MiB|8,491 MiB|-77.5%|\\n|Database Size|52,446 MiB|14,212 MiB|-72.9%|\\n\\n:::note Compressed Filesystems\\nThe above benchmarks ran on filesystems without compression. We expect the gain\\nfrom compression to be smaller when using compressed filesystems like\\n[`btrfs`][btrfs].\\n:::\\n\\n[speeve]: https://github.com/satta/speeve\\n[btrfs]: https://btrfs.wiki.kernel.org/index.php/Main_Page\\n\\n[^1]: VAST v2.0 failed to write its partitions to disk with the defaults for\\n  v2.1 because the on-disk size exceeded the maximum possible size of a\\n  FlatBuffers table, which VAST internally uses to have an open standard for its\\n  persistent state.\\n[^2]: This resulted in 167,772,160 events, with a total of 200\'917\'930 unique\\n  values with a schema distribution of 80.74% `suricata.flow`, 7.85%\\n  `suricata.dns`, 5.35% `suricata.http`, 4.57% `suricata.fileinfo`, 1.04%\\n  `suricata.tls`, 0.41% `suricata.ftp`, and 0.04% `suricata.smtp`.\\n\\n## Rebuild VAST Databases\\n\\nThe new changes to VAST\'s internal data format only apply to newly ingested\\ndata. To retrofit changes, we introduce a new `rebuild` command with this\\nrelease. A rebuild effectively re-ingests events from existing partitions and\\natomically replaces them with partitions of the new format.\\n\\nThis makes it possible to upgrade persistent state to a newer version, or\\nrecreate persistent state after changing configuration parameters, e.g.,\\nswitching from the Feather to the Parquet store backend (that will land in\\nv2.2). Rebuilding partitions also recreates their sparse indexes that\\naccellerate query execution. The process takes place asynchronously in the\\nbackground.\\n\\nWe recommend running `vast rebuild` to upgrade your VAST v1.x partitions to VAST\\nv2.x partitions to take advantage of the new compression and an improved\\ninternal representation.\\n\\nThis is how you run it:\\n\\n```bash\\nvast rebuild [--all] [--undersized] [--parallel=<number>] [<expression>]\\n```\\n\\nA rebuild is not only useful when upgrading outdated partitions, but also when\\nchanging parameters of up-to-date partitions. Use the `--all` flag to extend a\\nrebuild operation to _all_ partitions. (Internally, VAST versions the partition\\nstate via FlatBuffers. An outdated partition is one whose version number is not\\nthe newest.)\\n\\nThe `--undersized` flag causes VAST to only rebuild partitions that are under\\nthe configured partition size limit `vast.max-partition-size`.\\n\\nThe `--parallel` options is a performance tuning knob. The parallelism level\\ncontrols how many sets of partitions to rebuild in parallel. This value defaults\\nto 1 to limit the CPU and memory requirements of the rebuilding process, which\\ngrow linearly with the selected parallelism level.\\n\\nAn optional expression allows for restricting the set of partitions to rebuild.\\nVAST performs a catalog lookup with the expression to identify the set of\\ncandidate partitions. This process may yield false positives, as with regular\\nqueries, which may cause unaffected partitions to undergo a rebuild. For\\nexample, to rebuild outdated partitions containing `suricata.flow` events\\nolder than 2 weeks, run the following command:\\n\\n```bash\\nvast rebuild \'#type == \\"suricata.flow\\" && #import_time < 2 weeks ago\'\\n```"},{"id":"/apache-arrow-as-platform-for-security-data-engineering","metadata":{"permalink":"/blog/apache-arrow-as-platform-for-security-data-engineering","source":"@site/blog/apache-arrow-as-platform-for-security-data-engineering/index.md","title":"Apache Arrow as Platform for Security Data Engineering","description":"How VAST leverages Apache Arrow for Security Data Engineering","date":"2022-06-17T00:00:00.000Z","formattedDate":"June 17, 2022","tags":[{"label":"architecture","permalink":"/blog/tags/architecture"},{"label":"arrow","permalink":"/blog/tags/arrow"},{"label":"performance","permalink":"/blog/tags/performance"},{"label":"query","permalink":"/blog/tags/query"}],"readingTime":6.01,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Co-Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"description":"How VAST leverages Apache Arrow for Security Data Engineering","authors":"mavam","date":"2022-06-17T00:00:00.000Z","tags":["architecture","arrow","performance","query"]},"prevItem":{"title":"VAST v2.1","permalink":"/blog/vast-v2.1"},"nextItem":{"title":"VAST v2.0","permalink":"/blog/vast-v2.0"}},"content":"VAST bets on [Apache Arrow][arrow] as the open interface to structured data. By\\n\\"bet,\\" we mean that VAST does not work without Arrow. And we are not alone.\\nInflux\'s [IOx][iox], DataDog\'s [Husky][husky], Anyscale\'s [Ray][ray],\\n[TensorBase][tensorbase], and [others][arrow-projects] committed themselves to\\nmaking Arrow a corner stone of their system architecture. For us, Arrow was not\\nalways a required dependency. We shifted to a tighter integration over the years\\nas the Arrow ecosystem matured. In this blog post we explain our journey of\\nbecoming an Arrow-native engine.\\n\\n[arrow]: https://arrow.apache.org\\n[iox]: https://github.com/influxdata/influxdb_iox\\n[husky]: https://www.datadoghq.com/blog/engineering/introducing-husky/\\n[ray]: https://github.com/ray-project/ray\\n[tensorbase]: https://github.com/tensorbase/tensorbase\\n[arrow-projects]: https://arrow.apache.org/powered_by/\\n\\n\x3c!--truncate--\x3e\\n\\nToday, the need to bring advanced security analytics and data engineering\\ntogether is stronger than ever, but there is a huge gap between the two fields.\\nWe see Arrow as the vehicle to close this gap, allowing us developers to\\npractice *security data engineering* to make security analytics easy for users.\\nThat is, the experience should allow experts to interact with the data in the\\nsecurity domain, end-to-end without context switching. To achieve this, we began\\nour journey with VAST by developing a data model for structured security\\ntelemetry. Having worked for a decade with the [Zeek][zeek] (fka. Bro) network\\nsecurity monitor, we understood the value of having first-class support for\\ndomain-specific entities (e.g., native representation of IPv4 and IPv6\\naddresses) and type-specific operations (e.g., the ability to perform top-k\\nprefix search to answer subnet membership queries). In addition, the ability to\\nembed domain semantics with user-defined types (e.g., IP addresses, subnets, and\\nURLs) was central to expressing complex relationships to develop effective\\nanalytical models. It was clear that we needed the domain model deep in the core\\nof the system to successfully support security analytics.\\n\\nAfter having identified the data model requirements, the question of\\nrepresentation came next. At first, we unified the internal representation with\\na row-oriented representation using [MsgPack][msgpack], which comes with a\\nmechanism for adding custom types. The assumption was that a row-based data\\nrepresentation more closely matches typical event data (e.g., JSONL) and\\ntherefore allows for much higher processing rates. Moreover, early use cases of\\nVAST were limited to interactive, multi-dimensional search to extract a subset\\nof *entire* records, spread over a longitudinal archive of data. The\\nrow-oriented encoding worked well for this.\\n\\nBut as security operations were maturing, requirements extended to analytical\\nprocessing of structured data, making a columnar format increasingly beneficial.\\nAfter having witnessed first-hand the early commitment of [Ray][ray] to Arrow,\\nwe started using Arrow as optional dependency as additional column-oriented\\nencoding. We abstracted a batch of data encoding-independent behind a \\"table\\nslice\\":\\n\\n![MsgPack & Arrow](msgpack-arrow.excalidraw.svg)\\n\\nHiding the concrete encoding behind a cell-based access interface worked for\\nlow-volume use cases, but backfired as we scaled up and slowed us down\\nsubstantially in development. We needed to make a choice. This is where timing\\nwas right: our perception of the rapidly evolving Arrow ecosystem changed.\\nArrow-based runtimes were mushrooming all over the place. Nowadays it requires\\nonly a few lines of code to integrate Arrow data into the central logic of\\napplications. We realized that the primary value proposition of Arrow is to\\n*make data interoperability easy*.\\n\\nBut data interoperability is only a sufficient condition for enabling\\nsustainable security analytics. The differentiating value of a *security* data\\nplatform is support for the *security* domain. This is where Arrow\'s [extension\\ntypes][extension-types] come into play. They add *semantics* to otherwise\\ngeneric types, e.g., by telling the user \\"this is a transport-layer port\\" and\\nnot just a 16-bit unsigned integer, or \\"this is a connection 4-tuple to\\nrepresent a network flow\\" instead of \\"this is a record with 4 fields of type\\nstring and unsigned integer\\". Extension types are composable and allow for\\ncreating a rich typing layer with meaningful domain objects on top of a\\nstandardized data representation. Since they are embedded in the data, they do\\nnot have to be made available out-of-band when crossing the boundaries of\\ndifferent tools. Now we have self-describing security data.\\n\\nInteroperability plus support for a domain-specific data model makes Arrow a\\nsolid *data plane*. It turns out that Arrow is much more than a standardized\\ndata representation. Arrow also comes with bag of tools for working with the\\nstandardized data. In the diagram below, we show the various Arrow pieces that\\npower the architecture of VAST:\\n\\n![Arrow Data Plane](arrow-data-plane.excalidraw.svg)\\n\\nIn the center we have the Arrow data plane that powers other parts of the\\nsystem. Green elements highlight Arrow building blocks that we use today, and\\norange pieces elements we plan to use in the future. There are several aspects\\nworth pointing out:\\n\\n1. **Unified Data Plane**: When users ingest data into VAST, the\\n   parsing process converts the native data into Arrow. Similarly, a\\n   conversation boundary exists when data leaves the system, e.g., when a user\\n   wants a query result shown in JSON, CSV, or some custom format. Source and\\n   sink data formats are [exchangeable\\n   plugins](/VAST%20v3.0/understand/architecture/plugins).\\n\\n2. **Read/Write Path Separation**: one design goal of VAST is a strict\\n   separation of read and write path, in order to scale them independently. The\\n   write path follows a horizontally scalable architecture where builders (one per\\n   schema) turn the in-memory record batches into a persistent representation.\\n   VAST currently has support for Parquet and Feather.\\n\\n3. **Pluggable Query Engine**: VAST has live/continuous queries that simply run\\n   over the stream of incoming data, and historical queries that operate on\\n   persistent data. The harboring execution engine is something we are about to\\n   make pluggable. The reason is that VAST runs in extremely different\\n   environments, from cluster to edge. Query engines are usually optimized for a\\n   specific use case, so why not use the best engine for the job at hand? Arrow\\n   makes this possible. [DuckDB][duckdb] and [DataFusion][datafusion] are great\\n   example of embeddable query engines.\\n\\n4. **Unified Control Plane**: to realize a pluggable query engine, we also need\\n   a standardized control plane. This is where [Substrait][substrait] and\\n   [Flight][flight] come into play. Flight for communication and Substrait as\\n   canonical query representation. We already experimented with Substrait,\\n   converting VAST queries into a logical query plan. In fact, VAST has a \\"query\\n   language\\" plugin to make it possible to translate security content. (For\\n   example, our [Sigma plugin][sigma-plugin] translates [Sigma rules][sigma]\\n   into VAST queries.) In short: Substrait is to the control plane what Arrow is\\n   to the data plane. Both are needed to modularize the concept of a query\\n   engine.\\n\\nMaking our own query engine more suitable for analytical workloads has\\nreceived less attention in the past, as we prioritized high-performance data\\nacquisition, low-latency search, in-stream matching using [Compute][compute],\\nand expressiveness of the underlying domain data model. We did so because VAST\\nmust run robustly in production on numerous appliances all over the world in a\\nsecurity service provider setting, with confined processing and storage where\\nefficiency is key.\\n\\nMoving forward, we are excited to bring more analytical horse power to the\\nsystem, while opening up the arena for third-party engines. With the bag of\\ntools from the Arrow ecosystem, plus all other embeddable Arrow engines that are\\nemerging, we have a modular architecture to can cover a very wide spectrum of\\nuse cases.\\n\\n[compute]: https://arrow.apache.org/VAST%20v3.0/cpp/compute.html\\n[extension-types]: https://arrow.apache.org/VAST%20v3.0/format/Columnar.html#extension-types\\n[flight]: https://arrow.apache.org/VAST%20v3.0/format/Flight.html\\n[substrait]: https://substrait.io/\\n[datafusion]: https://arrow.apache.org/datafusion/\\n[msgpack]: https://msgpack.org/index.html\\n[duckdb]: https://duckdb.org/\\n[sigma]: https://github.com/SigmaHQ/sigma\\n[sigma-plugin]: /VAST%20v3.0/understand/language/frontends/sigma\\n[zeek]: https://zeek.org"},{"id":"/vast-v2.0","metadata":{"permalink":"/blog/vast-v2.0","source":"@site/blog/vast-v2.0/index.md","title":"VAST v2.0","description":"VAST v2.0 - Smarter Query Scheduling & Tunable Filters","date":"2022-05-16T00:00:00.000Z","formattedDate":"May 16, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"compaction","permalink":"/blog/tags/compaction"},{"label":"performance","permalink":"/blog/tags/performance"},{"label":"pcap","permalink":"/blog/tags/pcap"}],"readingTime":6.335,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.0","description":"VAST v2.0 - Smarter Query Scheduling & Tunable Filters","authors":"dominiklohmann","date":"2022-05-16T00:00:00.000Z","tags":["release","compaction","performance","pcap"]},"prevItem":{"title":"Apache Arrow as Platform for Security Data Engineering","permalink":"/blog/apache-arrow-as-platform-for-security-data-engineering"},"nextItem":{"title":"VAST v1.1.2","permalink":"/blog/vast-v1.1.2"}},"content":"Dear community, we are excited to announce [VAST v2.0][github-vast-release],\\nbringing faster execution of bulk-submitted queries, improved tunability of\\nindex structures, and new configurability through environment variables.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.0.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Query Scheduling\\n\\nVAST is now more intelligent in how it schedules queries.\\n\\nWhen a query arrives at the VAST server, VAST first goes to the catalog which\\nreturns a set of on-disk candidate partitions that the query may be applicable\\nto. Previous versions of VAST simply iterated through the available queries as\\nthey came in, loading partition by partition to extract events. Due to memory\\nconstraints, VAST is only able to keep some partitions in memory, which causes\\nfrequent loading and unloading of the same partitions for queries that access\\nthe same data. Now, VAST loads partitions depending on how many queries they are\\nrelevant for and evaluates all ongoing queries for one partition at a time.\\n\\nAdditionally, VAST now partitions the data for each schema separately, moving\\naway from partitions that contain events of multiple schemas. This helps with\\ncommon access patterns and speeds up queries restricted to a single schema.\\n\\nThe numbers speak for themselves:\\n\\n![Benchmarks](scheduler-light.png#gh-light-mode-only)\\n![Benchmarks](scheduler-dark.png#gh-dark-mode-only)\\n\\n## Updates to Aging, Compaction, and the Disk Monitor\\n\\nVAST v1.0 deprecated the experimental aging feature. Given popular demand we\'ve\\ndecided to un-deprecate it and to actually implement it on top of the same\\nbuilding blocks the new compaction mechanism uses, which means that it is now\\nfully working and no longer considered experimental.\\n\\nThe compaction plugin is now able to apply general time-based compactions that\\nare not restricted to a specific set of types. This makes it possible for\\noperators to implement rules like \\"delete all data after 1 week\\", without having\\nto list all possible data types that may occur.\\n\\nSome smaller interface changes improve the observability of the compactor for\\noperators: The  `vast compaction status` command prints the current compaction\\nstatus, and the `vast compaction list` command now lists all configured\\ncompaction rules of the VAST node.\\n\\nAdditionally, we\'ve improved overall stability and fault tolerance improvements\\nsurrounding the disk monitor and compaction features.\\n\\n## Fine-tuned Catalog Configuration\\n\\n:::note Advanced Users\\nThis section is for advanced users only.\\n:::\\n\\nThe catalog manages partition metadata and is responsible for deciding whether a\\npartition qualifies for a certain query. It does so by maintaining sketch data\\nstructures (e.g., Bloom filters, summary statistics) for each partition.\\nSketches are highly space-efficient at the cost of being probabilistic and\\nyielding false positives.\\n\\nDue to this characteristic, sketches can grow sublinear: doubling the number of\\nevents in a sketch does not lead to a doubling of the memory requirement.\\nBecause the catalog must be traversed in full for a given query it needs to be\\nmaintained in active memory to provide high responsiveness.\\n\\nA false positive can have substantial impact on the query latency by\\nmaterializing irrelevant partitions, which involves unnecessary I/O. Based on\\nthe cost of I/O, this penalty may be substantial. Conversely, reducing the false\\npositive rate increases the memory consumption, leading to a higher resident set\\nsize and larger RAM requirements.\\n\\nYou can control this space-time trade-off in the configuration section\\n`vast.index` by specifying index rules. Each rule corresponds to one sketch and\\nconsists of the following components:\\n\\n`targets`: a list of extractors to describe the set of fields whose values to\\nadd to the sketch. `fp-rate`: an optional value to control the false-positive\\nrate of the sketch.\\n\\nVAST does not create field-level sketches unless a dedicated rule with a\\nmatching target configuration exists. Here\'s an example:\\n\\n```yaml\\nvast:\\n  index:\\n    rules:\\n      - targets:\\n          # field synopses: need to specify fully qualified field name\\n          - suricata.http.http.url\\n        fp-rate: 0.005\\n      - targets:\\n          - :ip\\n        fp-rate: 0.1\\n```\\n\\nThis configuration includes two rules (= two sketches), where the first rule\\nincludes a field extractor and the second a type extractor. The first rule\\napplies to a single field, `suricata.http.http.url`, and has a false-positive\\nrate of 0.5%. The second rule creates one sketch for all fields of type `addr`\\nthat has a false-positive rate of 10%.\\n\\n## Configuring VAST with Environment Variables\\n\\nVAST now offers an additional configuration path besides editing YAML\\nconfiguration files and providing command line arguments: *setting environment\\nvariables*. This enables a convenient configuration experience when using\\ncontainer runtimes, such as Docker, where the other two configuration paths have\\na mediocre UX at best:\\n\\nThe container entry point is limited to adding command line arguments, where not\\nall options may be set. For Docker Compose and Kubernetes, it is often not\\ntrivially possible to even add command line arguments.\\n\\nProviding a manual configuration file is a heavy-weight action, because it\\nrequires (1) generating a potentially templated configuration file, and (2)\\nmounting that file into a location where VAST would read it.\\n\\nAn environment variable has the form `KEY=VALUE`. VAST processes only\\nenvironment variables having the form `VAST_{KEY}=VALUE`. For example,\\n`VAST_ENDPOINT=1.2.3.4` translates to the command line option\\n`--endpoint=1.2.3.4` and YAML configuration `vast.endpoint: 1.2.3.4`.\\n\\nRegarding precedence, environment variables override configuration file\\nsettings, and command line arguments override environment variables. Please\\nconsult the documentation for a more detailed explanation of how to specify keys\\nand values.\\n\\n## VLAN Tag Extraction and Better Packet Decapsulation\\n\\nVAST now extracts [802.1Q VLAN tags](https://en.wikipedia.org/wiki/IEEE_802.1Q)\\nfrom packets, making it possible to filter packets based on VLAN ID. The packet\\nschema includes a new nested record `vlan` with two fields: `outer` and `inner`\\nto represent the respective VLAN ID. For example, you can generate PCAP traces\\nof packets based on VLAN IDs as follows:\\n\\n```bash\\nvast export pcap \'vlan.outer > 0 || vlan.inner in [1, 2, 3]\' | tcpdump -r - -nl\\n```\\n\\nVLAN tags occur in many variations, and VAST extracts them in case of\\nsingle-tagging and  [QinQ\\ndouble-tagging](https://en.wikipedia.org/wiki/IEEE_802.1ad). Consult the [PCAP\\ndocumentation](/VAST%20v3.0/understand/formats/pcap) for details on this feature.\\n\\nInternally, the packet decapsulation logic has been rewritten to follow a\\nlayered approach: frames, packets, and segments are the building blocks. The\\nplan is to reuse this architecture when switching to kernel-bypass packet\\nacquisition using DPDK. If you would like to see more work on the front of\\nhigh-performance packet recording, please reach out.\\n\\n## Breaking Changes\\n\\nThe `--verbosity` command-line option is now called `--console-verbosity`. The\\nshorthand options `-v`, `-vv`, `-vvv`, `-q`, `-qq`, and  `-qqq`  are unchanged.\\nThis aligns the command-line option with the configuration option\\n`vast.console-verbosity`, and disambiguates from the `vast.file-verbosity`\\noption.\\n\\nThe _Meta Index_ is now called the _Catalog_. This affects multiple status and\\nmetrics keys. We plan to extend the functionality of the Catalog in a future\\nrelease, turning it into a more powerful first instance for lookups.\\n\\nTransform steps that add or modify columns now add or modify the columns\\nin-place rather than at the end, preserving the nesting structure of the\\noriginal data.\\n\\n## Changes for Developers\\n\\nThe `vast get` command no longer exists. The command allowed for retrieving\\nevents by their internal unique ID, which we are looking to remove entirely in\\nthe future.\\n\\nChanges to the internal data representation of VAST require all transform step\\nplugins to be updated. The output format of the vast export arrow command\\nchanged for the address, subnet, pattern, and enumeration types, which are now\\nmodeled as [Arrow Extension\\nTypes](https://arrow.apache.org/docs/format/Columnar.html#extension-types). The\\nrecord type is no longer flattened. The mapping of VAST types to Apache Arrow\\ndata types  is now considered stable.\\n\\n## Smaller Things\\n\\n- VAST client commands now start much faster and use less memory.\\n- The `vast count --estimate \'<query>\'` feature no longer unnecessarily causes\\n  stores to load from disk, resulting in major speedups for larger databases and\\n  broad queries.\\n- The [tenzir/vast](https://github.com/tenzir/vast) repository now contains\\n  experimental Terraform scripts for deploying VAST to AWS Fargate and Lambda."},{"id":"/vast-v1.1.2","metadata":{"permalink":"/blog/vast-v1.1.2","source":"@site/blog/vast-v1.1.2/index.md","title":"VAST v1.1.2","description":"VAST v1.1.2 - Compaction & Query Language Frontends","date":"2022-03-29T00:00:00.000Z","formattedDate":"March 29, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"compaction","permalink":"/blog/tags/compaction"},{"label":"query","permalink":"/blog/tags/query"}],"readingTime":0.33,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"}],"frontMatter":{"title":"VAST v1.1.2","description":"VAST v1.1.2 - Compaction & Query Language Frontends","authors":"lava","date":"2022-03-29T00:00:00.000Z","tags":["release","compaction","query"]},"prevItem":{"title":"VAST v2.0","permalink":"/blog/vast-v2.0"},"nextItem":{"title":"VAST v1.1.1","permalink":"/blog/vast-v1.1.1"}},"content":"Dear community, we are happy to announce the release of [VAST\\nv1.1.2](https://github.com/tenzir/vast/releases/tag/v1.1.2), the latest release\\non the VAST v1.1 series. This release contains a fix for a race condition that\\ncould lead to VAST eventually becoming unresponsive to queries in large\\ndeployments.\\n\\n\x3c!--truncate--\x3e\\n\\nFixed a race condition that would cause queries to become stuck when an exporter\\nwould time out during the meta index lookup.\\n[#2165](https://github.com/tenzir/vast/pull/2165)"},{"id":"/vast-v1.1.1","metadata":{"permalink":"/blog/vast-v1.1.1","source":"@site/blog/vast-v1.1.1/index.md","title":"VAST v1.1.1","description":"VAST v1.1.1 - Compaction & Query Language Frontends","date":"2022-03-25T00:00:00.000Z","formattedDate":"March 25, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"compaction","permalink":"/blog/tags/compaction"},{"label":"query","permalink":"/blog/tags/query"}],"readingTime":0.635,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v1.1.1","description":"VAST v1.1.1 - Compaction & Query Language Frontends","authors":"dominiklohmann","date":"2022-03-25T00:00:00.000Z","tags":["release","compaction","query"]},"prevItem":{"title":"VAST v1.1.2","permalink":"/blog/vast-v1.1.2"},"nextItem":{"title":"VAST v1.1","permalink":"/blog/vast-v1.1"}},"content":"Dear community, we are excited to announce [VAST\\nv1.1.1][github-vast-release-new].\\n\\nThis release contains some important bug fixes on top of everything included in\\nthe [VAST v1.1][github-vast-release-old] release.\\n\\n[github-vast-release-new]: https://github.com/tenzir/vast/releases/tag/v1.1.1\\n[github-vast-release-old]: https://github.com/tenzir/vast/releases/tag/v1.1.0\\n\\n\x3c!--truncate--\x3e\\n\\n- The disk monitor now correctly continues deleting until below the low water\\n  mark after a partition failed to delete.\\n- We fixed a rarely occurring race condition that caused query workers to become\\n  stuck after delivering all results until the corresponding client process\\n  terminated.\\n- Queries that timed out or were externally terminated while in the query\\n  backlog that had more unhandled candidate than taste partitions no longer\\n  permanently get stuck. This critical bug caused VAST to idle permanently on\\n  the export path once all workers were stuck.\\n\\nThanks to [@norg](https://github.com/norg) for reporting the issues."},{"id":"/vast-v1.1","metadata":{"permalink":"/blog/vast-v1.1","source":"@site/blog/vast-v1.1/index.md","title":"VAST v1.1","description":"VAST v1.1 - Compaction & Query Language Frontends","date":"2022-03-03T00:00:00.000Z","formattedDate":"March 3, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"compaction","permalink":"/blog/tags/compaction"},{"label":"query","permalink":"/blog/tags/query"}],"readingTime":5.985,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v1.1","description":"VAST v1.1 - Compaction & Query Language Frontends","authors":"dominiklohmann","date":"2022-03-03T00:00:00.000Z","last_updated":"2022-07-15T00:00:00.000Z","tags":["release","compaction","query"]},"prevItem":{"title":"VAST v1.1.1","permalink":"/blog/vast-v1.1.1"},"nextItem":{"title":"VAST v1.0","permalink":"/blog/vast-v1.0"}},"content":"Dear community, we are excited to announce [VAST v1.1][github-vast-release],\\nwhich ships with exciting new features: *query language plugins* to exchange the\\nquery expression frontend, and *compaction* as a mechanism for expressing\\nfine-grained data retention policies and gradually aging out data instead of\\nsimply deleting it.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v1.1.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Query Language Plugins\\n\\nVAST features [a new query language plugin\\ntype](https://vast.io/VAST%20v3.0/understand/architecture/plugins#language)\\nthat makes it possible to exchange the querying frontend, that is, replace the\\nlanguage in which the user writes queries. This makes it easier to integrate\\nVAST into specific domains without compromising the policy-neutral system core.\\n\\nThe first instance of the query language plugin is the [`sigma`\\nplugin](https://github.com/tenzir/vast/tree/master/plugins/sigma), which make it\\npossible to pass [Sigma\\nrules](https://vast.io/VAST%20v3.0/understand/language/frontends/sigma) as\\ninput instead of a standard VAST query expression. Prior to this plugin, VAST\\nattempted to parse a query as Sigma rule first, and if that failed, tried to\\nparse it as a VAST expression. The behavior changed in that VAST now always\\ntries to interpret user input as VAST expression, and if that fails, goes\\nthrough all other loaded query language plugins.\\n\\nMoving forward, we will make it easier for integrators to BYO query language and\\nleverage VAST as an execution engine. We have already\\n[experimented](https://github.com/tenzir/vast/pull/2075) with\\n[Substrait](https://substrait.io), a cross-language protobuf spec for query\\nplans. The vision is that users can easily connect *any* query language that\\ncompiles into Substrait, and VAST takes the query plan as binary substrait blob.\\nSubstrait is still a very young project, but if the Arrow integration starts to\\nmature, it has the potential to enable very powerful types of queries without\\nmuch heavy lifting on our end. We already use the Arrow Compute API to implement\\ngeneric grouping and aggregation during compaction, which allows us to avoid\\nhand-roll and optimize compute kernels for standard functions.\\n\\n## Compaction Plugin\\n\\nCompaction is a feature to perform fine-grained transformation of historical\\ndata to manage a fixed storage budget. This gives operators full control over\\nshrinking data gradually\u2014both from a temporal and spatial angle:\\n\\n**Spatial**: Traditionally, reaching a storage budget triggers deletion of the\\noldest (or least-recently-used) data. This is a binary decision to throw away a\\nsubset of events. It does not differentiate the utility of data within an event.\\nWhat if you could only throw away the irrelevant parts and keep the information\\nthat might still be useful for longitudinal investigations? What if you could\\naggregate multiple events into a single one that captures valuable information?\\nImagine, for example, halving the space utilization of events with network flow\\ninformation and keeping them 6 months longer; or imagine you could roll up a set\\nof flows into a traffic matrix that only captures who communicated with whom in\\na given timeframe.\\n\\nBy incrementally elevating data into more space-efficient representations,\\ncompaction gives you a much more powerful mechanism to achieve long retention\\nperiods while working with high-volume telemetry.\\n\\n**Temporal**: data residency regulations often come with compliance policies\\nwith maximum retention periods, e.g., data containing personal data. For\\nexample, a policy may dictate a maximum retention of 1 week for events\\ncontaining URIs and 3 months for events containing IP addresses related to\\nnetwork connections. However, these retention windows could be broadened when\\npseudonomyzing or anonymizing the relevant fields.\\n\\nCompaction has a policy-based approach to specify these temporal constraints in\\na clear, declarative fashion.\\n\\nCompaction supersedes both the disk monitor and aging, being able to cover the\\nentire functionality of their behaviors in a more configurable way. The disk\\nmonitor remains unchanged and the experimental aging feature is deprecated (see\\nbelow).\\n\\n## Updates to Transform Steps\\n\\n### Aggregate Step\\n\\n:::info Transforms \u2192 Pipelines\\nIn [VAST v2.2](/blog/vast-v2.2), we renamed *transforms* to *pipelines*, and\\n*transform steps* to *pipeline operators*. This caused several configuration key\\nchanges. Additionally, we renamed the `aggregate` operator to\\n[`summarize`][summarize]. Please keep this in mind when reading the example\\nbelow and consult the\\n[documentation](/VAST%20v3.0/understand/language/pipelines) for the\\nup-to-date syntax.\\n[summarize]: /VAST%20v3.0/understand/language/operators/summarize\\n:::\\n\\nThe new `aggregate` transform step plugin allows for reducing data with an\\naggregation operation over a group of columns.\\n\\nAggregation is a two-step process of first bucketing data in groups of values,\\nand then executing an aggregation function that computes a single value over the\\nbucket. The functionality is in line with what standard execution engines offer\\nvia \\"group-by\\" and \\"aggregate\\".\\n\\nBased on how the transformation is invoked in VAST, the boundary for determining\\nwhat goes into a grouping can be a table slice (e.g., during import/export) or\\nan entire partition (during compaction).\\n\\nHow this works is best shown on example data. Consider the following events\\nrepresenting flow data that contain a source IP address, a start and end\\ntimestamp, the number of bytes per flow, a boolean flag whether there is an\\nassociated alert, and a unique identifier.\\n\\n```json\\n{\\"source_ip\\": \\"10.0.0.1\\", \\"num_bytes\\": 87122, \\"start\\": \\"2022-02-22T10:36:40\\", \\"end\\": \\"2022-02-22T10:36:47\\", \\"alerted\\": false, \\"unique_id\\": 1}\\n{\\"source_ip\\": \\"10.0.0.2\\", \\"num_bytes\\": 62335, \\"start\\": \\"2022-02-22T10:36:43\\", \\"end\\": \\"2022-02-22T10:36:48\\", \\"alerted\\": false, \\"unique_id\\": 2}\\n{\\"source_ip\\": \\"10.0.0.1\\", \\"num_bytes\\": 640, \\"start\\": \\"2022-02-22T10:36:46\\", \\"end\\": \\"2022-02-22T10:36:47\\", \\"alerted\\": true, \\"unique_id\\": 3}\\n{\\"source_ip\\": \\"10.0.0.1\\", \\"num_bytes\\": 2162, \\"start\\": \\"2022-02-22T10:36:49\\", \\"end\\": \\"2022-02-22T10:36:51\\", \\"alerted\\": false, \\"unique_id\\": 4}\\n```\\n\\nWe can now configure a transformation that groups the events by their source IP\\naddress, takes the sum of the number of bytes, the minimum of the start\\ntimestamp, the maximum of the end timestamp, and the disjunction of the alerted\\nflag. Since the unique identifier cannot be aggregated in a meaningful manner,\\nit  is discarded.\\n\\n```yaml\\nvast:\\n  transforms:\\n    example-aggregation:\\n      - aggregate:\\n          group-by:\\n            - source_ip\\n          sum:\\n            - num_bytes\\n          min:\\n            - start\\n          max:\\n            - end\\n          any:\\n            - alerted\\n```\\n\\nAfter applying the transform, the resulting events will look like this:\\n\\n```json\\n{\\"source_ip\\": \\"10.0.0.1\\", \\"num_bytes\\": 89924, \\"start\\": \\"2022-02-22T10:36:40\\", \\"end\\": \\"2022-02-02T10:36:51\\", \\"alerted\\": true}\\n{\\"source_ip\\": \\"10.0.0.2\\", \\"num_bytes\\": 62335, \\"start\\": \\"2020-11-06T10:36:43\\", \\"end\\": \\"2020-02-22T10:36:48\\", \\"alerted\\": false}\\n```\\n\\nUnlike the built-in transform steps, `aggregate` is a separate open-source\\nplugin that needs to be manually enabled in your `vast.yaml` configuration to be\\nusable:\\n\\n```yaml\\nvast:\\n  plugins:\\n    - aggregate\\n```\\n\\n### Rename Step\\n\\nThe new `rename` transform step is a built-in that allows for changing the name\\nof the schema of data. This is particularly useful when a transformation changes\\nthe shape of the data. E.g., an aggregated `suricata.flow` should likely be\\nrenamed because it is of a different layout.\\n\\nThis is how you configure the transform step:\\n\\n```yaml\\nrename:\\n  layout-names:\\n    - from: suricata.flow\\n      to: suricata.aggregated_flow\\n```\\n\\n### Project and Select Steps\\n\\nThe built-in `project` and `select` transform steps now drop table slices where\\nno columns and rows match the configuration respectively instead of leaving the\\ndata untouched.\\n\\n## Deprecations\\n\\nThe `msgpack` encoding no longer exists. As we integrate deeper with Apache\\nArrow, the `arrow` encoding is now the only option. Configuration options for\\n`msgpack` will be removed in an upcoming major release. On startup, VAST now\\nwarns if any of the deprecated options are in use.\\n\\nVAST\u2019s *aging* feature never made it out of the experimental stage: it only\\nerased data without updating the index correctly, leading to unnecessary lookups\\ndue to overly large candidate sets and miscounts in the statistics. Because\\ntime-based compaction is a superset of the aging functionality (that also\\nupdates the index correctly), we will remove aging in a future release. VAST now\\nwarns on startup if it\u2019s configured to run aging."},{"id":"/vast-v1.0","metadata":{"permalink":"/blog/vast-v1.0","source":"@site/blog/vast-v1.0/index.md","title":"VAST v1.0","description":"VAST v1.0 \u2013 New Year, New Versioning Scheme","date":"2022-01-27T00:00:00.000Z","formattedDate":"January 27, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"transforms","permalink":"/blog/tags/transforms"},{"label":"query","permalink":"/blog/tags/query"}],"readingTime":3.195,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v1.0","description":"VAST v1.0 \u2013 New Year, New Versioning Scheme","authors":"dominiklohmann","date":"2022-01-27T00:00:00.000Z","last_updated":"2022-07-15T00:00:00.000Z","tags":["release","transforms","query"]},"prevItem":{"title":"VAST v1.1","permalink":"/blog/vast-v1.1"}},"content":"We are happy to announce [VAST v1.0][github-vast-release]!\\n\\nThis release brings a new approach to software versioning for Tenzir. We laid\\nout the semantics in detail in a new [VERSIONING][github-versioning-md]\\ndocument.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v1.0.0\\n[github-versioning-md]: https://github.com/tenzir/vast/blob/v1.0.0/VERSIONING.md\\n\\n\x3c!--truncate--\x3e\\n\\n## Query events based on their import time\\n\\nThe new [`#import_time` extractor][docs-meta-extractor] allows for exporting\\nevents based on the time they arrived at VAST. Most of the time, this timestamp\\nis not far away from the timestamp of when the event occurred, but in certain\\ncases the two may deviate substantially, e.g., when ingesting historical events\\nfrom several years ago.\\n\\nFor example, to export all Suricata alerts that arrived at VAST on New Years Eve\\nas JSON, run this command:\\n\\n```bash\\nvast export json \'#type == \\"suricata.alert\\" && #import_time >= 2021-12-31 && #import_time < 2022-01-01\'\\n```\\n\\nThis differs from the [`:timestamp` type extractor][docs-type-extractor] that\\nqueries all events that contain a type `timestamp`, which is an alias for the\\n`time` type.  By convention, the `timestamp` type represents the event time\\nembedded in the data itself. However, the import time  is not part of the event\\ndata itself, but rather part of metadata of every batch of events that VAST\\ncreates.\\n\\n[docs-meta-extractor]: https://vast.io/VAST%20v3.0/understand/language/expressions#meta-extractor\\n[docs-type-extractor]: https://vast.io/VAST%20v3.0/understand/language/expressions#type-extractor\\n\\n## Omit `null` fields in the JSON export\\n\\nVAST renders all fields defined in the schema when exporting events as JSON. A\\ncommon option for many tools that handle JSON is to skip rendering `null`\\nfields, and the new `--omit-nulls` option to the JSON export does exactly that.\\n\\nTo use it on a case-by-case basis, add this flag to any JSON export.\\n\\n```bash\\nvast export json --omit-nulls \'<query>\'\\n\\n# This also works when attaching to a matcher.\\nvast matcher attach json --omit-nulls <matcher>\\n```\\n\\nTo always enable it, add this to your `vast.yaml` configuration file:\\n\\n```yaml\\nvast:\\n  import:\\n    omit-nulls: true\\n```\\n\\n## Selection and Projection Transform Steps\\n\\n:::info Transforms \u2192 Pipelines\\nIn [VAST v2.2](/blog/vast-v2.2), we renamed *transforms* to *pipelines*, and\\n*transform steps* to *pipeline operators*. This caused several configuration key\\nchanges. Please keep this in mind when reading the example below and consult the\\n[documentation](/VAST%20v3.0/understand/language/pipelines) for the\\nup-to-date syntax.\\n:::\\n\\nReshaping data during import and export is a common use case that VAST now\\nsupports. The two new built-in transform steps allow for filtering columns and\\nrows. Filtering columns (*projection*) takes a list of column names as input,\\nand filtering rows (*selection*)  works with an arbitrary query expression.\\n\\nHere\u2019s a usage example that sanitizes data leaving VAST during a query. If any\\nstring field in an event contains the value `tenzir` or `secret-username`, VAST\\nwill not include the event in the result set. The example below applies this\\nsanitization only to the events  `suricata.dns` and `suricata.http`, as defined\\nin the section `transform-triggers`.\\n\\n```yaml\\nvast:\\n  # Specify and name our transforms, each of which are a list of configured\\n  # transform steps. Transform steps are plugins, enabling users to write more\\n  # complex transformations in native code using C++ and Apache Arrow.\\n  transforms:\\n     # Prevent events with certain strings to be exported, e.g., \\"tenzir\\" or\\n     # \\"secret-username\\".\\n     remove-events-with-secrets:\\n       - select:\\n           expression: \':string !in [\\"tenzir\\", \\"secret-username\\"]\'\\n\\n  # Specify whether to trigger each transform at server- or client-side, on\\n  # import or export, and restrict them to a list of event types.\\n  transform-triggers:\\n    export:\\n      # Apply the remove-events-with-secrets transformation server-side on\\n      # export to the suricata.dns and suricata.http event types.\\n      - transform: remove-events-with-secrets\\n        location: server\\n        events:\\n          - suricata.dns\\n          - suricata.http\\n```\\n\\n## Threat Bus 2022.01.27\\n\\nThanks to a contribution from Sascha Steinbiss\\n([@satta](https://github.com/satta)), Threat Bus only reports failure when\\ntransforming a sighting context if the return code of the transforming program\\nindicates failure.\\n\\nA small peek behind the curtain: We\u2019re building the next generation of Threat\\nBus as part of VAST. We will continue to develop and maintain Threat Bus and its\\napps for the time being.\\n\\nThreat Bus 2022.01.27 is available [\ud83d\udc49\\nhere](https://github.com/tenzir/threatbus/releases/tag/2022.01.27)."}]}')}}]);